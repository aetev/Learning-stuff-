{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/trading%20sac%20indicator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z1IPtawZ_QU6",
        "outputId": "54787d7e-65ec-4668-9b23-70ef8dd84350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_agents[reverb]\n",
        "!pip install pandas_ta"
      ],
      "metadata": {
        "id": "gkseWxmn_R_h",
        "outputId": "e8061b51-5b70-4fda-bb08-d132008b29fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf_agents[reverb]\n",
            "  Downloading tf_agents-0.16.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf_agents[reverb])\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (8.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (4.6.3)\n",
            "Collecting pygame==2.1.3 (from tf_agents[reverb])\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-probability~=0.19.0 (from tf_agents[reverb])\n",
            "  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rlds (from tf_agents[reverb])\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb~=0.11.0 (from tf_agents[reverb])\n",
            "  Downloading dm_reverb-0.11.0-cp310-cp310-manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.11.0->tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.11.0->tf_agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (1.54.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (0.32.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.19.0->tf_agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.12.0->tf_agents[reverb]) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.12.0->tf_agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.12.0->tf_agents[reverb]) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (3.2.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697641 sha256=3da3d49403069fd466b203e4cdc07ab4c56136ad696da944634d0592045ba29b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: tensorflow-probability, rlds, pygame, gym, dm-reverb, tf_agents\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.20.1\n",
            "    Uninstalling tensorflow-probability-0.20.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.20.1\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.4.0\n",
            "    Uninstalling pygame-2.4.0:\n",
            "      Successfully uninstalled pygame-2.4.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed dm-reverb-0.11.0 gym-0.23.0 pygame-2.1.3 rlds-0.1.8 tensorflow-probability-0.19.0 tf_agents-0.16.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas_ta\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pandas_ta) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->pandas_ta) (1.16.0)\n",
            "Building wheels for collected packages: pandas_ta\n",
            "  Building wheel for pandas_ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218908 sha256=d1bb2b79933d8775c397583bee935ef440249aeaa0eb2f3d4a1cbab6b9e3e7c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/00/ac/f7fa862c34b0e2ef320175100c233377b4c558944f12474cf0\n",
            "Successfully built pandas_ta\n",
            "Installing collected packages: pandas_ta\n",
            "Successfully installed pandas_ta-0.3.14b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import reverb\n",
        "import tempfile\n",
        "import PIL.Image\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pandas_ta as ta\n",
        "\n",
        "from tf_agents.environments import suite_gym\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import utils\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.train import actor\n",
        "from tf_agents.train import learner\n",
        "from tf_agents.train import triggers\n",
        "from tf_agents.train.utils import spec_utils\n",
        "from tf_agents.train.utils import strategy_utils\n",
        "from tf_agents.train.utils import train_utils"
      ],
      "metadata": {
        "id": "BDqp9KQn_yrH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "outputId": "d128af89-9044-42d9-d719-2740765ccf90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
        "# 1e5 is just so this doesn't take too long (1 hr)\n",
        "num_iterations = 1000000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256 # @param {type:\"integer\"}\n",
        "\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\n",
        "target_update_period = 1 # @param {type:\"number\"}\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "actor_fc_layer_params = (256, 256)\n",
        "critic_joint_fc_layer_params = (256, 256)\n",
        "\n",
        "log_interval = 1000 # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 5 # @param {type:\"integer\"}\n",
        "eval_interval = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "policy_save_interval = 5000 # @param {type:\"integer\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8Xo8gJchAtla"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relative_change(df, column_name):\n",
        "    relative_changes = []\n",
        "    for i in range(len(df)):\n",
        "        if i == 0:\n",
        "            relative_changes.append(0)  # First element has no previous value\n",
        "        else:\n",
        "            relative_change = (df.iloc[i] - df.iloc[i-1]) / df.iloc[i-1]\n",
        "            relative_changes.append(relative_change)\n",
        "    return pd.DataFrame({column_name: relative_changes})\n"
      ],
      "metadata": {
        "id": "6oWETAkPbXok"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/eurusd_hour.csv')\n",
        "pd_close = df['BC']\n",
        "bid_close = pd_close.to_numpy()\n",
        "\n",
        "\n",
        "change = get_relative_change(pd_close,\"Change\")\n",
        "rsi = ta.rsi(pd_close,14,scalar=1)\n",
        "stdev = ta.stdev(pd_close,14)\n",
        "macd = ta.macd(pd_close,14)\n",
        "\n",
        "\n",
        "df = pd.concat([change,rsi,stdev,macd], axis=1)\n",
        "df = df.dropna(axis=0)\n",
        "df = df.reset_index()\n",
        "\n",
        "change = df.pop('Change')\n",
        "df.pop('index')\n",
        "\n",
        "indicators = df.to_numpy()\n",
        "change = change.to_numpy()\n",
        "change_dev = np.std(change)\n",
        "np.set_printoptions(suppress=True)\n",
        "print(change_dev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3hj-E1MVcS1",
        "outputId": "8cb01b81-3a5d-4346-ad3c-40cee374be98"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0012178838054657292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "commission_rate = .005\n",
        "default_val = 10\n",
        "max_risk = .05\n",
        "max_ep_len = 5000"
      ],
      "metadata": {
        "id": "m3uSJdUBbJ52"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CardGameEnv(py_environment.PyEnvironment):\n",
        "    def __init__(self):\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(1,), dtype=np.float32, minimum=[-1], maximum=[1], name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(7,), dtype=np.float32, minimum=-10, maximum=10, name='observation')\n",
        "\n",
        "        self._starting_val = random.randint(0,10000)\n",
        "        self._state = np.append(indicators[self._starting_val],(0,0))\n",
        "        self._episode_ended = False\n",
        "        self._step_count = self._starting_val + 1\n",
        "        self._net_profit = 0\n",
        "\n",
        "\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "\n",
        "\n",
        "    def _reset(self):\n",
        "\n",
        "        self._starting_val = random.randint(0,10000)\n",
        "        self._state = np.append(indicators[self._starting_val],(0,0))\n",
        "        self._episode_ended = False\n",
        "        self._step_count = self._starting_val + 1\n",
        "        self._net_profit = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "\n",
        "\n",
        "    def _step(self, action):\n",
        "        if self._episode_ended:\n",
        "            return self.reset()\n",
        "\n",
        "        action = action[0]\n",
        "\n",
        "        cur_change = change[self._step_count]\n",
        "\n",
        "        cur_val = action * cur_change\n",
        "\n",
        "        self._net_profit += cur_val\n",
        "\n",
        "        #update state\n",
        "        self._state = np.append(indicators[self._step_count],(action,cur_change))\n",
        "\n",
        "        #end episode if max steps or invalid_action\n",
        "        if self._step_count >= self._starting_val + max_ep_len:\n",
        "            self._episode_ended = True\n",
        "\n",
        "        self._step_count += 1\n",
        "\n",
        "        if self._episode_ended:\n",
        "            reward = self._net_profit#self._total_pen\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), reward=reward)\n",
        "        else:\n",
        "            reward = cur_val\n",
        "            return ts.transition(np.array(self._state, dtype=np.float32), reward=reward,discount=.5)"
      ],
      "metadata": {
        "id": "48DkwFLnVg9O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "environment = CardGameEnv()\n",
        "utils.validate_py_environment(environment, episodes=2)\n",
        "#env = environment#tf_py_environment.TFPyEnvironment(environment)"
      ],
      "metadata": {
        "id": "0YMxohCHWezc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = environment #tf_py_environment.TFPyEnvironment(environment)\n",
        "env_name = 'MountainCarContinuous-v0'\n",
        "collect_env  = env #suite_gym.load(env_name)\n",
        "eval_env   = env #suite_gym.load(env_name)"
      ],
      "metadata": {
        "id": "2lIl-hO-_0Mm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu = False\n",
        "\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ],
      "metadata": {
        "id": "CkDfs-xfBJUU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation_spec, action_spec, time_step_spec = (\n",
        "      spec_utils.get_tensor_specs(collect_env))\n",
        "\n",
        "with strategy.scope():\n",
        "  critic_net = critic_network.CriticNetwork(\n",
        "        (observation_spec, action_spec),\n",
        "        observation_fc_layer_params=None,\n",
        "        action_fc_layer_params=None,\n",
        "        joint_fc_layer_params=(20,20),\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        last_kernel_initializer='glorot_uniform')"
      ],
      "metadata": {
        "id": "RbBZPAOXAf78"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Observation Spec:\", observation_spec)\n",
        "print(\"Action Spec:\", action_spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t4j7m6SVQqr",
        "outputId": "43c86d23-c44b-48a7-cd19-957700ebf587"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Spec: BoundedTensorSpec(shape=(7,), dtype=tf.float32, name='observation', minimum=array(-10., dtype=float32), maximum=array(10., dtype=float32))\n",
            "Action Spec: BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array([-1.], dtype=float32), maximum=array([1.], dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=(20,20),\n",
        "      continuous_projection_net=(\n",
        "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
      ],
      "metadata": {
        "id": "jI0afo1FBPga"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  train_step = train_utils.create_train_step()\n",
        "\n",
        "  tf_agent = sac_agent.SacAgent(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        actor_network=actor_net,\n",
        "        critic_network=critic_net,\n",
        "        actor_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=actor_learning_rate),\n",
        "        critic_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=critic_learning_rate),\n",
        "        alpha_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=alpha_learning_rate),\n",
        "        target_update_tau=target_update_tau,\n",
        "        target_update_period=target_update_period,\n",
        "        td_errors_loss_fn=tf.math.squared_difference,\n",
        "        gamma=gamma,\n",
        "        reward_scale_factor=reward_scale_factor,\n",
        "        train_step_counter=train_step)\n",
        "\n",
        "  tf_agent.initialize()"
      ],
      "metadata": {
        "id": "6V3WUw25B1k7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)\n",
        "\n",
        "table_name = 'uniform_table'\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    sequence_length=2,\n",
        "    table_name=table_name,\n",
        "    local_server=reverb_server)"
      ],
      "metadata": {
        "id": "8OtP7sjGBf6R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = reverb_replay.as_dataset(\n",
        "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
        "experience_dataset_fn = lambda: dataset"
      ],
      "metadata": {
        "id": "S2GPKTjeD8zP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_eval_policy = tf_agent.policy\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_eval_policy, use_tf_function=True)\n",
        "\n",
        "tf_collect_policy = tf_agent.collect_policy\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_collect_policy, use_tf_function=True)"
      ],
      "metadata": {
        "id": "Q8FrWue9EAej"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  collect_env.time_step_spec(), collect_env.action_spec())"
      ],
      "metadata": {
        "id": "9_GBDROTEQXK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  reverb_replay.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)"
      ],
      "metadata": {
        "id": "eJP741I1ESXV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  random_policy,\n",
        "  train_step,\n",
        "  steps_per_run=initial_collect_steps,\n",
        "  observers=[rb_observer])\n",
        "initial_collect_actor.run()"
      ],
      "metadata": {
        "id": "g8sD1uCkExqN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location = \"/content/drive/MyDrive/Model_foler/Mod1\""
      ],
      "metadata": {
        "id": "tAEqMRo2Jmxc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  collect_policy,\n",
        "  train_step,\n",
        "  steps_per_run=1,\n",
        "  metrics=actor.collect_metrics(10),\n",
        "  summary_dir=os.path.join(location, learner.TRAIN_DIR),\n",
        "  observers=[rb_observer, env_step_metric])"
      ],
      "metadata": {
        "id": "gdA_u_j_Ey6p"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_actor = actor.Actor(\n",
        "  eval_env,\n",
        "  eval_policy,\n",
        "  train_step,\n",
        "  episodes_per_run=num_eval_episodes,\n",
        "  metrics=actor.eval_metrics(num_eval_episodes),\n",
        "  summary_dir=os.path.join(location, 'eval'),\n",
        ")"
      ],
      "metadata": {
        "id": "IHYlGKpsE0Hs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_dir = os.path.join(location, learner.POLICY_SAVED_MODEL_DIR)\n",
        "\n",
        "# Triggers to save the agent's policy checkpoints.\n",
        "learning_triggers = [\n",
        "    triggers.PolicySavedModelTrigger(\n",
        "        saved_model_dir,\n",
        "        tf_agent,\n",
        "        train_step,\n",
        "        interval=policy_save_interval),\n",
        "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
        "]\n",
        "\n",
        "agent_learner = learner.Learner(\n",
        "  location,\n",
        "  train_step,\n",
        "  tf_agent,\n",
        "  experience_dataset_fn,\n",
        "  triggers=learning_triggers,\n",
        "  strategy=strategy)"
      ],
      "metadata": {
        "id": "ybubwekdFFNs",
        "outputId": "0a0e5c63-1058-4c23-a959-3046b07100a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/distributions/distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
            "Instructions for updating:\n",
            "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fbcd9f37dc0>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fbcd9f37dc0>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_eval_metrics():\n",
        "  eval_actor.run()\n",
        "  results = {}\n",
        "  for metric in eval_actor.metrics:\n",
        "    results[metric.name] = metric.result()\n",
        "  return results\n",
        "\n",
        "metrics = get_eval_metrics()"
      ],
      "metadata": {
        "id": "NyOHolq_FUhG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_eval_metrics(step, metrics):\n",
        "  eval_results = (', ').join(\n",
        "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
        "  print('step = {0}: {1}'.format(step, eval_results))\n",
        "\n",
        "log_eval_metrics(0, metrics)"
      ],
      "metadata": {
        "id": "yjlwf8G-FV0m",
        "outputId": "6ab9296f-e1af-4760-8e58-4b3d23dc004d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 0: AverageReturn = -0.009489, AverageEpisodeLength = 5000.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Training.\n",
        "  collect_actor.run()\n",
        "  loss_info = agent_learner.run(iterations=1)\n",
        "\n",
        "  # Evaluating.\n",
        "  step = agent_learner.train_step_numpy\n",
        "\n",
        "  if eval_interval and step % eval_interval == 0:\n",
        "    metrics = get_eval_metrics()\n",
        "    log_eval_metrics(step, metrics)\n",
        "    returns.append(metrics[\"AverageReturn\"])\n",
        "\n",
        "  if log_interval and step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
        "\n",
        "rb_observer.close()\n",
        "reverb_server.stop()"
      ],
      "metadata": {
        "id": "aspLZmK1Fjjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d2600d-0f87-4d78-84d1-60c40c79bfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1000: loss = -6.242703437805176\n",
            "step = 2000: loss = -0.856689989566803\n",
            "step = 3000: loss = -0.6459925770759583\n",
            "step = 4000: loss = -1.4494155645370483\n",
            "step = 5000: loss = -0.8806740045547485\n",
            "step = 6000: loss = -1.4620229005813599\n",
            "step = 7000: loss = 0.12397831678390503\n",
            "step = 8000: loss = 0.6182622909545898\n",
            "step = 9000: loss = 1.3557088375091553\n",
            "step = 10000: AverageReturn = 0.072471, AverageEpisodeLength = 5000.200195\n",
            "step = 10000: loss = -0.9146647453308105\n",
            "step = 11000: loss = 3.6555144786834717\n",
            "step = 12000: loss = 0.6765638589859009\n",
            "step = 13000: loss = 0.26623326539993286\n",
            "step = 14000: loss = 0.7152742743492126\n",
            "step = 15000: loss = -2.111241579055786\n",
            "step = 16000: loss = -2.1701903343200684\n",
            "step = 17000: loss = 1.2632752656936646\n",
            "step = 18000: loss = -0.3420027792453766\n",
            "step = 19000: loss = -1.7012863159179688\n",
            "step = 20000: AverageReturn = 0.047942, AverageEpisodeLength = 4999.799805\n",
            "step = 20000: loss = 0.32669904828071594\n",
            "step = 21000: loss = -1.9018296003341675\n",
            "step = 22000: loss = -0.4885706901550293\n",
            "step = 23000: loss = -2.615569829940796\n",
            "step = 24000: loss = 2.2313084602355957\n",
            "step = 25000: loss = 0.5022730827331543\n",
            "step = 26000: loss = -1.329870343208313\n",
            "step = 27000: loss = -2.0021400451660156\n",
            "step = 28000: loss = 1.4433863162994385\n",
            "step = 29000: loss = -3.0531165599823\n",
            "step = 30000: AverageReturn = 0.022331, AverageEpisodeLength = 5000.200195\n",
            "step = 30000: loss = 1.892384648323059\n",
            "step = 31000: loss = 1.5901767015457153\n",
            "step = 32000: loss = -1.5464729070663452\n",
            "step = 33000: loss = 2.0100784301757812\n",
            "step = 34000: loss = 2.2515869140625\n",
            "step = 35000: loss = 2.374859571456909\n",
            "step = 36000: loss = -1.1495178937911987\n",
            "step = 37000: loss = 0.9842404723167419\n",
            "step = 38000: loss = 0.75373774766922\n",
            "step = 39000: loss = -0.8470082879066467\n",
            "step = 40000: AverageReturn = 0.058864, AverageEpisodeLength = 4999.799805\n",
            "step = 40000: loss = 0.19739599525928497\n",
            "step = 41000: loss = 0.41049352288246155\n",
            "step = 42000: loss = -0.36091718077659607\n",
            "step = 43000: loss = -0.591681718826294\n",
            "step = 44000: loss = 1.2389030456542969\n",
            "step = 45000: loss = -0.38543081283569336\n",
            "step = 46000: loss = -0.5836179256439209\n",
            "step = 47000: loss = -1.0963404178619385\n",
            "step = 48000: loss = -0.9096524119377136\n",
            "step = 49000: loss = -0.48276737332344055\n",
            "step = 50000: AverageReturn = 0.022756, AverageEpisodeLength = 5000.200195\n",
            "step = 50000: loss = -0.9524693489074707\n",
            "step = 51000: loss = -1.770002007484436\n",
            "step = 52000: loss = 1.4467079639434814\n",
            "step = 53000: loss = -0.4451448917388916\n",
            "step = 54000: loss = 1.6931697130203247\n",
            "step = 55000: loss = 0.22426322102546692\n",
            "step = 56000: loss = 1.2453548908233643\n",
            "step = 57000: loss = -2.218989610671997\n",
            "step = 58000: loss = -0.14181716740131378\n",
            "step = 59000: loss = -0.316142201423645\n",
            "step = 60000: AverageReturn = 0.030696, AverageEpisodeLength = 4999.799805\n",
            "step = 60000: loss = 1.3538017272949219\n",
            "step = 61000: loss = -1.369042992591858\n",
            "step = 62000: loss = -0.8121206164360046\n",
            "step = 63000: loss = -2.4948740005493164\n",
            "step = 64000: loss = -1.2871441841125488\n",
            "step = 65000: loss = 1.1172683238983154\n",
            "step = 66000: loss = -1.703589677810669\n",
            "step = 67000: loss = 0.7527092099189758\n",
            "step = 68000: loss = -1.4840538501739502\n",
            "step = 69000: loss = -0.3353748321533203\n",
            "step = 70000: AverageReturn = 0.015135, AverageEpisodeLength = 5000.200195\n",
            "step = 70000: loss = 1.97141695022583\n",
            "step = 71000: loss = -0.6082133650779724\n",
            "step = 72000: loss = -0.3983887732028961\n",
            "step = 73000: loss = 1.7454971075057983\n",
            "step = 74000: loss = 0.8474903702735901\n",
            "step = 75000: loss = -0.9260982871055603\n",
            "step = 76000: loss = 1.7448495626449585\n",
            "step = 77000: loss = 1.5166099071502686\n",
            "step = 78000: loss = 0.4794628918170929\n",
            "step = 79000: loss = 1.9038554430007935\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}