{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxAdTdzEWOUf5YqY6b32qx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/wganworkking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xI7rEeMMgDQI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgyl_WtbgO5k",
        "outputId": "b8fe388b-053f-4353-a985-e218964cd9cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7,activation='LeakyReLU',use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(14*14,activation='LeakyReLU',use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(28*28,activation='LeakyReLU',use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(28*28,activation='sigmoid',use_bias=False))\n",
        "    model.add(layers.Reshape((28,28,1)))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_discriminator_model():\n",
        "    input_img = layers.Input(shape=(28,28,1))\n",
        "\n",
        "    x = layers.Conv2D(64,3,2,padding='same',activation='LeakyReLU')(input_img)\n",
        "\n",
        "    x = layers.Dropout(.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(64,3,2,padding='same',activation='LeakyReLU')(x)\n",
        "\n",
        "    x = layers.Dropout(.2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    dense_output = layers.Dense(128, activation='LeakyReLU')(x)\n",
        "\n",
        "\n",
        "    x = layers.Dropout(.2)(x)\n",
        "\n",
        "    dense_output = layers.Dense(64, activation='LeakyReLU')(x)\n",
        "\n",
        "    x = layers.Dropout(.2)(x)\n",
        "\n",
        "    dense_output = layers.Dense(1, activation=None)(dense_output)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=input_img, outputs=dense_output)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "kik7wQ1qgOV-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "qzdSabqdVu_H"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator_model()\n",
        "discriminatorW = make_discriminator_model()\n",
        "discriminatorU = make_discriminator_model()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0004)\n",
        "discriminatorW_optimizer = tf.keras.optimizers.Adam(0.0004)\n",
        "discriminatorU_optimizer = tf.keras.optimizers.Adam(0.000004)"
      ],
      "metadata": {
        "id": "lS0DF8rjhHdY"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 10\n",
        "\n",
        "#@tf.function\n",
        "def discriminator_lossW(real_output, fake_output):\n",
        "    real_loss = tf.reduce_mean(real_output)\n",
        "    fake_loss = tf.reduce_mean(fake_output)\n",
        "    total_loss = fake_loss - real_loss\n",
        "    return total_loss\n",
        "\n",
        "#@tf.function\n",
        "def generator_lossW(fake_output):\n",
        "    return -tf.reduce_mean(fake_output)\n",
        "\n",
        "#@tf.function\n",
        "def gradient_penalty(real_images, fake_images):\n",
        "    alpha = tf.random.uniform([BATCH_SIZE, 1, 1, 1], 0., 1.)\n",
        "    real_images, fake_images = tf.cast(real_images, tf.float32), tf.cast(fake_images, tf.float32)\n",
        "    interpolated_images = alpha * real_images + ((1 - alpha) * fake_images)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated_images)\n",
        "        pred = discriminatorW(interpolated_images, training=True)\n",
        "    gradients = tape.gradient(pred, [interpolated_images])[0]\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "    gp = tf.reduce_mean((norm - 1.)**2)\n",
        "    return gp"
      ],
      "metadata": {
        "id": "DYQpIZyEgSlN"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NOISE_DIM = 100\n",
        "GP_WEIGHT = 10\n",
        "\n",
        "\n",
        "#@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
        "\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tapeW, tf.GradientTape() as disc_tapeU:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_outputW = discriminatorW(images, training=True)\n",
        "        fake_outputW = discriminatorW(generated_images, training=True)\n",
        "\n",
        "        real_outputU = discriminatorU(images, training=True)\n",
        "        fake_outputU = discriminatorU(generated_images, training=True)\n",
        "\n",
        "        disc_lossW = discriminator_lossW(real_outputW, fake_outputW)\n",
        "        disc_lossU = cross_entropy(tf.ones_like(real_outputU), real_outputU)\n",
        "\n",
        "        gen_loss = generator_lossW(fake_outputW)\n",
        "        gen_loss += cross_entropy(tf.ones_like(fake_outputU), fake_outputU)\n",
        "        gp = gradient_penalty(images, generated_images)\n",
        "        disc_lossW += gp * GP_WEIGHT\n",
        "\n",
        "\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    gradients_of_discriminatorW = disc_tapeW.gradient(disc_lossW, discriminatorW.trainable_variables)\n",
        "    gradients_of_discriminatorU = disc_tapeU.gradient(disc_lossU, discriminatorU.trainable_variables)\n",
        "    discriminatorW_optimizer.apply_gradients(zip(gradients_of_discriminatorW, discriminatorW.trainable_variables))\n",
        "    discriminatorU_optimizer.apply_gradients(zip(gradients_of_discriminatorU, discriminatorU.trainable_variables))\n",
        "\n",
        "    tf.print(\"disc_lossW\",disc_lossW,'disc_lossU',disc_lossU,'gen_loss',gen_loss)"
      ],
      "metadata": {
        "id": "cP3-wZztEeaE"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for batch in range(len(dataset) // BATCH_SIZE):\n",
        "\n",
        "            target_images = dataset[batch * BATCH_SIZE: (batch+1) * BATCH_SIZE]\n",
        "\n",
        "\n",
        "            train_step(target_images)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      print(epoch)\n",
        "\n"
      ],
      "metadata": {
        "id": "Sx_KbIfjiogE"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "x_train2 = np.expand_dims(x_train, axis=-1)\n",
        "x_train2 = (x_train2 - np.min(x_train2)) / (np.max(x_train2) - np.min(x_train2))\n",
        "train(x_train2, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWp_3RkPg248",
        "outputId": "244d2df7-c0e6-4340-cd37-504834464c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "disc_lossW 6.49962044 disc_lossU 0.726438165 gen_loss 0.914006829\n",
            "disc_lossW 5.90742683 disc_lossU 0.725367188 gen_loss 1.10277843\n",
            "disc_lossW 5.27705765 disc_lossU 0.721236169 gen_loss 1.31378031\n",
            "disc_lossW 4.69356823 disc_lossU 0.708376229 gen_loss 1.57315946\n",
            "disc_lossW 3.82161379 disc_lossU 0.714574695 gen_loss 1.87556267\n",
            "disc_lossW 2.92020535 disc_lossU 0.72244978 gen_loss 2.15437984\n",
            "disc_lossW 2.11110449 disc_lossU 0.706939757 gen_loss 2.58246684\n",
            "disc_lossW 1.03695369 disc_lossU 0.71627605 gen_loss 3.05320811\n",
            "disc_lossW 0.0152146816 disc_lossU 0.715115845 gen_loss 3.54584932\n",
            "disc_lossW -1.27456307 disc_lossU 0.712330341 gen_loss 4.26060772\n",
            "disc_lossW -2.49069476 disc_lossU 0.69756192 gen_loss 4.91810465\n",
            "disc_lossW -3.13314486 disc_lossU 0.73112464 gen_loss 5.8240571\n",
            "disc_lossW -4.6005888 disc_lossU 0.71171844 gen_loss 6.75211954\n",
            "disc_lossW -5.97942686 disc_lossU 0.699934125 gen_loss 7.90419388\n",
            "disc_lossW -7.1408 disc_lossU 0.705795 gen_loss 9.33201\n",
            "disc_lossW -7.88134193 disc_lossU 0.69891721 gen_loss 10.6762648\n",
            "disc_lossW -8.23905182 disc_lossU 0.696003795 gen_loss 12.0611229\n",
            "disc_lossW -9.5577383 disc_lossU 0.708546102 gen_loss 14.0831881\n",
            "disc_lossW -8.24954796 disc_lossU 0.703950584 gen_loss 15.2640972\n",
            "disc_lossW -9.14816475 disc_lossU 0.708179474 gen_loss 17.0754471\n",
            "disc_lossW -9.43450356 disc_lossU 0.706362605 gen_loss 18.4268703\n",
            "disc_lossW -9.58955574 disc_lossU 0.701147914 gen_loss 19.4790401\n",
            "disc_lossW -8.68943787 disc_lossU 0.712825 gen_loss 20.2232647\n",
            "disc_lossW -10.176815 disc_lossU 0.696619153 gen_loss 20.5694332\n",
            "disc_lossW -9.95515442 disc_lossU 0.712451577 gen_loss 20.5647411\n",
            "disc_lossW -8.93228436 disc_lossU 0.705425382 gen_loss 20.1706276\n",
            "disc_lossW -11.0895634 disc_lossU 0.709 gen_loss 19.9419785\n",
            "disc_lossW -11.7470531 disc_lossU 0.693600655 gen_loss 19.2679138\n",
            "disc_lossW -9.08493 disc_lossU 0.701858163 gen_loss 18.2581673\n",
            "disc_lossW -11.2203035 disc_lossU 0.702707231 gen_loss 18.0944633\n",
            "disc_lossW -10.7927723 disc_lossU 0.696208119 gen_loss 16.8933182\n",
            "disc_lossW -12.3765907 disc_lossU 0.701360166 gen_loss 16.118681\n",
            "disc_lossW -11.695179 disc_lossU 0.694401383 gen_loss 15.6180906\n",
            "disc_lossW -11.3995705 disc_lossU 0.711605906 gen_loss 14.9671917\n",
            "disc_lossW -12.7058334 disc_lossU 0.695568502 gen_loss 14.620122\n",
            "disc_lossW -12.1224174 disc_lossU 0.704252243 gen_loss 14.5400295\n",
            "disc_lossW -12.8000813 disc_lossU 0.699908376 gen_loss 14.4294033\n",
            "disc_lossW -12.2969656 disc_lossU 0.703994513 gen_loss 14.8368778\n",
            "disc_lossW -11.3265028 disc_lossU 0.696127 gen_loss 14.8542271\n",
            "disc_lossW -13.038353 disc_lossU 0.681243539 gen_loss 15.2102375\n",
            "disc_lossW -12.8593788 disc_lossU 0.704628289 gen_loss 15.0431051\n",
            "disc_lossW -11.9005241 disc_lossU 0.697828472 gen_loss 15.846489\n",
            "disc_lossW -12.5214577 disc_lossU 0.697029769 gen_loss 15.8572617\n",
            "disc_lossW -12.3497963 disc_lossU 0.693186581 gen_loss 16.5857849\n",
            "disc_lossW -12.1596966 disc_lossU 0.696335316 gen_loss 16.6099892\n",
            "disc_lossW -12.1205883 disc_lossU 0.695756912 gen_loss 17.2932949\n",
            "disc_lossW -12.501668 disc_lossU 0.698989034 gen_loss 17.2217102\n",
            "disc_lossW -14.2180614 disc_lossU 0.693891704 gen_loss 17.6985817\n",
            "disc_lossW -13.23386 disc_lossU 0.694146812 gen_loss 17.9077015\n",
            "disc_lossW -12.9641428 disc_lossU 0.691692829 gen_loss 18.0850964\n",
            "disc_lossW -12.0546513 disc_lossU 0.690808892 gen_loss 18.4492989\n",
            "disc_lossW -13.0848312 disc_lossU 0.70906657 gen_loss 18.0086155\n",
            "disc_lossW -12.2522182 disc_lossU 0.703697622 gen_loss 18.2043591\n",
            "disc_lossW -13.6275768 disc_lossU 0.695722818 gen_loss 18.0556164\n",
            "disc_lossW -14.024971 disc_lossU 0.67299 gen_loss 18.0470142\n",
            "disc_lossW -13.3781576 disc_lossU 0.678727 gen_loss 17.7581654\n",
            "disc_lossW -14.1489315 disc_lossU 0.69271338 gen_loss 18.2565327\n",
            "disc_lossW -13.76721 disc_lossU 0.699405193 gen_loss 18.3649082\n",
            "disc_lossW -13.6942368 disc_lossU 0.684194863 gen_loss 18.2584591\n",
            "disc_lossW -13.6681852 disc_lossU 0.690465629 gen_loss 18.5560226\n",
            "disc_lossW -13.237793 disc_lossU 0.68430686 gen_loss 18.8139877\n",
            "disc_lossW -13.4434757 disc_lossU 0.691940784 gen_loss 18.9582691\n",
            "disc_lossW -12.8732185 disc_lossU 0.681343138 gen_loss 19.2987614\n",
            "disc_lossW -13.460392 disc_lossU 0.690900445 gen_loss 19.0875168\n",
            "disc_lossW -13.663909 disc_lossU 0.68583715 gen_loss 18.4469032\n",
            "disc_lossW -13.6447086 disc_lossU 0.672819555 gen_loss 18.9184074\n",
            "disc_lossW -12.351078 disc_lossU 0.678814769 gen_loss 18.4585476\n",
            "disc_lossW -13.5936813 disc_lossU 0.675286412 gen_loss 18.4878635\n",
            "disc_lossW -13.4483023 disc_lossU 0.692010283 gen_loss 18.1731148\n",
            "disc_lossW -12.9660234 disc_lossU 0.679056644 gen_loss 18.4354591\n",
            "disc_lossW -12.3826694 disc_lossU 0.691932857 gen_loss 17.7286377\n",
            "disc_lossW -12.7079754 disc_lossU 0.670012653 gen_loss 17.3667107\n",
            "disc_lossW -13.488265 disc_lossU 0.67916286 gen_loss 17.4051704\n",
            "disc_lossW -12.8501434 disc_lossU 0.67863816 gen_loss 17.3119946\n",
            "disc_lossW -13.304163 disc_lossU 0.695744872 gen_loss 17.4241467\n",
            "disc_lossW -12.7451038 disc_lossU 0.680421829 gen_loss 17.6947975\n",
            "disc_lossW -13.0014 disc_lossU 0.67458123 gen_loss 17.4594154\n",
            "disc_lossW -12.9533501 disc_lossU 0.669717729 gen_loss 17.5112972\n",
            "disc_lossW -13.217514 disc_lossU 0.68346417 gen_loss 17.719\n",
            "disc_lossW -13.4028244 disc_lossU 0.67535758 gen_loss 17.9489365\n",
            "disc_lossW -14.0172577 disc_lossU 0.677117288 gen_loss 17.8555202\n",
            "disc_lossW -12.7011967 disc_lossU 0.677593112 gen_loss 17.7975922\n",
            "disc_lossW -12.8504543 disc_lossU 0.68785578 gen_loss 18.0506649\n",
            "disc_lossW -12.9843397 disc_lossU 0.661671 gen_loss 17.7797966\n",
            "disc_lossW -12.5186539 disc_lossU 0.656509161 gen_loss 17.4554482\n",
            "disc_lossW -12.8291636 disc_lossU 0.676791191 gen_loss 17.8063908\n",
            "disc_lossW -12.8886127 disc_lossU 0.669656575 gen_loss 17.5719471\n",
            "disc_lossW -12.1306715 disc_lossU 0.663037121 gen_loss 17.2063141\n",
            "disc_lossW -11.9627075 disc_lossU 0.673153698 gen_loss 16.6384697\n",
            "disc_lossW -12.5543594 disc_lossU 0.674106538 gen_loss 16.7328167\n",
            "disc_lossW -12.5449762 disc_lossU 0.681645393 gen_loss 16.9394283\n",
            "disc_lossW -12.7827406 disc_lossU 0.662675738 gen_loss 16.764576\n",
            "disc_lossW -12.767971 disc_lossU 0.666761756 gen_loss 16.7904606\n",
            "disc_lossW -12.5737667 disc_lossU 0.676119 gen_loss 16.3280697\n",
            "disc_lossW -12.6253929 disc_lossU 0.662556052 gen_loss 17.2160892\n",
            "disc_lossW -12.3079567 disc_lossU 0.658673286 gen_loss 17.4234371\n",
            "disc_lossW -12.4857388 disc_lossU 0.675522208 gen_loss 17.8451748\n",
            "disc_lossW -12.6078033 disc_lossU 0.655428529 gen_loss 18.2553043\n",
            "disc_lossW -11.4572706 disc_lossU 0.666716754 gen_loss 17.7526245\n",
            "disc_lossW -12.0404911 disc_lossU 0.675316811 gen_loss 17.667078\n",
            "disc_lossW -12.9352188 disc_lossU 0.670712113 gen_loss 17.6356182\n",
            "disc_lossW -12.0374012 disc_lossU 0.676999867 gen_loss 16.952076\n",
            "disc_lossW -12.508646 disc_lossU 0.669851422 gen_loss 17.3469658\n",
            "disc_lossW -12.5891342 disc_lossU 0.669165432 gen_loss 16.6982975\n",
            "disc_lossW -11.5042763 disc_lossU 0.668143272 gen_loss 16.7850266\n",
            "disc_lossW -12.5761232 disc_lossU 0.678551853 gen_loss 16.9172192\n",
            "disc_lossW -12.7898474 disc_lossU 0.659632325 gen_loss 16.9554558\n",
            "disc_lossW -11.77176 disc_lossU 0.667699695 gen_loss 17.2376232\n",
            "disc_lossW -12.7443895 disc_lossU 0.663709104 gen_loss 17.2971821\n",
            "disc_lossW -10.8548346 disc_lossU 0.665903032 gen_loss 16.8132439\n",
            "disc_lossW -11.6042023 disc_lossU 0.656603813 gen_loss 17.2485619\n",
            "disc_lossW -12.5133858 disc_lossU 0.668719 gen_loss 16.9677334\n",
            "disc_lossW -11.7849693 disc_lossU 0.687259376 gen_loss 17.2823067\n",
            "disc_lossW -11.9881363 disc_lossU 0.651208699 gen_loss 17.2589703\n",
            "disc_lossW -11.33465 disc_lossU 0.654185891 gen_loss 16.8933296\n",
            "disc_lossW -11.4747171 disc_lossU 0.660984695 gen_loss 17.1419258\n",
            "disc_lossW -11.7931747 disc_lossU 0.668651462 gen_loss 16.7015076\n",
            "disc_lossW -11.1714268 disc_lossU 0.663607419 gen_loss 16.6528988\n",
            "disc_lossW -11.0820026 disc_lossU 0.643279433 gen_loss 16.6112404\n",
            "disc_lossW -10.1241302 disc_lossU 0.667230129 gen_loss 16.0902443\n",
            "disc_lossW -10.3969994 disc_lossU 0.650884509 gen_loss 16.0976295\n",
            "disc_lossW -10.8869419 disc_lossU 0.639888108 gen_loss 15.844614\n",
            "disc_lossW -10.8208666 disc_lossU 0.652457535 gen_loss 15.1976023\n",
            "disc_lossW -10.5203161 disc_lossU 0.650785565 gen_loss 14.8710842\n",
            "disc_lossW -10.2164364 disc_lossU 0.675254345 gen_loss 14.8604021\n",
            "disc_lossW -10.6684284 disc_lossU 0.633146465 gen_loss 14.5642538\n",
            "disc_lossW -9.11480522 disc_lossU 0.619149864 gen_loss 14.0578346\n",
            "disc_lossW -10.4297314 disc_lossU 0.644289 gen_loss 13.9439831\n",
            "disc_lossW -10.3494291 disc_lossU 0.656365633 gen_loss 13.1539392\n",
            "disc_lossW -9.84113121 disc_lossU 0.642866254 gen_loss 13.0373516\n",
            "disc_lossW -9.91295147 disc_lossU 0.642500818 gen_loss 12.6308756\n",
            "disc_lossW -9.1819191 disc_lossU 0.665388763 gen_loss 12.2051086\n",
            "disc_lossW -9.87645 disc_lossU 0.650842309 gen_loss 12.1691208\n",
            "disc_lossW -9.3421936 disc_lossU 0.629256606 gen_loss 11.7586365\n",
            "disc_lossW -9.65198612 disc_lossU 0.641113 gen_loss 11.7206392\n",
            "disc_lossW -9.39577675 disc_lossU 0.644156754 gen_loss 11.3630285\n",
            "disc_lossW -8.88927937 disc_lossU 0.649763405 gen_loss 11.3836756\n",
            "disc_lossW -9.2057085 disc_lossU 0.638864517 gen_loss 11.0264835\n",
            "disc_lossW -9.19737 disc_lossU 0.632133424 gen_loss 10.9742355\n",
            "disc_lossW -8.92809296 disc_lossU 0.625858545 gen_loss 10.5138922\n",
            "disc_lossW -8.38157749 disc_lossU 0.651928127 gen_loss 10.1580124\n",
            "disc_lossW -8.36380768 disc_lossU 0.656044126 gen_loss 9.93223572\n",
            "disc_lossW -8.02472305 disc_lossU 0.664992034 gen_loss 9.92939472\n",
            "disc_lossW -8.0689764 disc_lossU 0.638669074 gen_loss 9.75099182\n",
            "disc_lossW -7.6345892 disc_lossU 0.654743791 gen_loss 9.88595\n",
            "disc_lossW -8.0371809 disc_lossU 0.666336536 gen_loss 9.69495392\n",
            "disc_lossW -7.75171661 disc_lossU 0.641418338 gen_loss 9.92407513\n",
            "disc_lossW -7.30059528 disc_lossU 0.649202466 gen_loss 9.9985733\n",
            "disc_lossW -7.62113667 disc_lossU 0.650921285 gen_loss 10.3227472\n",
            "disc_lossW -7.97255182 disc_lossU 0.654152393 gen_loss 10.3682461\n",
            "disc_lossW -7.9907465 disc_lossU 0.635637939 gen_loss 10.617012\n",
            "disc_lossW -7.39754343 disc_lossU 0.637651265 gen_loss 10.4205103\n",
            "disc_lossW -7.43654633 disc_lossU 0.656691194 gen_loss 10.4916573\n",
            "disc_lossW -6.70440769 disc_lossU 0.636649907 gen_loss 10.4716244\n",
            "disc_lossW -6.95949459 disc_lossU 0.64305687 gen_loss 10.1976671\n",
            "disc_lossW -7.24960518 disc_lossU 0.620698333 gen_loss 10.4876528\n",
            "disc_lossW -6.6081028 disc_lossU 0.638492942 gen_loss 10.3429689\n",
            "disc_lossW -7.1983 disc_lossU 0.644348502 gen_loss 10.2654333\n",
            "disc_lossW -6.12705898 disc_lossU 0.624770761 gen_loss 10.0475473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noise = tf.random.normal(shape=(1,100))\n",
        "test = generator.predict(noise)\n",
        "plt.imshow(test.squeeze(), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "9vPp5AiDkrAF",
        "outputId": "ca1b2298-66c2-4ce6-f140-cf1344a78179"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkB0lEQVR4nO3de2zV9f3H8Vevh1LaU0vpTQoW5CJyWYbCiIg6Gi4mRIQYb0vAGIiumCFjOhYF3Ra7YeLPaBhmywa6Cd7GRYmSIEiZG2C4BVHXQUEooS2C9Jz2QE8v5/v7g9CtUoTPh57zOS3PR3ISevp99fM53572xWlP3yfB8zxPAADEWKLrDQAArk0UEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnkl1v4LsikYhOnDihjIwMJSQkuN4OAMCQ53mqr69XYWGhEhMv/Tgn7groxIkTKioqcr0NAMBVqqqqUt++fS/5/rgroIyMjLZ/mzwCKi4uNl7r5MmTxhlJ8vl8VjlT2dnZxplTp04ZZwoLC40zknTw4EHjTFpamnGmsbHROHP99dcbZyTp66+/Ns7YTLNKSUkxzjQ1NRlnYikpKck407NnT+NMQ0NDTNaR7G5Tc3NzTNaxuQ/Z6t+/v9Hxra2t+vzzz9t9P+9I1Apo2bJlevHFF1VTU6NRo0bp1Vdf1ZgxYy6bu1A6CQkJRgX0fQ/zLreWKZu1bNjcKW1uk806tmvFKmP7OYrVj33jfR2bUo3n+4PteYjn/cXyNkXre0RUvpO+/fbbWrBggZYsWaI9e/Zo1KhRmjx5svUjDgBA9xOVAnrppZc0Z84cPfLIIxo2bJhee+019ezZU3/5y1+isRwAoAvq9AJqamrS7t27VVJS8t9FEhNVUlKi7du3X3R8OBxWMBhsdwEAdH+dXkCnTp1Sa2ur8vLy2l2fl5enmpqai44vKyuT3+9vu/AMOAC4Njj/Q9RFixYpEAi0XaqqqlxvCQAQA53+LLicnBwlJSWptra23fW1tbXKz8+/6HifzxezpzUDAOJHpz8CSk1N1ejRo7V58+a26yKRiDZv3qxx48Z19nIAgC4qKn8HtGDBAs2aNUu33HKLxowZo5dfflmhUEiPPPJINJYDAHRBUSmg+++/X998840WL16smpoa/eAHP9DGjRsvemICAODaleDZ/MlzFAWDQfn9fuXm5hr9NXt9fb3xWrY3PTnZvLezsrKMM4FAwDgTDoeNM5FIxDgjSbm5ucaZuro644zNbbrcCJBLsfkr8TNnzhhnbO5DNiNebMe1tLS0GGfS09ONMzfddJNxZu/evcaZ3r17G2ckKRQKWeVM2fwe3Pb713XXXWeVMxGJRHT48GEFAgFlZmZe8jjnz4IDAFybKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOBEVKZhd4ZTp04ZDYY0GVx6QVpamnFGkvx+v3Hm3LlzxplYDRbt1auXcUaSGhoajDM2+0tNTTXO2Ax/laTTp08bZ2zOn83gTpvz3adPH+OMJNXU1BhnbIZj7t+/PybrNDY2Gmcku2Gkw4cPN8785z//Mc7ccMMNxhlJGjhwoHHGdADslX6d8wgIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATsTtNOy0tDSjadg+n894DZsJ2pLU1NRknKmrqzPO2OzPJmOzN0lKSkqyypn65S9/aZwZP3681Vr9+vUzzvTs2dM486c//ck4c9999xlnJkyYYJyR7KZH20wtt8n06NHDOBMIBIwzkpSdnW2cqaysNM707t3bOGM74fuLL74wzphOIL/S43kEBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOxO0w0kgkYjSMtKGhwWoNG4MHDzbO2A78NNXS0mKcsRkIKdmdv6FDhxpnqqqqjDN+v984I0m9evUyzuTm5hpn5s+fb5yxGbBqO4TThs1wzHA4bJxJTjb/tpWfn2+ckaRbbrnFOLN161bjzOnTp40zNt+HJOno0aPGmby8PKPjW1tbVVtbe9njeAQEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE4keJ7nud7E/woGg/L7/UpPTzcaRmpzM2xves+ePY0zZ86cMc7YDPuMs08nHDP5GvpfNkNZTQdWStKaNWuMM6NHjzbONDc3G2cku/Nn8zVoc+6GDRtmnJGkL774wipnIhKJ6NSpUwoEAsrMzLzkcTwCAgA4QQEBAJzo9AJ67rnnlJCQ0O5i8xowAIDuLSovSHfzzTfr448//u8iFi8gBQDo3qLSDMnJydavQAgAuDZE5XdABw8eVGFhoQYMGKCHH35Yx44du+Sx4XBYwWCw3QUA0P11egGNHTtWK1eu1MaNG7V8+XIdOXJEt99+u+rr6zs8vqysTH6/v+1SVFTU2VsCAMShTi+gqVOn6r777tPIkSM1efJkffjhh6qrq9M777zT4fGLFi1SIBBou1RVVXX2lgAAcSjqzw7IysrS4MGDdejQoQ7f7/P55PP5or0NAECcifrfATU0NKiyslIFBQXRXgoA0IV0egEtXLhQ5eXl+vrrr/Wvf/1L9957r5KSkvTggw929lIAgC6s038Ed/z4cT344IM6ffq0+vTpo/Hjx2vHjh3q06dPZy8FAOjCOr2A3nrrrU75OKFQyGgQYEpKivEaLS0txhnJbrBha2ur1VrABX6/3zjz97//3WqtiRMnGmfC4bBxZvz48cYZ28GiNmI13Pcf//iHcebuu++2Wsvm/Jmehys9nllwAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOBE1F+QzlZycrLRMNKmpqYo7qa9xER6G1fnj3/8o3Fmzpw5UdhJ53nxxReNM3v27InCTrqeQYMGGWeOHj1qtVYkEjHOZGVlRWUNvpMCAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADAibidhu15ntHxJpOzbdcAOlJaWmqcefTRR6OwE7c2btxonOnVq5dxJhgMGmfinc33L9up/Onp6caZjIwMo+MjkYjOnDlz2eN4BAQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATsTtMFKfz2c0oM9mMF9jY6NxRpKam5utcogdm+GOknT8+HHjTGFhoXEmEokYZ+JddXW1caY7Dha1YfP9y+/3W611xx13GGc2b95sdPyVDnrmERAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOBG3w0iTk5ONBkr6fD7jNcLhsHFGkpKSkowzra2txhmbgZozZswwztjsTZJeeOEF48zQoUONM5s2bTLO3HjjjcYZScrJyTHO2AwWtRnCmZWVZZyJpcOHD7veQpeVlpZmnLEdpnzo0CHjzN133210fHNzs9asWXPZ43gEBABwggICADhhXEDbtm3TtGnTVFhYqISEBK1bt67d+z3P0+LFi1VQUKC0tDSVlJTo4MGDnbVfAEA3YVxAoVBIo0aN0rJlyzp8/9KlS/XKK6/otdde086dO5Wenq7Jkydb/7wSANA9GT8JYerUqZo6dWqH7/M8Ty+//LKeeeYZ3XPPPZKkN954Q3l5eVq3bp0eeOCBq9stAKDb6NTfAR05ckQ1NTUqKSlpu87v92vs2LHavn17h5lwOKxgMNjuAgDo/jq1gGpqaiRJeXl57a7Py8tre993lZWVye/3t12Kioo6c0sAgDjl/FlwixYtUiAQaLtUVVW53hIAIAY6tYDy8/MlSbW1te2ur62tbXvfd/l8PmVmZra7AAC6v04toOLiYuXn52vz5s1t1wWDQe3cuVPjxo3rzKUAAF2c8bPgGhoa2o1yOHLkiPbt26fs7Gz169dP8+fP129/+1sNGjRIxcXFevbZZ1VYWKjp06d35r4BAF2ccQHt2rVLd911V9vbCxYskCTNmjVLK1eu1FNPPaVQKKS5c+eqrq5O48eP18aNG9WjR4/O2zUAoMtL8DzPc72J/xUMBuX3+5Wdna3ExCv/CWEgEDBeq6WlxTgTSykpKcaZwYMHG2cu9RT5y7H54+Lc3FzjjM15sMlI0vvvv2+c2bt3r3Fm9erVxpmdO3caZ2wGpUp2w33j7FtJl2Lyve4Cm2HFkt3XRnKy2WMVz/MUCoUUCAS+9/f6zp8FBwC4NlFAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOBE3E7DTkhIMJr2Gstpsn6/3zhTX19vnElNTTXOhEIh40zPnj2NM5J09uxZ40xWVpZxJiMjwzhz8uRJ44xkN+E7Vj777DPjTN++fa3WKigosMrBjs33L9uvW5v7uM007HA4zDRsAEB8ooAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATZhPmYig9Pd1oWGhKSorxGjbDNG2lpaUZZ+rq6jp/Ix1oaGiIyTqSdObMGeNMJBIxzpw7d844E0t9+vQxzgwYMMA4k52dbZzBf9kMLLaZ72w67NM2I0kDBw40zph+/2ptbdXnn39+2eN4BAQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATsTtMNKWlhajQYCNjY3Ga7S2thpnJLshobZrxUJiot3/Q2yGhNoMagyFQsaZeLdnzx7jTCwHi06dOtU489FHH0VhJ24NGzbMOFNRUWGcsRk0e/DgQeOMJNXX1xtnMjIyjI6/0q9zHgEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMJns10yCgKBoPy+/1KSEgwGkZqcuwFNsM0Y71WLAwZMsQqd/r0aePMe++9Z5y56aabjDO9evUyzkjS4cOHjTPDhw+3WisWbL+8/X6/ccZmyGW8y8rKMs4EAgHjjM1AYNvvKUlJScaZ5GSzudWe5ykcDisQCCgzM/OSx/EICADgBAUEAHDCuIC2bdumadOmqbCwUAkJCVq3bl2798+ePbvtx2cXLlOmTOms/QIAugnjAgqFQho1apSWLVt2yWOmTJmi6urqtsvq1auvapMAgO7H+BVRp06detlXS/T5fMrPz7feFACg+4vK74C2bt2q3NxcDRkyRI8//vj3PlsqHA4rGAy2uwAAur9OL6ApU6bojTfe0ObNm/X73/9e5eXlmjp1qlpbWzs8vqysTH6/v+1SVFTU2VsCAMQh4x/BXc4DDzzQ9u8RI0Zo5MiRGjhwoLZu3aqJEydedPyiRYu0YMGCtreDwSAlBADXgKg/DXvAgAHKycnRoUOHOny/z+dTZmZmuwsAoPuLegEdP35cp0+fVkFBQbSXAgB0IcY/gmtoaGj3aObIkSPat2+fsrOzlZ2dreeff14zZ85Ufn6+Kisr9dRTT+nGG2/U5MmTO3XjAICuzbiAdu3apbvuuqvt7Qu/v5k1a5aWL1+u/fv36/XXX1ddXZ0KCws1adIk/eY3v5HP5+u8XQMAury4HUYqmQ39tBnmZ5ORzAfzSdK5c+es1oqF9PR0q1xjY6Nx5ic/+Ylx5umnnzbO5OTkGGckqXfv3sYZ2/tRLDQ1NVnl/vfJRFdq7dq1VmuZshkG/MILL1ittWXLFuOMze+xt23bZpypq6szzkjS4MGDjTNHjx41Ot7zPIVCIYaRAgDiEwUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE7E7TTs6667zmjqbSgUMl6rZ8+exhnJbrJ1OBw2zsTZp6ZTXJh0bmLIkCHGmb/97W/GGUkaNGiQVa67SUlJMc60tLREYScXe//9940zti8HYzNFu6KiwjhTW1trnElKSjLOSFJWVpZxplevXkbHRyIRHTt2jGnYAID4RAEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAn4nYYaXJystEw0kgkYryW7U23GQLY3NxstVYsmJzn/2Vz/mzOXWpqqnHm7Nmzxhn8V3JysnGmtbU1Cju5WF1dnXHGZoCpJD311FPGGZvBojYSE+0eP5gOFpXMhz17nqfW1laGkQIA4hMFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnDCfOBgjCQkJUR9GajMYU7IfYhqvYnl7YjWwEufZfF1Isfs8LVy40Djj9/uNM9OmTTPOSNKuXbuMM++9955x5sSJE8YZW+Fw2DiTnp5udLzneQoGg5c9jkdAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOBEghdnkzWDwWDbsEGTYaQmx15NxjbX0tJitRbs2A6abW5uNs7Y3o9iIZ73JtkNS62trY3JOpI0YcIE40yPHj2MM4cPHzbO2H5P6dWrl3EmMzPT6PhIJKKqqioFAoHvzfIICADgBAUEAHDCqIDKysp06623KiMjQ7m5uZo+fboqKiraHdPY2KjS0lL17t1bvXr10syZM60eMgMAujejAiovL1dpaal27NihTZs2qbm5WZMmTVIoFGo75sknn9QHH3ygd999V+Xl5Tpx4oRmzJjR6RsHAHRtRq+IunHjxnZvr1y5Urm5udq9e7cmTJigQCCgP//5z1q1apV+/OMfS5JWrFihm266STt27NCPfvSjzts5AKBLu6rfAQUCAUlSdna2JGn37t1qbm5WSUlJ2zFDhw5Vv379tH379g4/RjgcVjAYbHcBAHR/1gUUiUQ0f/583XbbbRo+fLgkqaamRqmpqcrKymp3bF5enmpqajr8OGVlZfL7/W2XoqIi2y0BALoQ6wIqLS3VgQMH9NZbb13VBhYtWqRAINB2qaqquqqPBwDoGox+B3TBvHnztGHDBm3btk19+/Ztuz4/P19NTU2qq6tr9yiotrZW+fn5HX4sn88nn89nsw0AQBdm9AjI8zzNmzdPa9eu1ZYtW1RcXNzu/aNHj1ZKSoo2b97cdl1FRYWOHTumcePGdc6OAQDdgtEjoNLSUq1atUrr169XRkZG2+91/H6/0tLS5Pf79eijj2rBggXKzs5WZmamnnjiCY0bN45nwAEA2jEqoOXLl0uS7rzzznbXr1ixQrNnz5Yk/d///Z8SExM1c+ZMhcNhTZ48WX/4wx86ZbMAgO7DqICuZG5pjx49tGzZMi1btsx6U5KUnp5uNEjRZqbquXPnjDNS/A94hDRt2jSrXDx/br/7I+8rYXt7bL6ebCae2KyTnp5unFm8eLFxRpK+/fZb40xjY6PVWqZSUlKschkZGcYZ0wGmra2tV3Qcs+AAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADghNUrosZCOByO+mRi248fzxOTcd66deuscsnJsfmSsFknHA4bZxIT7f6PGYlEjDMXXh/MxJkzZ4wzW7ZsMc6sXLnSOCPFbrK13+83zgwbNsxqLZsJ3ydPnjQ6/krvPzwCAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAn4nYYaXp6utHQz5aWFuM1zp49a5yRpObmZuOMzQBTz/OMM7g6NkM4ly5dapxZuHChcSaWKisrjTPLly83zqSmphpnXn/9deNMIBAwzkh2X4M2t6lHjx7GmS+//NI4I0mhUMg4M336dKPjm5qa9M4771z2OB4BAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATcTuMtF+/fkpKSrri45uamozX+Pbbb40zkpSfn2+cqa6uNs6cOXPGONPa2hqTTLyzGf4qSaWlpcaZOXPmWK1lymYIbl5entVaGRkZxpmqqiqrtUzZfG4TE+3+r20zWHTUqFHGma+++so4U1BQYJyRpMbGRuPMhg0bjI6/0iGuPAICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACfidhhp7969lZx85dsbMmSI8Rp//etfjTOSdOeddxpnPvroI+OMzUDIw4cPG2e6oysdhvhdH374oXHmoYceMs7YDIR8/fXXjTPhcNg4I0mhUMg4YzPwM1aDcE2+l1ytG264wThjM1i0vLzcOCNJLS0txhmfz2d0PMNIAQBxjQICADhhVEBlZWW69dZblZGRodzcXE2fPl0VFRXtjrnzzjuVkJDQ7vLYY4916qYBAF2fUQGVl5ertLRUO3bs0KZNm9Tc3KxJkyZd9PPiOXPmqLq6uu2ydOnSTt00AKDrM/rN3MaNG9u9vXLlSuXm5mr37t2aMGFC2/U9e/a0etVQAMC146p+BxQIBCRJ2dnZ7a5/8803lZOTo+HDh2vRokU6e/bsJT9GOBxWMBhsdwEAdH/Wz02MRCKaP3++brvtNg0fPrzt+oceekj9+/dXYWGh9u/fr6effloVFRVas2ZNhx+nrKxMzz//vO02AABdlHUBlZaW6sCBA/r000/bXT937ty2f48YMUIFBQWaOHGiKisrNXDgwIs+zqJFi7RgwYK2t4PBoIqKimy3BQDoIqwKaN68edqwYYO2bdumvn37fu+xY8eOlSQdOnSowwLy+XzGf+QEAOj6jArI8zw98cQTWrt2rbZu3ari4uLLZvbt2yfJ7i99AQDdl1EBlZaWatWqVVq/fr0yMjJUU1MjSfL7/UpLS1NlZaVWrVqlu+++W71799b+/fv15JNPasKECRo5cmRUbgAAoGsyKqDly5dLungW2ooVKzR79mylpqbq448/1ssvv6xQKKSioiLNnDlTzzzzTKdtGADQPRj/CO77FBUVWQ/IAwBcWxI827HBURIMBuX3+zVo0CAlJSVdcc5m2q3t5Oi0tDTjTENDg3GmubnZOJOQkGCc6dOnj3FGspuYXF9fb7VWrJjc566GzefJZoqxzTqSlJKSYpyx2V8kEjHOZGZmxmQdScrKyjLO1NXVGWfS09ONM7179zbOSHZTy8+cOWN0fCQSUXV1tQKBwPd+vhhGCgBwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOWL8kd7QdO3bMaJCizRDJ7Oxs44wkBQIB40xOTo5x5ttvvzXONDY2Gme++eYb44wkpaamGmdshmPafG5th4raDIC1WctmcKfNgNDW1lbjjO1aAwYMMM7YDARuamoyzvj9fuOMdPlXAOiIzX3c5ms9GAwaZyQpIyPDOGP6uW1paVF1dfVlj+MREADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcCLuZsFdmL1kOoPJZmZTJBIxzsRyLZt1YpWJ5VrcpthmbHM2c+didZtsv9a749etzW0ynV144b5wuT0meLa3IkqOHz+uoqIi19sAAFylqqoq9e3b95Lvj7sCikQiOnHihDIyMi6aKhsMBlVUVKSqqiplZmY62qF7nIfzOA/ncR7O4zycFw/nwfM81dfXq7CwUImJl/5NT9z9CC4xMfF7G1OSMjMzr+k72AWch/M4D+dxHs7jPJzn+jxcyUtg8CQEAIATFBAAwIkuVUA+n09LliyRz+dzvRWnOA/ncR7O4zycx3k4ryudh7h7EgIA4NrQpR4BAQC6DwoIAOAEBQQAcIICAgA40WUKaNmyZbrhhhvUo0cPjR07Vp999pnrLcXcc889p4SEhHaXoUOHut5W1G3btk3Tpk1TYWGhEhIStG7dunbv9zxPixcvVkFBgdLS0lRSUqKDBw+62WwUXe48zJ49+6L7x5QpU9xsNkrKysp06623KiMjQ7m5uZo+fboqKiraHdPY2KjS0lL17t1bvXr10syZM1VbW+tox9FxJefhzjvvvOj+8Nhjjznacce6RAG9/fbbWrBggZYsWaI9e/Zo1KhRmjx5sk6ePOl6azF38803q7q6uu3y6aefut5S1IVCIY0aNUrLli3r8P1Lly7VK6+8otdee007d+5Uenq6Jk+erMbGxhjvNLoudx4kacqUKe3uH6tXr47hDqOvvLxcpaWl2rFjhzZt2qTm5mZNmjRJoVCo7Zgnn3xSH3zwgd59912Vl5frxIkTmjFjhsNdd74rOQ+SNGfOnHb3h6VLlzra8SV4XcCYMWO80tLStrdbW1u9wsJCr6yszOGuYm/JkiXeqFGjXG/DKUne2rVr296ORCJefn6+9+KLL7ZdV1dX5/l8Pm/16tUOdhgb3z0Pnud5s2bN8u655x4n+3Hl5MmTniSvvLzc87zzn/uUlBTv3XffbTvmq6++8iR527dvd7XNqPvuefA8z7vjjju8n/3sZ+42dQXi/hFQU1OTdu/erZKSkrbrEhMTVVJSou3btzvcmRsHDx5UYWGhBgwYoIcffljHjh1zvSWnjhw5opqamnb3D7/fr7Fjx16T94+tW7cqNzdXQ4YM0eOPP67Tp0+73lJUBQIBSVJ2drYkaffu3Wpubm53fxg6dKj69evXre8P3z0PF7z55pvKycnR8OHDtWjRIp09e9bF9i4p7oaRftepU6fU2tqqvLy8dtfn5eXp3//+t6NduTF27FitXLlSQ4YMUXV1tZ5//nndfvvtOnDggDIyMlxvz4mamhpJ6vD+ceF914opU6ZoxowZKi4uVmVlpX71q19p6tSp2r59u5KSklxvr9NFIhHNnz9ft912m4YPHy7p/P0hNTVVWVlZ7Y7tzveHjs6DJD300EPq37+/CgsLtX//fj399NOqqKjQmjVrHO62vbgvIPzX1KlT2/49cuRIjR07Vv3799c777yjRx991OHOEA8eeOCBtn+PGDFCI0eO1MCBA7V161ZNnDjR4c6io7S0VAcOHLgmfg/6fS51HubOndv27xEjRqigoEATJ05UZWWlBg4cGOttdijufwSXk5OjpKSki57FUltbq/z8fEe7ig9ZWVkaPHiwDh065Horzly4D3D/uNiAAQOUk5PTLe8f8+bN04YNG/TJJ5+0e/mW/Px8NTU1qa6urt3x3fX+cKnz0JGxY8dKUlzdH+K+gFJTUzV69Ght3ry57bpIJKLNmzdr3LhxDnfmXkNDgyorK1VQUOB6K84UFxcrPz+/3f0jGAxq586d1/z94/jx4zp9+nS3un94nqd58+Zp7dq12rJli4qLi9u9f/To0UpJSWl3f6ioqNCxY8e61f3hcuehI/v27ZOk+Lo/uH4WxJV46623PJ/P561cudL78ssvvblz53pZWVleTU2N663F1M9//nNv69at3pEjR7x//vOfXklJiZeTk+OdPHnS9daiqr6+3tu7d6+3d+9eT5L30ksveXv37vWOHj3qeZ7n/e53v/OysrK89evXe/v37/fuuecer7i42Dt37pzjnXeu7zsP9fX13sKFC73t27d7R44c8T7++GPvhz/8oTdo0CCvsbHR9dY7zeOPP+75/X5v69atXnV1ddvl7Nmzbcc89thjXr9+/bwtW7Z4u3bt8saNG+eNGzfO4a473+XOw6FDh7xf//rX3q5du7wjR45469ev9wYMGOBNmDDB8c7b6xIF5Hme9+qrr3r9+vXzUlNTvTFjxng7duxwvaWYu//++72CggIvNTXVu/76673777/fO3TokOttRd0nn3ziSbroMmvWLM/zzj8V+9lnn/Xy8vI8n8/nTZw40auoqHC76Sj4vvNw9uxZb9KkSV6fPn28lJQUr3///t6cOXO63X/SOrr9krwVK1a0HXPu3Dnvpz/9qXfdddd5PXv29O69916vurra3aaj4HLn4dixY96ECRO87Oxsz+fzeTfeeKP3i1/8wgsEAm43/h28HAMAwIm4/x0QAKB7ooAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIAT/w8/TRQmyDZiaQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(x_train2))"
      ],
      "metadata": {
        "id": "fhuRFc0ro5T_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}