{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOko51Vap7de58bTo0cUSQ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/wganworkking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xI7rEeMMgDQI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgyl_WtbgO5k",
        "outputId": "b8fe388b-053f-4353-a985-e218964cd9cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7,activation='LeakyReLU',use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(14*14,activation='LeakyReLU',use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(28*28,activation='LeakyReLU',use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(28*28,activation='sigmoid',use_bias=False))\n",
        "    model.add(layers.Reshape((28,28,1)))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_discriminator_model():\n",
        "    input_img = layers.Input(shape=(28,28,1))\n",
        "\n",
        "    x = layers.Conv2D(64,3,2,padding='same',activation='LeakyReLU')(input_img)\n",
        "\n",
        "    x = layers.Dropout(.3)(x)\n",
        "\n",
        "    x = layers.Conv2D(64,3,2,padding='same',activation='LeakyReLU')(x)\n",
        "\n",
        "    x = layers.Dropout(.3)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    dense_output = layers.Dense(128, activation='LeakyReLU')(x)\n",
        "\n",
        "\n",
        "    x = layers.Dropout(.3)(x)\n",
        "\n",
        "    dense_output = layers.Dense(64, activation='LeakyReLU')(x)\n",
        "\n",
        "    x = layers.Dropout(.3)(x)\n",
        "\n",
        "    dense_output = layers.Dense(1, activation=None)(dense_output)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=input_img, outputs=dense_output)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "kik7wQ1qgOV-"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "qzdSabqdVu_H"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator_model()\n",
        "discriminatorW = make_discriminator_model()\n",
        "discriminatorU = make_discriminator_model()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0004)\n",
        "discriminatorW_optimizer = tf.keras.optimizers.Adam(0.0004)\n",
        "discriminatorU_optimizer = tf.keras.optimizers.Adam(0.0004)"
      ],
      "metadata": {
        "id": "lS0DF8rjhHdY"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 10\n",
        "\n",
        "#@tf.function\n",
        "def discriminator_lossW(real_output, fake_output):\n",
        "    real_loss = tf.reduce_mean(real_output)\n",
        "    fake_loss = tf.reduce_mean(fake_output)\n",
        "    total_loss = fake_loss - real_loss\n",
        "    return total_loss\n",
        "\n",
        "#@tf.function\n",
        "def generator_lossW(fake_output):\n",
        "    return -tf.reduce_mean(fake_output)\n",
        "\n",
        "#@tf.function\n",
        "def gradient_penalty(real_images, fake_images):\n",
        "    alpha = tf.random.uniform([BATCH_SIZE, 1, 1, 1], 0., 1.)\n",
        "    real_images, fake_images = tf.cast(real_images, tf.float32), tf.cast(fake_images, tf.float32)\n",
        "    interpolated_images = alpha * real_images + ((1 - alpha) * fake_images)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated_images)\n",
        "        pred = discriminatorW(interpolated_images, training=True)\n",
        "    gradients = tape.gradient(pred, [interpolated_images])[0]\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "    gp = tf.reduce_mean((norm - 1.)**2)\n",
        "    return gp"
      ],
      "metadata": {
        "id": "DYQpIZyEgSlN"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NOISE_DIM = 100\n",
        "GP_WEIGHT = 100\n",
        "\n",
        "\n",
        "#@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
        "\n",
        "    for i in range(5):\n",
        "      with tf.GradientTape() as disc_tapeU:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_outputU = discriminatorU(images, training=True)\n",
        "        fake_outputU = discriminatorU(generated_images, training=True)\n",
        "        disc_lossU = cross_entropy(tf.ones_like(real_outputU), real_outputU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      gradients_of_discriminatorU = disc_tapeU.gradient(disc_lossU, discriminatorU.trainable_variables)\n",
        "      discriminatorU_optimizer.apply_gradients(zip(gradients_of_discriminatorU, discriminatorU.trainable_variables))\n",
        "      if i == 0:\n",
        "        weights = discriminatorU.get_weights()\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tapeW:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_outputW = discriminatorW(images, training=True)\n",
        "        fake_outputW = discriminatorW(generated_images, training=True)\n",
        "        real_outputU = discriminatorU(images, training=True)\n",
        "        fake_outputU = discriminatorU(generated_images, training=True)\n",
        "        disc_lossW = discriminator_lossW(real_outputW, fake_outputW)\n",
        "\n",
        "\n",
        "        gen_lossU = cross_entropy(tf.ones_like(fake_outputU), fake_outputU)\n",
        "        gen_lossW = generator_lossW(fake_outputW)\n",
        "        gen_loss = gen_lossU+gen_lossW\n",
        "\n",
        "        gp = gradient_penalty(images, generated_images)\n",
        "        disc_lossW += gp * GP_WEIGHT\n",
        "\n",
        "\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    gradients_of_discriminatorW = disc_tapeW.gradient(disc_lossW, discriminatorW.trainable_variables)\n",
        "    discriminatorW_optimizer.apply_gradients(zip(gradients_of_discriminatorW, discriminatorW.trainable_variables))\n",
        "\n",
        "    discriminatorU.set_weights(weights)\n",
        "\n",
        "    tf.print(\"disc_lossW\",disc_lossW,'disc_lossU',disc_lossU,'gen_lossU',gen_lossU,'gen_lossW',gen_lossW)"
      ],
      "metadata": {
        "id": "cP3-wZztEeaE"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ihRh4v9euLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for batch in range(len(dataset) // BATCH_SIZE):\n",
        "\n",
        "            target_images = dataset[batch * BATCH_SIZE: (batch+1) * BATCH_SIZE]\n",
        "\n",
        "\n",
        "            train_step(target_images)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      print(epoch)\n",
        "\n"
      ],
      "metadata": {
        "id": "Sx_KbIfjiogE"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "x_train2 = np.expand_dims(x_train, axis=-1)\n",
        "x_train2 = (x_train2 - np.min(x_train2)) / (np.max(x_train2) - np.min(x_train2))\n",
        "train(x_train2, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IWp_3RkPg248",
        "outputId": "54758da5-5d50-4ec6-956f-f5a0642b382f"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "disc_lossW 60.7172966 disc_lossU 0.452938259 gen_lossU 0.274308413 gen_lossW 0.0259488858\n",
            "disc_lossW 56.642292 disc_lossU 0.403111875 gen_lossU 0.16581893 gen_lossW 0.0293054618\n",
            "disc_lossW 54.6427765 disc_lossU 0.307659805 gen_lossU 0.0919928774 gen_lossW 0.152300537\n",
            "disc_lossW 50.1189384 disc_lossU 0.256576687 gen_lossU 0.0341245234 gen_lossW 0.1446466\n",
            "disc_lossW 46.3003693 disc_lossU 0.230242893 gen_lossU 0.0195491202 gen_lossW 0.228560761\n",
            "disc_lossW 42.0719833 disc_lossU 0.129773229 gen_lossU 0.00593155902 gen_lossW 0.206511185\n",
            "disc_lossW 36.874218 disc_lossU 0.111255862 gen_lossU 0.00178138562 gen_lossW 0.228810832\n",
            "disc_lossW 32.385 disc_lossU 0.0773154944 gen_lossU 0.000628498557 gen_lossW 0.185133785\n",
            "disc_lossW 26.9661484 disc_lossU 0.0469269641 gen_lossU 0.000285636459 gen_lossW 0.389571726\n",
            "disc_lossW 23.7707176 disc_lossU 0.0426990502 gen_lossU 0.000120831872 gen_lossW 0.334014684\n",
            "disc_lossW 17.2415142 disc_lossU 0.0511624441 gen_lossU 4.86359058e-05 gen_lossW 0.402810425\n",
            "disc_lossW 14.7089653 disc_lossU 0.0196202956 gen_lossU 2.29518409e-05 gen_lossW 0.558194\n",
            "disc_lossW 8.94050694 disc_lossU 0.0220093522 gen_lossU 1.53406691e-05 gen_lossW 0.665594697\n",
            "disc_lossW 8.30835056 disc_lossU 0.0159961451 gen_lossU 9.31025716e-06 gen_lossW 0.699061692\n",
            "disc_lossW 4.71312761 disc_lossU 0.0182784908 gen_lossU 6.17227352e-06 gen_lossW 0.876843452\n",
            "disc_lossW 2.35054445 disc_lossU 0.0158835147 gen_lossU 2.21570758e-06 gen_lossW 0.991029859\n",
            "disc_lossW 0.382121444 disc_lossU 0.0054652933 gen_lossU 2.30780893e-06 gen_lossW 1.16377\n",
            "disc_lossW -0.572144866 disc_lossU 0.0111340638 gen_lossU 2.0329112e-06 gen_lossW 1.20279336\n",
            "disc_lossW 0.611259 disc_lossU 0.00847826432 gen_lossU 1.62397919e-06 gen_lossW 1.45021093\n",
            "disc_lossW -0.106133878 disc_lossU 0.00461879792 gen_lossU 9.71884e-07 gen_lossW 1.63724554\n",
            "disc_lossW 1.17439759 disc_lossU 0.00655851141 gen_lossU 1.18495223e-06 gen_lossW 1.7289536\n",
            "disc_lossW 2.84424257 disc_lossU 0.00629907241 gen_lossU 7.53246127e-07 gen_lossW 1.93774\n",
            "disc_lossW 2.51611519 disc_lossU 0.00413155789 gen_lossU 8.59249838e-07 gen_lossW 1.78038096\n",
            "disc_lossW 3.60082817 disc_lossU 0.00570451 gen_lossU 5.55695522e-07 gen_lossW 2.04553962\n",
            "disc_lossW 2.20386744 disc_lossU 0.00296620326 gen_lossU 4.5882993e-07 gen_lossW 2.12387037\n",
            "disc_lossW 2.83426881 disc_lossU 0.00468606967 gen_lossU 3.11264699e-07 gen_lossW 2.58789015\n",
            "disc_lossW 3.06307697 disc_lossU 0.00594199076 gen_lossU 2.58567241e-07 gen_lossW 2.39805269\n",
            "disc_lossW 0.187570095 disc_lossU 0.00842562597 gen_lossU 1.52623826e-07 gen_lossW 2.68225837\n",
            "disc_lossW 0.65270257 disc_lossU 0.00407371251 gen_lossU 1.38345541e-07 gen_lossW 2.60715389\n",
            "disc_lossW -0.461357594 disc_lossU 0.00282436842 gen_lossU 1.26548088e-07 gen_lossW 2.86759901\n",
            "disc_lossW -1.69356596 disc_lossU 0.00313509488 gen_lossU 7.56961782e-08 gen_lossW 2.73846245\n",
            "disc_lossW -1.71244979 disc_lossU 0.00464703422 gen_lossU 5.16722487e-08 gen_lossW 2.74993491\n",
            "disc_lossW -1.8526051 disc_lossU 0.000878593302 gen_lossU 7.49069073e-08 gen_lossW 3.02344847\n",
            "disc_lossW -2.4011488 disc_lossU 0.00190198678 gen_lossU 4.82643365e-08 gen_lossW 3.20952272\n",
            "disc_lossW -2.51200962 disc_lossU 0.00215098215 gen_lossU 4.26591704e-08 gen_lossW 3.25456357\n",
            "disc_lossW -2.64491892 disc_lossU 0.00335516268 gen_lossU 4.14785575e-08 gen_lossW 3.24554753\n",
            "disc_lossW -2.97078681 disc_lossU 0.00289539574 gen_lossU 2.37943052e-08 gen_lossW 3.41171885\n",
            "disc_lossW -2.19052696 disc_lossU 0.001945636 gen_lossU 2.8544628e-08 gen_lossW 3.31257319\n",
            "disc_lossW -2.88831711 disc_lossU 0.00225102203 gen_lossU 1.53227706e-08 gen_lossW 3.92928123\n",
            "disc_lossW -3.52591252 disc_lossU 0.00271129934 gen_lossU 1.62433409e-08 gen_lossW 4.06564808\n",
            "disc_lossW -3.3097291 disc_lossU 0.00217259442 gen_lossU 8.65710525e-09 gen_lossW 4.16298676\n",
            "disc_lossW -3.97173071 disc_lossU 0.00333604217 gen_lossU 8.29317592e-09 gen_lossW 4.61522579\n",
            "disc_lossW -2.84086156 disc_lossU 0.000982502708 gen_lossU 1.29826194e-08 gen_lossW 4.81610489\n",
            "disc_lossW -3.32811379 disc_lossU 0.00162554067 gen_lossU 1.45541534e-08 gen_lossW 4.69857597\n",
            "disc_lossW -2.97032738 disc_lossU 0.00179101655 gen_lossU 5.43457901e-09 gen_lossW 5.53126\n",
            "disc_lossW -4.57211876 disc_lossU 0.00197775662 gen_lossU 5.55449287e-09 gen_lossW 5.86669064\n",
            "disc_lossW -2.21446562 disc_lossU 0.00204733969 gen_lossU 5.28822719e-09 gen_lossW 6.40366077\n",
            "disc_lossW -3.23660421 disc_lossU 0.00151817128 gen_lossU 3.038092e-09 gen_lossW 6.4359107\n",
            "disc_lossW -4.8146286 disc_lossU 0.00140153116 gen_lossU 3.39274342e-09 gen_lossW 6.37649393\n",
            "disc_lossW -4.65001488 disc_lossU 0.000829975761 gen_lossU 3.93550748e-09 gen_lossW 6.67554188\n",
            "disc_lossW -5.5880127 disc_lossU 0.00138862862 gen_lossU 2.06423811e-09 gen_lossW 7.01394796\n",
            "disc_lossW -4.95144606 disc_lossU 0.00101309293 gen_lossU 2.13925588e-09 gen_lossW 7.23812962\n",
            "disc_lossW -4.04010296 disc_lossU 0.000569936 gen_lossU 1.90106886e-09 gen_lossW 7.37512112\n",
            "disc_lossW -6.12336302 disc_lossU 0.00126866752 gen_lossU 1.85537741e-09 gen_lossW 7.46461248\n",
            "disc_lossW -5.70514774 disc_lossU 0.000602643 gen_lossU 1.61436842e-09 gen_lossW 7.86365414\n",
            "disc_lossW -6.25452614 disc_lossU 0.000989849446 gen_lossU 1.05106257e-09 gen_lossW 7.94229126\n",
            "disc_lossW -3.93726778 disc_lossU 0.000851814286 gen_lossU 1.2078416e-09 gen_lossW 7.89050388\n",
            "disc_lossW -3.50177169 disc_lossU 0.00125062 gen_lossU 1.57689573e-09 gen_lossW 7.69523478\n",
            "disc_lossW -4.43258476 disc_lossU 0.00120186585 gen_lossU 9.66499547e-10 gen_lossW 7.9795351\n",
            "disc_lossW -6.51947498 disc_lossU 0.000644214917 gen_lossU 8.12521606e-10 gen_lossW 7.75634\n",
            "disc_lossW -6.46804047 disc_lossU 0.00168980099 gen_lossU 1.12890097e-09 gen_lossW 7.34926462\n",
            "disc_lossW -4.43132257 disc_lossU 0.00079608243 gen_lossU 7.49296347e-10 gen_lossW 7.89211\n",
            "disc_lossW -6.07916641 disc_lossU 0.000418789859 gen_lossU 7.12387815e-10 gen_lossW 7.58776617\n",
            "disc_lossW -6.18856049 disc_lossU 0.00151704322 gen_lossU 8.0993684e-10 gen_lossW 7.64140034\n",
            "disc_lossW -5.74496794 disc_lossU 0.00109234825 gen_lossU 4.83943385e-10 gen_lossW 7.83966064\n",
            "disc_lossW -6.57315302 disc_lossU 0.000295397651 gen_lossU 4.51426091e-10 gen_lossW 8.04373932\n",
            "disc_lossW -6.77162313 disc_lossU 0.000330750074 gen_lossU 4.01296607e-10 gen_lossW 7.98660135\n",
            "disc_lossW -6.88053083 disc_lossU 0.00107019 gen_lossU 3.11658394e-10 gen_lossW 8.1445179\n",
            "disc_lossW -5.7303524 disc_lossU 0.000649798487 gen_lossU 3.829915e-10 gen_lossW 8.03126431\n",
            "disc_lossW -6.86019325 disc_lossU 0.000297948893 gen_lossU 3.00300423e-10 gen_lossW 8.48326874\n",
            "disc_lossW -7.01556349 disc_lossU 0.000180849311 gen_lossU 3.51384283e-10 gen_lossW 8.37587547\n",
            "disc_lossW -6.54342794 disc_lossU 0.000456023554 gen_lossU 1.93168218e-10 gen_lossW 8.64933205\n",
            "disc_lossW -6.59462 disc_lossU 0.000289024028 gen_lossU 4.74448925e-10 gen_lossW 8.54049492\n",
            "disc_lossW -7.32350206 disc_lossU 0.000750115491 gen_lossU 3.7084652e-10 gen_lossW 9.05037117\n",
            "disc_lossW -7.02951908 disc_lossU 0.000370671944 gen_lossU 8.50523652e-10 gen_lossW 8.93132591\n",
            "disc_lossW -5.85028172 disc_lossU 0.000477844791 gen_lossU 2.71809047e-10 gen_lossW 9.03610516\n",
            "disc_lossW -7.45575 disc_lossU 0.000215176 gen_lossU 1.81740109e-10 gen_lossW 8.73340416\n",
            "disc_lossW -6.26948166 disc_lossU 0.000763552845 gen_lossU 2.86410173e-10 gen_lossW 8.46590328\n",
            "disc_lossW -7.02388048 disc_lossU 0.00066138024 gen_lossU 2.41271975e-10 gen_lossW 7.94645166\n",
            "disc_lossW -6.91857481 disc_lossU 0.000153531597 gen_lossU 1.9518194e-10 gen_lossW 7.95591974\n",
            "disc_lossW -7.89845419 disc_lossU 0.00038939956 gen_lossU 1.67350744e-10 gen_lossW 8.32243156\n",
            "disc_lossW -7.18717 disc_lossU 7.32777262e-05 gen_lossU 1.57887065e-10 gen_lossW 8.26173592\n",
            "disc_lossW -7.30604 disc_lossU 0.00060002628 gen_lossU 1.3207635e-10 gen_lossW 8.27407646\n",
            "disc_lossW -7.97694635 disc_lossU 0.000302952307 gen_lossU 1.73971088e-10 gen_lossW 8.70711708\n",
            "disc_lossW -8.033638 disc_lossU 6.56363845e-05 gen_lossU 2.05982828e-10 gen_lossW 9.20390129\n",
            "disc_lossW -7.881742 disc_lossU 0.00026299624 gen_lossU 2.08533885e-10 gen_lossW 9.79205894\n",
            "disc_lossW -8.1505 disc_lossU 0.000412014837 gen_lossU 2.12879603e-10 gen_lossW 9.68769646\n",
            "disc_lossW -7.00405264 disc_lossU 0.000405774073 gen_lossU 1.08505718e-10 gen_lossW 9.87784\n",
            "disc_lossW -7.70508909 disc_lossU 0.000141143377 gen_lossU 2.78469525e-10 gen_lossW 10.2315779\n",
            "disc_lossW -8.49108219 disc_lossU 0.000254460407 gen_lossU 9.60926547e-11 gen_lossW 9.73396873\n",
            "disc_lossW -8.43303394 disc_lossU 0.000365849817 gen_lossU 1.34714795e-10 gen_lossW 9.88068771\n",
            "disc_lossW -7.94736719 disc_lossU 0.000202149618 gen_lossU 1.241236e-10 gen_lossW 9.42528343\n",
            "disc_lossW -7.79123 disc_lossU 0.000362498598 gen_lossU 2.03901826e-10 gen_lossW 9.1343\n",
            "disc_lossW -8.66893673 disc_lossU 0.00034395518 gen_lossU 1.04774647e-10 gen_lossW 9.44133186\n",
            "disc_lossW -7.60002518 disc_lossU 0.000435834576 gen_lossU 1.15276025e-10 gen_lossW 9.26371193\n",
            "disc_lossW -8.69166374 disc_lossU 0.000136884802 gen_lossU 2.30946234e-10 gen_lossW 9.85997581\n",
            "disc_lossW -8.47421932 disc_lossU 0.000366289227 gen_lossU 9.5177824e-11 gen_lossW 9.71998787\n",
            "disc_lossW -9.02592278 disc_lossU 0.000343737658 gen_lossU 9.72906408e-11 gen_lossW 10.0922756\n",
            "disc_lossW -8.62941456 disc_lossU 0.000150775231 gen_lossU 1.21882296e-10 gen_lossW 9.9855442\n",
            "disc_lossW -8.80453205 disc_lossU 0.000338284 gen_lossU 1.25097196e-10 gen_lossW 10.3018465\n",
            "disc_lossW -8.91799927 disc_lossU 0.000758432434 gen_lossU 8.65571365e-11 gen_lossW 10.5226135\n",
            "disc_lossW -8.55751801 disc_lossU 0.000336092897 gen_lossU 7.51913115e-11 gen_lossW 11.1608725\n",
            "disc_lossW -8.93285656 disc_lossU 0.000496945 gen_lossU 9.14888373e-11 gen_lossW 10.7655602\n",
            "disc_lossW -8.38819218 disc_lossU 0.000636731624 gen_lossU 7.74068448e-11 gen_lossW 10.3241854\n",
            "disc_lossW -8.57476521 disc_lossU 0.000249483157 gen_lossU 9.64035518e-11 gen_lossW 11.0189104\n",
            "disc_lossW -9.03530788 disc_lossU 0.000321482017 gen_lossU 1.61781727e-10 gen_lossW 10.5321321\n",
            "disc_lossW -8.83906746 disc_lossU 0.000474603294 gen_lossU 9.26894672e-11 gen_lossW 9.9464817\n",
            "disc_lossW -8.20636 disc_lossU 0.000164589481 gen_lossU 6.95858787e-11 gen_lossW 9.77819347\n",
            "disc_lossW -8.92756939 disc_lossU 0.00116533332 gen_lossU 5.8568983e-11 gen_lossW 9.66525841\n",
            "disc_lossW -8.67218494 disc_lossU 0.000253779755 gen_lossU 7.56478491e-11 gen_lossW 9.9901371\n",
            "disc_lossW -8.82104683 disc_lossU 7.10148379e-05 gen_lossU 5.24805858e-11 gen_lossW 10.1500444\n",
            "disc_lossW -9.1001215 disc_lossU 0.000340331637 gen_lossU 1.11759116e-10 gen_lossW 10.546463\n",
            "disc_lossW -9.59202766 disc_lossU 0.000288843556 gen_lossU 8.7916438e-11 gen_lossW 11.1933107\n",
            "disc_lossW -9.07122 disc_lossU 0.000431340653 gen_lossU 6.71259298e-11 gen_lossW 11.3049898\n",
            "disc_lossW -8.72570229 disc_lossU 9.82911588e-05 gen_lossU 6.6943992e-11 gen_lossW 11.0929518\n",
            "disc_lossW -8.66437721 disc_lossU 0.000249723438 gen_lossU 2.19686186e-10 gen_lossW 11.3269844\n",
            "disc_lossW -9.30205345 disc_lossU 0.00034396915 gen_lossU 8.99948e-11 gen_lossW 11.2851667\n",
            "disc_lossW -9.62699223 disc_lossU 0.000153731831 gen_lossU 7.59457913e-11 gen_lossW 11.4339247\n",
            "disc_lossW -8.38226128 disc_lossU 0.000127795545 gen_lossU 1.64803129e-10 gen_lossW 10.6470509\n",
            "disc_lossW -8.87186527 disc_lossU 0.000148336301 gen_lossU 7.42918782e-11 gen_lossW 10.6351242\n",
            "disc_lossW -8.8159132 disc_lossU 5.8316371e-05 gen_lossU 6.47088702e-11 gen_lossW 10.798399\n",
            "disc_lossW -8.74646473 disc_lossU 4.89019658e-05 gen_lossU 9.11223527e-11 gen_lossW 10.6886387\n",
            "disc_lossW -9.21468 disc_lossU 7.39595271e-05 gen_lossU 1.14039132e-10 gen_lossW 10.986867\n",
            "disc_lossW -9.2579689 disc_lossU 0.000160015115 gen_lossU 7.48430415e-11 gen_lossW 11.2472477\n",
            "disc_lossW -8.84479 disc_lossU 0.000279281754 gen_lossU 1.43857912e-10 gen_lossW 11.4625549\n",
            "disc_lossW -8.70380211 disc_lossU 0.000109423549 gen_lossU 1.28501126e-10 gen_lossW 10.9076567\n",
            "disc_lossW -8.67424679 disc_lossU 1.26537607e-05 gen_lossU 9.94450078e-10 gen_lossW 11.3478146\n",
            "disc_lossW -8.38770485 disc_lossU 5.43071728e-05 gen_lossU 2.78933238e-10 gen_lossW 10.653964\n",
            "disc_lossW -9.14009094 disc_lossU 0.000138374424 gen_lossU 2.20746e-10 gen_lossW 10.8117123\n",
            "disc_lossW -8.63453293 disc_lossU 1.52297944e-05 gen_lossU 2.32218772e-10 gen_lossW 10.7283983\n",
            "disc_lossW -8.57068634 disc_lossU 5.28619639e-05 gen_lossU 1.51771123e-10 gen_lossW 9.95375\n",
            "disc_lossW -8.84017944 disc_lossU 3.00698121e-05 gen_lossU 2.52595667e-10 gen_lossW 10.3338881\n",
            "disc_lossW -8.50716114 disc_lossU 7.84224103e-05 gen_lossU 1.70497325e-10 gen_lossW 10.1029291\n",
            "disc_lossW -9.28822613 disc_lossU 5.29317658e-05 gen_lossU 2.09498946e-10 gen_lossW 10.6063662\n",
            "disc_lossW -8.62029648 disc_lossU 4.58262875e-05 gen_lossU 2.28737945e-10 gen_lossW 10.2189589\n",
            "disc_lossW -8.62429142 disc_lossU 8.59276188e-05 gen_lossU 1.93848049e-10 gen_lossW 10.0832548\n",
            "disc_lossW -8.07816792 disc_lossU 2.52985283e-05 gen_lossU 2.58563421e-10 gen_lossW 10.0256147\n",
            "disc_lossW -8.77410507 disc_lossU 7.63430653e-05 gen_lossU 1.26606031e-10 gen_lossW 9.96011543\n",
            "disc_lossW -8.44874763 disc_lossU 6.65710331e-06 gen_lossU 2.71282607e-10 gen_lossW 10.0166807\n",
            "disc_lossW -8.73734093 disc_lossU 9.45398569e-05 gen_lossU 1.80389204e-10 gen_lossW 9.76109791\n",
            "disc_lossW -8.45881844 disc_lossU 1.75119785e-05 gen_lossU 1.70169476e-10 gen_lossW 9.45049286\n",
            "disc_lossW -8.60381794 disc_lossU 8.73627869e-05 gen_lossU 1.14070101e-10 gen_lossW 9.49586201\n",
            "disc_lossW -8.24231243 disc_lossU 0.000108229113 gen_lossU 4.34757785e-10 gen_lossW 9.55813599\n",
            "disc_lossW -8.04399 disc_lossU 7.75233639e-05 gen_lossU 8.53233484e-10 gen_lossW 9.58457\n",
            "disc_lossW -7.95945597 disc_lossU 7.01507161e-05 gen_lossU 7.90961852e-10 gen_lossW 9.76090813\n",
            "disc_lossW -8.87311459 disc_lossU 0.000209303456 gen_lossU 2.67560751e-10 gen_lossW 9.35263348\n",
            "disc_lossW -7.97362614 disc_lossU 0.000111577552 gen_lossU 9.32139921e-10 gen_lossW 9.12235641\n",
            "disc_lossW -7.74616671 disc_lossU 0.00011400415 gen_lossU 1.24745533e-10 gen_lossW 8.79992485\n",
            "disc_lossW -8.02554512 disc_lossU 0.000129568682 gen_lossU 4.18610036e-10 gen_lossW 9.31982899\n",
            "disc_lossW -8.01619339 disc_lossU 0.000150826483 gen_lossU 2.47056953e-09 gen_lossW 9.20186806\n",
            "disc_lossW -8.40024948 disc_lossU 9.4005838e-05 gen_lossU 9.3939e-10 gen_lossW 9.21936131\n",
            "disc_lossW -8.10295296 disc_lossU 5.76465027e-05 gen_lossU 2.28144753e-10 gen_lossW 9.16499901\n",
            "disc_lossW -8.42488098 disc_lossU 0.000220289716 gen_lossU 5.57924373e-10 gen_lossW 9.35972309\n",
            "disc_lossW -7.91358614 disc_lossU 5.72089339e-05 gen_lossU 4.33293845e-10 gen_lossW 9.44579506\n",
            "disc_lossW -8.35864449 disc_lossU 0.000166497339 gen_lossU 1.30100197e-09 gen_lossW 9.18640137\n",
            "disc_lossW -8.1534605 disc_lossU 9.82367e-05 gen_lossU 5.59441549e-10 gen_lossW 9.40844822\n",
            "disc_lossW -7.55266237 disc_lossU 5.26244476e-05 gen_lossU 4.88950214e-10 gen_lossW 9.34482765\n",
            "disc_lossW -7.95219135 disc_lossU 0.000178734073 gen_lossU 9.65819202e-10 gen_lossW 9.3632021\n",
            "disc_lossW -7.75991869 disc_lossU 2.62055364e-05 gen_lossU 3.31622146e-10 gen_lossW 9.315\n",
            "disc_lossW -7.69798565 disc_lossU 6.96712887e-05 gen_lossU 1.23757948e-09 gen_lossW 9.3115139\n",
            "disc_lossW -7.25955868 disc_lossU 0.000105990795 gen_lossU 5.89081228e-10 gen_lossW 9.00267792\n",
            "disc_lossW -7.02568626 disc_lossU 0.000101411657 gen_lossU 1.97833111e-10 gen_lossW 8.77953529\n",
            "disc_lossW -7.44000101 disc_lossU 9.31697432e-05 gen_lossU 7.78836e-10 gen_lossW 9.20317554\n",
            "disc_lossW -7.70437288 disc_lossU 6.38508136e-05 gen_lossU 5.57985769e-10 gen_lossW 9.24634743\n",
            "disc_lossW -7.01654863 disc_lossU 0.000258465501 gen_lossU 8.90043039e-10 gen_lossW 9.24351215\n",
            "disc_lossW -7.54927254 disc_lossU 0.000218826026 gen_lossU 8.45734482e-10 gen_lossW 9.38894463\n",
            "disc_lossW -7.38923502 disc_lossU 0.000243519506 gen_lossU 2.12143614e-09 gen_lossW 9.15404797\n",
            "disc_lossW -6.86157322 disc_lossU 0.000103008257 gen_lossU 1.23245525e-09 gen_lossW 9.1677\n",
            "disc_lossW -7.23310423 disc_lossU 8.79274e-05 gen_lossU 2.27120101e-09 gen_lossW 9.13919353\n",
            "disc_lossW -7.3485136 disc_lossU 0.000181710726 gen_lossU 4.8400145e-10 gen_lossW 9.15823078\n",
            "disc_lossW -7.03686094 disc_lossU 0.000190572318 gen_lossU 8.85421403e-10 gen_lossW 9.12826538\n",
            "disc_lossW -7.04588795 disc_lossU 0.000231937 gen_lossU 1.13891951e-09 gen_lossW 9.04302883\n",
            "disc_lossW -6.02933168 disc_lossU 2.81401772e-05 gen_lossU 7.93294e-10 gen_lossW 8.77718925\n",
            "disc_lossW -7.26303816 disc_lossU 9.3334449e-05 gen_lossU 6.49765797e-10 gen_lossW 8.65093517\n",
            "disc_lossW -6.62724 disc_lossU 4.56110793e-05 gen_lossU 3.14404924e-09 gen_lossW 8.47729301\n",
            "disc_lossW -6.47623348 disc_lossU 1.94299973e-05 gen_lossU 1.00589026e-09 gen_lossW 8.70525169\n",
            "disc_lossW -6.71085167 disc_lossU 7.49224419e-05 gen_lossU 1.25508071e-09 gen_lossW 8.57716084\n",
            "disc_lossW -5.93986559 disc_lossU 6.71379821e-05 gen_lossU 1.89142368e-09 gen_lossW 8.59405518\n",
            "disc_lossW -6.46962833 disc_lossU 0.000342399202 gen_lossU 8.63624561e-10 gen_lossW 8.60114193\n",
            "disc_lossW -5.53712654 disc_lossU 2.62178764e-05 gen_lossU 1.47229895e-09 gen_lossW 8.67037201\n",
            "disc_lossW -5.98684931 disc_lossU 8.72372912e-05 gen_lossU 7.57706453e-10 gen_lossW 8.18237305\n",
            "disc_lossW -6.05495739 disc_lossU 0.000141024895 gen_lossU 1.24793775e-09 gen_lossW 8.05020428\n",
            "disc_lossW -5.97733593 disc_lossU 7.33560664e-05 gen_lossU 2.35516895e-09 gen_lossW 8.06719398\n",
            "disc_lossW -6.01611519 disc_lossU 9.96040544e-05 gen_lossU 6.92874114e-09 gen_lossW 7.81259394\n",
            "disc_lossW -5.30169439 disc_lossU 3.11197145e-05 gen_lossU 7.31411864e-09 gen_lossW 7.7305069\n",
            "disc_lossW -5.54670095 disc_lossU 0.000248557364 gen_lossU 5.2327751e-09 gen_lossW 7.54853964\n",
            "disc_lossW -5.28320646 disc_lossU 6.39368227e-05 gen_lossU 1.23849251e-08 gen_lossW 7.78172207\n",
            "disc_lossW -4.95859766 disc_lossU 4.13067828e-05 gen_lossU 1.30619231e-08 gen_lossW 7.69324875\n",
            "disc_lossW -4.83913851 disc_lossU 3.76601565e-05 gen_lossU 1.33611646e-08 gen_lossW 7.70719767\n",
            "disc_lossW -5.28842449 disc_lossU 4.54504079e-05 gen_lossU 5.25213384e-09 gen_lossW 7.64797354\n",
            "disc_lossW -5.4347024 disc_lossU 0.00011928731 gen_lossU 1.29682292e-08 gen_lossW 7.10277176\n",
            "disc_lossW -4.74691534 disc_lossU 4.87738944e-05 gen_lossU 7.56816831e-09 gen_lossW 6.67934942\n",
            "disc_lossW -4.36192513 disc_lossU 2.24329451e-05 gen_lossU 1.39132315e-08 gen_lossW 6.63931894\n",
            "disc_lossW -4.97621059 disc_lossU 0.000110342844 gen_lossU 8.68299921e-09 gen_lossW 6.49712896\n",
            "disc_lossW -4.73558283 disc_lossU 6.57016499e-05 gen_lossU 1.96269063e-08 gen_lossW 6.53603506\n",
            "disc_lossW -4.41008854 disc_lossU 1.67317594e-05 gen_lossU 5.87069628e-08 gen_lossW 6.51557302\n",
            "disc_lossW -4.22233 disc_lossU 2.35639855e-05 gen_lossU 1.7562197e-08 gen_lossW 6.33201742\n",
            "disc_lossW -4.26548529 disc_lossU 6.66592e-05 gen_lossU 2.2001732e-08 gen_lossW 6.38936329\n",
            "disc_lossW -4.72422552 disc_lossU 6.03789e-05 gen_lossU 6.42330917e-08 gen_lossW 6.43068457\n",
            "disc_lossW -3.47904921 disc_lossU 2.63367201e-05 gen_lossU 2.71445337e-08 gen_lossW 5.61160088\n",
            "disc_lossW -3.71013045 disc_lossU 3.85713138e-05 gen_lossU 3.89303416e-08 gen_lossW 5.46171713\n",
            "disc_lossW -3.77851152 disc_lossU 0.000121783465 gen_lossU 1.80465634e-07 gen_lossW 5.25467062\n",
            "disc_lossW -4.03131819 disc_lossU 1.56793485e-05 gen_lossU 1.38911901e-07 gen_lossW 5.38592958\n",
            "disc_lossW -3.54605627 disc_lossU 2.48899923e-05 gen_lossU 7.1379084e-08 gen_lossW 5.00854349\n",
            "disc_lossW -4.34784508 disc_lossU 7.32482149e-05 gen_lossU 2.04905376e-07 gen_lossW 5.04579639\n",
            "disc_lossW -3.45936775 disc_lossU 7.92521168e-05 gen_lossU 5.28104849e-08 gen_lossW 4.77849293\n",
            "disc_lossW -3.7840364 disc_lossU 2.30939895e-05 gen_lossU 8.78172557e-08 gen_lossW 4.80077314\n",
            "disc_lossW -2.97677755 disc_lossU 3.10564646e-05 gen_lossU 1.97718322e-07 gen_lossW 4.36939383\n",
            "disc_lossW -3.21169305 disc_lossU 8.81088927e-05 gen_lossU 1.36755446e-07 gen_lossW 4.19787645\n",
            "disc_lossW -3.94089603 disc_lossU 3.24677749e-05 gen_lossU 5.14889791e-07 gen_lossW 4.34366512\n",
            "disc_lossW -3.23302078 disc_lossU 6.73489412e-05 gen_lossU 2.87497272e-07 gen_lossW 3.87390375\n",
            "disc_lossW -3.2049253 disc_lossU 8.20204295e-05 gen_lossU 4.74758963e-07 gen_lossW 3.79519272\n",
            "disc_lossW -3.11402297 disc_lossU 6.07731054e-05 gen_lossU 2.47429796e-07 gen_lossW 3.53614616\n",
            "disc_lossW -2.93410587 disc_lossU 0.000104951403 gen_lossU 4.92056756e-07 gen_lossW 3.52203679\n",
            "disc_lossW -2.94304657 disc_lossU 2.70107066e-05 gen_lossU 6.23968049e-07 gen_lossW 3.46517825\n",
            "disc_lossW -2.87569618 disc_lossU 6.04092384e-05 gen_lossU 3.18174529e-07 gen_lossW 3.28060532\n",
            "disc_lossW -2.85272169 disc_lossU 4.17016054e-05 gen_lossU 7.20163257e-07 gen_lossW 3.23041916\n",
            "disc_lossW -2.9666419 disc_lossU 5.06999131e-05 gen_lossU 3.97626252e-07 gen_lossW 3.28840494\n",
            "disc_lossW -2.80611587 disc_lossU 0.000119161698 gen_lossU 4.1771159e-07 gen_lossW 2.9027915\n",
            "disc_lossW -2.64644814 disc_lossU 0.000152287466 gen_lossU 4.70210665e-07 gen_lossW 2.69750667\n",
            "disc_lossW -2.67842937 disc_lossU 1.12747466e-05 gen_lossU 7.55164876e-07 gen_lossW 2.93571925\n",
            "disc_lossW -2.46377182 disc_lossU 2.17636662e-05 gen_lossU 4.77627736e-07 gen_lossW 2.73523092\n",
            "disc_lossW -2.59307575 disc_lossU 0.000128404441 gen_lossU 8.23677794e-07 gen_lossW 2.60616255\n",
            "disc_lossW -2.22594023 disc_lossU 1.85553617e-05 gen_lossU 9.97005714e-07 gen_lossW 2.63820791\n",
            "disc_lossW -2.27703381 disc_lossU 1.81090109e-05 gen_lossU 1.1561508e-06 gen_lossW 2.51046443\n",
            "disc_lossW -1.91285682 disc_lossU 3.95430943e-05 gen_lossU 1.12797352e-06 gen_lossW 2.22680545\n",
            "disc_lossW -2.03182054 disc_lossU 6.27347472e-05 gen_lossU 7.50054312e-07 gen_lossW 2.16206241\n",
            "disc_lossW -2.20616794 disc_lossU 7.26382768e-06 gen_lossU 9.68036488e-07 gen_lossW 2.03884125\n",
            "disc_lossW -1.79853106 disc_lossU 5.5094617e-05 gen_lossU 1.10460201e-06 gen_lossW 2.24984407\n",
            "disc_lossW -1.87228477 disc_lossU 1.88203558e-05 gen_lossU 1.45480703e-06 gen_lossW 2.2272706\n",
            "disc_lossW -1.94615459 disc_lossU 0.000178565941 gen_lossU 9.30140118e-07 gen_lossW 2.06270862\n",
            "disc_lossW -1.9035697 disc_lossU 0.000101751903 gen_lossU 9.67952474e-07 gen_lossW 1.93363535\n",
            "disc_lossW -1.78602254 disc_lossU 1.48952167e-05 gen_lossU 1.41424391e-06 gen_lossW 1.89708674\n",
            "disc_lossW -1.97172201 disc_lossU 8.31956932e-05 gen_lossU 1.83607631e-06 gen_lossW 2.01587081\n",
            "disc_lossW -1.63171494 disc_lossU 3.792813e-05 gen_lossU 1.93870596e-06 gen_lossW 1.84624839\n",
            "disc_lossW -0.858834624 disc_lossU 0.000136220624 gen_lossU 1.24858229e-06 gen_lossW 1.66514242\n",
            "disc_lossW -1.46991551 disc_lossU 7.08613952e-05 gen_lossU 1.91253503e-06 gen_lossW 1.76926649\n",
            "disc_lossW -1.31292665 disc_lossU 2.29977977e-05 gen_lossU 1.49604091e-06 gen_lossW 1.71765542\n",
            "disc_lossW -1.35246587 disc_lossU 5.15104839e-05 gen_lossU 2.2144834e-06 gen_lossW 1.62832797\n",
            "disc_lossW -1.41280591 disc_lossU 6.12767326e-05 gen_lossU 1.26788723e-06 gen_lossW 1.65505862\n",
            "disc_lossW -1.1489222 disc_lossU 1.84756045e-05 gen_lossU 2.52988525e-06 gen_lossW 1.60600281\n",
            "disc_lossW -1.19744194 disc_lossU 2.36952e-05 gen_lossU 2.16489457e-06 gen_lossW 1.73418713\n",
            "disc_lossW -0.93917191 disc_lossU 6.89944864e-05 gen_lossU 2.67674363e-06 gen_lossW 1.28804803\n",
            "disc_lossW -1.5445478 disc_lossU 4.54912e-05 gen_lossU 3.7393911e-06 gen_lossW 1.5494349\n",
            "disc_lossW -1.4901011 disc_lossU 5.2464351e-05 gen_lossU 3.62537867e-06 gen_lossW 1.64632344\n",
            "disc_lossW -1.13134694 disc_lossU 1.47254977e-05 gen_lossU 1.87490718e-06 gen_lossW 1.44266045\n",
            "disc_lossW -0.95821929 disc_lossU 6.14435e-05 gen_lossU 2.32046159e-06 gen_lossW 1.40802693\n",
            "disc_lossW -0.80892694 disc_lossU 3.34575407e-06 gen_lossU 3.56735768e-06 gen_lossW 1.48158407\n",
            "disc_lossW -1.13293576 disc_lossU 2.02041156e-05 gen_lossU 3.19123069e-06 gen_lossW 1.3748728\n",
            "disc_lossW -1.15387464 disc_lossU 6.23705128e-05 gen_lossU 2.2759898e-06 gen_lossW 1.14410245\n",
            "disc_lossW -1.27148199 disc_lossU 8.54484733e-06 gen_lossU 4.33507466e-06 gen_lossW 1.34246886\n",
            "disc_lossW -0.984083235 disc_lossU 2.2739925e-05 gen_lossU 3.79435778e-06 gen_lossW 1.12070346\n",
            "disc_lossW -1.14574623 disc_lossU 5.11438247e-06 gen_lossU 2.46311606e-06 gen_lossW 1.14990366\n",
            "disc_lossW -1.02735341 disc_lossU 7.00701639e-05 gen_lossU 2.73810952e-06 gen_lossW 1.11585248\n",
            "disc_lossW -0.999694765 disc_lossU 2.87032126e-05 gen_lossU 3.84439682e-06 gen_lossW 1.20919156\n",
            "disc_lossW -1.1830864 disc_lossU 6.5950313e-05 gen_lossU 2.91581659e-06 gen_lossW 0.988692939\n",
            "disc_lossW -1.39326906 disc_lossU 4.22790836e-05 gen_lossU 2.39669748e-06 gen_lossW 1.05728471\n",
            "disc_lossW -0.891269565 disc_lossU 2.0348074e-05 gen_lossU 4.66280562e-06 gen_lossW 0.965189576\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-271-fb4f987b5b30>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_train2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-195-17e27b39feb0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Save the model every 15 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-259-758eae5d45c1>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mgradients_of_discriminatorU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_tapeU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_lossU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminatorU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mdiscriminatorU_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_discriminatorU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminatorU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminatorU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_gradients_aggregation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         return tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[1;32m   1261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply_gradients_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[0m in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             distribution.extended.update(\n\u001b[0m\u001b[1;32m   1353\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2990\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m   2991\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2993\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m       return self._replica_ctx_update(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4060\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4061\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4062\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4064\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4066\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4067\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4068\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4069\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1347\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_step_xla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;34mf\"`tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             )\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/adam.py\u001b[0m in \u001b[0;36mupdate_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# Dense gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m  11557\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11558\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11559\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m  11560\u001b[0m         _ctx, \"Sub\", name, x, y)\n\u001b[1;32m  11561\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noise = tf.random.normal(shape=(1,100))\n",
        "test = generator.predict(noise)\n",
        "plt.imshow(test.squeeze(), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "9vPp5AiDkrAF",
        "outputId": "3d848da5-6eb3-4359-fadf-ba4caf55a081"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(x_train2))"
      ],
      "metadata": {
        "id": "fhuRFc0ro5T_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}