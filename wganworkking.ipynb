{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/wganworkking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.14.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2023.3.post1 tzdata-2023.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xI7rEeMMgDQI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgyl_WtbgO5k",
    "outputId": "b8fe388b-053f-4353-a985-e218964cd9cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {
    "id": "kik7wQ1qgOV-"
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, input_shape=(100,)))\n",
    "    model.add(layers.Reshape((7,7,256)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2DTranspose(128,3,2,padding='same',activation='LeakyReLU'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2DTranspose(128,3,2,padding='same',activation='LeakyReLU'))\n",
    "    model.add(layers.Conv2D(1,3,1,padding='same',activation='sigmoid'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_discriminator_model():\n",
    "    input_img = layers.Input(shape=(28,28,1))\n",
    "\n",
    "    x = layers.Conv2D(256,5,2,padding='same',activation='LeakyReLU')(input_img)\n",
    "\n",
    "    x = layers.Dropout(.6)(x)\n",
    "\n",
    "    x = layers.Conv2D(256,3,2,padding='same',activation='LeakyReLU')(x)\n",
    "\n",
    "    x = layers.Dropout(.6)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256,3,2,padding='same',activation='LeakyReLU')(x)\n",
    "\n",
    "    x = layers.Dropout(.6)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256,3,2,padding='same',activation='LeakyReLU')(x)\n",
    "\n",
    "    x = layers.Dropout(.6)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    dense_output = layers.Dense(128, activation='LeakyReLU')(x)\n",
    "\n",
    "\n",
    "    x = layers.Dropout(.6)(x)\n",
    "\n",
    "    dense_output = layers.Dense(64, activation='LeakyReLU')(x)\n",
    "\n",
    "    x = layers.Dropout(.6)(x)\n",
    "\n",
    "    dense_output = layers.Dense(1, activation=None)(dense_output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_img, outputs=dense_output)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {
    "id": "qzdSabqdVu_H"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {
    "id": "lS0DF8rjhHdY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_227 (Dense)           (None, 12544)             1266944   \n",
      "                                                                 \n",
      " reshape_31 (Reshape)        (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_65 (Bat  (None, 7, 7, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_56 (Conv2D  (None, 14, 14, 128)      295040    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_66 (Bat  (None, 14, 14, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_57 (Conv2D  (None, 28, 28, 128)      147584    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_152 (Conv2D)         (None, 28, 28, 1)         1153      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,712,257\n",
      "Trainable params: 1,711,489\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "discriminatorW = make_discriminator_model()\n",
    "discriminatorU = make_discriminator_model()\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0004)\n",
    "discriminatorW_optimizer = tf.keras.optimizers.Adam(0.0004)\n",
    "discriminatorU_optimizer = tf.keras.optimizers.Adam(0.000004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {
    "id": "DYQpIZyEgSlN"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "#@tf.function\n",
    "def discriminator_lossW(real_output, fake_output):\n",
    "    real_loss = tf.reduce_mean(real_output)\n",
    "    fake_loss = tf.reduce_mean(fake_output)\n",
    "    total_loss = fake_loss - real_loss\n",
    "    return total_loss\n",
    "\n",
    "#@tf.function\n",
    "def generator_lossW(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "#@tf.function\n",
    "def gradient_penalty(real_images, fake_images):\n",
    "    alpha = tf.random.uniform([BATCH_SIZE, 1, 1, 1], 0., 1.)\n",
    "    real_images, fake_images = tf.cast(real_images, tf.float32), tf.cast(fake_images, tf.float32)\n",
    "    interpolated_images = alpha * real_images + ((1 - alpha) * fake_images)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated_images)\n",
    "        pred = discriminatorW(interpolated_images, training=True)\n",
    "    gradients = tape.gradient(pred, [interpolated_images])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
    "    gp = tf.reduce_mean((norm - 1.)**2)\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {
    "id": "cP3-wZztEeaE"
   },
   "outputs": [],
   "source": [
    "NOISE_DIM = 100\n",
    "GP_WEIGHT = 100\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def train_step(images):\n",
    "    \n",
    "\n",
    "    for i in range(3):\n",
    "      with tf.GradientTape() as disc_tapeU:\n",
    "        noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_outputU = discriminatorU(images, training=True)\n",
    "        fake_outputU = discriminatorU(generated_images, training=True)\n",
    "        disc_lossU = cross_entropy(tf.ones_like(real_outputU), real_outputU)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      gradients_of_discriminatorU = disc_tapeU.gradient(disc_lossU, discriminatorU.trainable_variables)\n",
    "      discriminatorU_optimizer.apply_gradients(zip(gradients_of_discriminatorU, discriminatorU.trainable_variables))\n",
    "      if i == 0:\n",
    "        weights = discriminatorU.get_weights()\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tapeW:\n",
    "        noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_outputW = discriminatorW(images, training=True)\n",
    "        fake_outputW = discriminatorW(generated_images, training=True)\n",
    "        real_outputU = discriminatorU(images, training=True)\n",
    "        fake_outputU = discriminatorU(generated_images, training=True)\n",
    "        disc_lossW = discriminator_lossW(real_outputW, fake_outputW)\n",
    "\n",
    "\n",
    "        gen_lossU = cross_entropy(tf.ones_like(fake_outputU), fake_outputU)\n",
    "        gen_lossW = generator_lossW(fake_outputW)\n",
    "        gen_loss = gen_lossW+gen_lossU\n",
    "\n",
    "        gp = gradient_penalty(images, generated_images)\n",
    "        disc_lossW += gp * GP_WEIGHT\n",
    "\n",
    "\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    gradients_of_discriminatorW = disc_tapeW.gradient(disc_lossW, discriminatorW.trainable_variables)\n",
    "    discriminatorW_optimizer.apply_gradients(zip(gradients_of_discriminatorW, discriminatorW.trainable_variables))\n",
    "\n",
    "    discriminatorU.set_weights(weights)\n",
    "\n",
    "    tf.print(\"disc_lossW\",disc_lossW,'disc_lossU',disc_lossU,'gen_lossU',gen_lossU,'gen_lossW',gen_lossW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ihRh4v9euLM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {
    "id": "Sx_KbIfjiogE"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    for batch in range(len(dataset) // BATCH_SIZE):\n",
    "\n",
    "            target_images = dataset[batch * BATCH_SIZE: (batch+1) * BATCH_SIZE]\n",
    "\n",
    "\n",
    "            train_step(target_images)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      print(epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IWp_3RkPg248",
    "outputId": "54758da5-5d50-4ec6-956f-f5a0642b382f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disc_lossW 1.43252265 disc_lossU 3.17056887e-08 gen_lossU 2.47580232e-08 gen_lossW 1.90192819\n",
      "disc_lossW 1.795892 disc_lossU 4.33302318e-08 gen_lossU 6.73314426e-09 gen_lossW 1.24813\n",
      "disc_lossW 1.28736389 disc_lossU 1.4187675e-08 gen_lossU 4.05521268e-06 gen_lossW 2.89440727\n",
      "disc_lossW 1.36363518 disc_lossU 1.7224886e-09 gen_lossU 2.60429175e-07 gen_lossW 1.62409401\n",
      "disc_lossW 1.43194759 disc_lossU 3.13397368e-06 gen_lossU 3.29205214e-08 gen_lossW 0.81630373\n",
      "disc_lossW -0.0373721719 disc_lossU 6.29462704e-08 gen_lossU 6.39937667e-08 gen_lossW 1.29184413\n",
      "disc_lossW 0.589345634 disc_lossU 3.34153079e-08 gen_lossU 1.70613191e-07 gen_lossW 2.60844755\n",
      "disc_lossW -1.92145216 disc_lossU 3.13526499e-07 gen_lossU 8.72514949e-10 gen_lossW 4.72038555\n",
      "disc_lossW 2.1538353 disc_lossU 3.65584825e-08 gen_lossU 6.46561915e-08 gen_lossW 1.43177462\n",
      "disc_lossW 2.2459445 disc_lossU 1.05306782e-07 gen_lossU 4.27239826e-08 gen_lossW 1.05999362\n",
      "disc_lossW 1.1764667 disc_lossU 6.30043786e-08 gen_lossU 1.94345695e-09 gen_lossW 4.31194973\n",
      "disc_lossW 0.0513659716 disc_lossU 1.41180774e-08 gen_lossU 1.43699737e-07 gen_lossW 3.36536646\n",
      "disc_lossW 0.260495543 disc_lossU 8.5312962e-08 gen_lossU 1.93260036e-07 gen_lossW 3.62575173\n",
      "disc_lossW -1.68288195 disc_lossU 3.82838472e-09 gen_lossU 1.65035097e-08 gen_lossW 5.52832794\n",
      "disc_lossW 0.60509336 disc_lossU 4.24307245e-07 gen_lossU 1.62282774e-08 gen_lossW 3.33246565\n",
      "disc_lossW 2.00686145 disc_lossU 1.13375802e-06 gen_lossU 2.01104067e-09 gen_lossW 2.16524172\n",
      "disc_lossW 2.64824796 disc_lossU 9.182653e-07 gen_lossU 3.14884346e-07 gen_lossW 2.70947838\n",
      "disc_lossW 2.10115957 disc_lossU 3.31356978e-06 gen_lossU 5.26304049e-08 gen_lossW 2.11644578\n",
      "disc_lossW -0.36175859 disc_lossU 9.31271416e-08 gen_lossU 2.06927737e-07 gen_lossW 2.39180374\n",
      "disc_lossW -1.00294292 disc_lossU 1.66400262e-08 gen_lossU 1.44482835e-08 gen_lossW 3.02254057\n",
      "disc_lossW 2.67128396 disc_lossU 4.28424629e-09 gen_lossU 7.18101205e-08 gen_lossW 2.78404903\n",
      "disc_lossW -0.32338953 disc_lossU 9.14174247e-09 gen_lossU 2.16249646e-06 gen_lossW 3.59821\n",
      "disc_lossW -0.718411088 disc_lossU 4.655951e-08 gen_lossU 1.94836467e-08 gen_lossW 3.15111351\n",
      "disc_lossW -0.167021155 disc_lossU 6.44484075e-08 gen_lossU 3.88235613e-08 gen_lossW 1.35145581\n",
      "disc_lossW -2.51375246 disc_lossU 1.70705441e-08 gen_lossU 3.43491884e-08 gen_lossW 3.28973961\n",
      "disc_lossW 1.77861 disc_lossU 7.07679391e-08 gen_lossU 1.10497282e-07 gen_lossW 0.739568233\n",
      "disc_lossW 1.48026705 disc_lossU 2.28285117e-07 gen_lossU 2.57835518e-06 gen_lossW 3.42654943\n",
      "disc_lossW 3.21595287 disc_lossU 1.0987889e-07 gen_lossU 1.01663247e-08 gen_lossW 2.62904215\n",
      "disc_lossW 0.151118755 disc_lossU 3.79614363e-07 gen_lossU 6.21184242e-08 gen_lossW 2.58548355\n",
      "disc_lossW -1.77722502 disc_lossU 5.44922294e-08 gen_lossU 1.30816477e-06 gen_lossW 4.8293\n",
      "disc_lossW 0.909465313 disc_lossU 2.18716956e-08 gen_lossU 5.12256122e-08 gen_lossW 3.10733271\n",
      "disc_lossW 0.910680175 disc_lossU 2.75535434e-08 gen_lossU 1.83807813e-07 gen_lossW 2.15287304\n",
      "disc_lossW -0.778989673 disc_lossU 1.25062812e-07 gen_lossU 4.33221146e-07 gen_lossW 2.03181648\n",
      "disc_lossW 0.530479908 disc_lossU 1.19298296e-08 gen_lossU 2.97820094e-07 gen_lossW 3.12253904\n",
      "disc_lossW -2.89957118 disc_lossU 1.32893492e-08 gen_lossU 8.44544203e-08 gen_lossW 3.39133072\n",
      "disc_lossW 1.08730483 disc_lossU 2.73718541e-08 gen_lossU 9.47712309e-08 gen_lossW 3.93491697\n",
      "disc_lossW 2.43626976 disc_lossU 8.27331164e-08 gen_lossU 9.50661487e-08 gen_lossW 2.0788486\n",
      "disc_lossW 6.18916035 disc_lossU 4.57277665e-08 gen_lossU 8.59769273e-07 gen_lossW -0.692141294\n",
      "disc_lossW 0.627220154 disc_lossU 4.73933603e-08 gen_lossU 6.85645887e-07 gen_lossW 3.02149463\n",
      "disc_lossW 0.193646193 disc_lossU 8.36834602e-08 gen_lossU 4.17906509e-09 gen_lossW 3.08682656\n",
      "disc_lossW 3.42031097 disc_lossU 4.1378545e-08 gen_lossU 2.45869018e-07 gen_lossW -0.916522205\n",
      "disc_lossW 2.54526567 disc_lossU 6.24055696e-09 gen_lossU 2.23626103e-07 gen_lossW 2.536412\n",
      "disc_lossW -3.20867109 disc_lossU 8.3891436e-08 gen_lossU 1.30047624e-06 gen_lossW 4.44427204\n",
      "disc_lossW 1.29448247 disc_lossU 2.67641358e-08 gen_lossU 9.96899e-08 gen_lossW 0.422441185\n",
      "disc_lossW 2.15806675 disc_lossU 2.25809558e-07 gen_lossU 3.30366e-08 gen_lossW 1.037696\n",
      "disc_lossW -2.35344815 disc_lossU 2.54155097e-08 gen_lossU 1.04503556e-06 gen_lossW 4.8637228\n",
      "disc_lossW 0.171671927 disc_lossU 2.09651986e-07 gen_lossU 4.49361508e-08 gen_lossW 1.28460121\n",
      "disc_lossW 1.86766279 disc_lossU 6.03536332e-09 gen_lossU 1.47882258e-08 gen_lossW 2.0081811\n",
      "disc_lossW -0.980229378 disc_lossU 7.62330288e-08 gen_lossU 8.07174786e-07 gen_lossW 2.61441469\n",
      "disc_lossW 0.996687412 disc_lossU 1.90060376e-08 gen_lossU 3.77697688e-07 gen_lossW 4.32758904\n",
      "disc_lossW 0.222331107 disc_lossU 6.93845692e-08 gen_lossU 2.18631669e-07 gen_lossW 1.74353147\n",
      "disc_lossW 1.2089231 disc_lossU 9.67777254e-08 gen_lossU 1.45995287e-07 gen_lossW 1.65792429\n",
      "disc_lossW -0.731853545 disc_lossU 2.36620057e-08 gen_lossU 4.68943426e-06 gen_lossW 3.21238279\n",
      "disc_lossW -0.134136677 disc_lossU 4.15447552e-08 gen_lossU 3.19963391e-07 gen_lossW 2.11943054\n",
      "disc_lossW -0.8663131 disc_lossU 3.72087669e-07 gen_lossU 3.09471034e-07 gen_lossW 3.58886385\n",
      "disc_lossW 0.849272847 disc_lossU 7.50290496e-08 gen_lossU 5.65472249e-07 gen_lossW 2.23846316\n",
      "disc_lossW 2.20353436 disc_lossU 6.40034692e-09 gen_lossU 4.54254478e-05 gen_lossW 2.0979445\n",
      "disc_lossW -0.353967667 disc_lossU 5.56395037e-08 gen_lossU 3.64477432e-06 gen_lossW 3.67410231\n",
      "disc_lossW 2.2124176 disc_lossU 3.38067032e-08 gen_lossU 3.70910698e-08 gen_lossW 2.61612439\n",
      "disc_lossW 2.63089585 disc_lossU 6.20463396e-08 gen_lossU 6.37321e-08 gen_lossW 1.57089496\n",
      "disc_lossW 0.0898252726 disc_lossU 8.02502029e-07 gen_lossU 3.61118896e-06 gen_lossW 2.02870321\n",
      "disc_lossW 2.52051926 disc_lossU 4.37098464e-08 gen_lossU 7.59338377e-07 gen_lossW 2.41433907\n",
      "disc_lossW -0.446558356 disc_lossU 1.03754367e-07 gen_lossU 3.01904947e-06 gen_lossW 3.68988609\n",
      "disc_lossW 0.975228906 disc_lossU 4.12135159e-06 gen_lossU 1.72963587e-06 gen_lossW 1.72238743\n",
      "disc_lossW 2.53885078 disc_lossU 1.70371042e-08 gen_lossU 3.81926895e-07 gen_lossW 3.0201056\n",
      "disc_lossW 1.8180542 disc_lossU 1.04977474e-07 gen_lossU 4.57899318e-09 gen_lossW 3.20158458\n",
      "disc_lossW 4.69496155 disc_lossU 3.46075346e-08 gen_lossU 6.54670885e-06 gen_lossW 2.54885674\n",
      "disc_lossW -0.109721422 disc_lossU 1.56242464e-07 gen_lossU 3.08318135e-07 gen_lossW 2.64499\n",
      "disc_lossW 2.53490973 disc_lossU 3.63287533e-08 gen_lossU 2.34067465e-06 gen_lossW 2.98393512\n",
      "disc_lossW -0.736744583 disc_lossU 9.39434557e-08 gen_lossU 1.01731544e-07 gen_lossW 3.03507614\n",
      "disc_lossW 0.181800842 disc_lossU 8.9478948e-07 gen_lossU 1.28087635e-07 gen_lossW 3.50758886\n",
      "disc_lossW 3.03144932 disc_lossU 3.17306146e-08 gen_lossU 1.26365236e-07 gen_lossW 1.29450464\n",
      "disc_lossW 0.567292333 disc_lossU 6.11069773e-09 gen_lossU 3.24204552e-06 gen_lossW 1.81842709\n",
      "disc_lossW 1.87481081 disc_lossU 1.36373558e-07 gen_lossU 1.71603702e-07 gen_lossW 2.98034\n",
      "disc_lossW -0.396578312 disc_lossU 1.42918e-07 gen_lossU 3.4782e-06 gen_lossW 3.85567307\n",
      "disc_lossW -1.89500785 disc_lossU 8.74527206e-09 gen_lossU 3.78710297e-06 gen_lossW 5.98695421\n",
      "disc_lossW 0.692505717 disc_lossU 8.44287058e-07 gen_lossU 2.48353172e-06 gen_lossW 2.30195284\n",
      "disc_lossW 2.09197474 disc_lossU 1.35619e-08 gen_lossU 2.68707458e-06 gen_lossW 0.458004683\n",
      "disc_lossW 0.605142057 disc_lossU 1.71967457e-07 gen_lossU 4.02949695e-07 gen_lossW 2.3825531\n",
      "disc_lossW 0.838556528 disc_lossU 2.35194966e-06 gen_lossU 6.8408923e-07 gen_lossW 1.02828324\n",
      "disc_lossW 6.48796797 disc_lossU 3.2731603e-07 gen_lossU 1.98477733e-06 gen_lossW -1.02143574\n",
      "disc_lossW 0.247721076 disc_lossU 3.69053907e-08 gen_lossU 2.76708505e-07 gen_lossW 3.28201628\n",
      "disc_lossW -0.208133101 disc_lossU 8.83846774e-09 gen_lossU 3.43773117e-08 gen_lossW 3.09387612\n",
      "disc_lossW -1.42640471 disc_lossU 8.07438951e-08 gen_lossU 1.81841457e-07 gen_lossW 2.99402618\n",
      "disc_lossW 2.31945 disc_lossU 4.71164228e-08 gen_lossU 1.04759565e-07 gen_lossW 0.23419413\n",
      "disc_lossW 1.02190912 disc_lossU 3.65708388e-08 gen_lossU 5.40949046e-08 gen_lossW 1.77614236\n",
      "disc_lossW 2.51115704 disc_lossU 1.67030905e-08 gen_lossU 5.57034525e-08 gen_lossW 0.249325216\n",
      "disc_lossW -2.13837838 disc_lossU 6.1708613e-07 gen_lossU 5.85600446e-06 gen_lossW 2.87304735\n",
      "disc_lossW -0.155358553 disc_lossU 1.99892355e-07 gen_lossU 3.60623531e-08 gen_lossW 2.39825106\n",
      "disc_lossW 2.40559244 disc_lossU 1.56302171e-08 gen_lossU 1.72631278e-08 gen_lossW 0.329121888\n",
      "disc_lossW 0.315598369 disc_lossU 1.10712585e-06 gen_lossU 1.41702753e-06 gen_lossW 1.97451735\n",
      "disc_lossW 1.31243873 disc_lossU 6.11633766e-08 gen_lossU 1.62877836e-08 gen_lossW 1.86486959\n",
      "disc_lossW 1.28285646 disc_lossU 3.13705399e-08 gen_lossU 1.3634887e-08 gen_lossW 1.1759572\n",
      "disc_lossW 2.87923098 disc_lossU 2.72151457e-08 gen_lossU 1.91694937e-07 gen_lossW 1.26167452\n",
      "disc_lossW -1.24955177 disc_lossU 7.26986737e-09 gen_lossU 3.07585083e-07 gen_lossW 2.65961194\n",
      "disc_lossW -0.560341895 disc_lossU 3.81883439e-07 gen_lossU 4.51188509e-09 gen_lossW 3.50510335\n",
      "disc_lossW -0.426660538 disc_lossU 1.91788949e-08 gen_lossU 1.7593301e-08 gen_lossW 1.81854486\n",
      "disc_lossW -0.91514945 disc_lossU 1.39716896e-08 gen_lossU 6.37063735e-08 gen_lossW 2.79808378\n",
      "disc_lossW -2.51624703 disc_lossU 2.03283346e-09 gen_lossU 7.39828963e-08 gen_lossW 3.55502081\n",
      "disc_lossW -1.06333888 disc_lossU 5.86592499e-08 gen_lossU 4.09576927e-07 gen_lossW 3.53612781\n",
      "disc_lossW -1.88909471 disc_lossU 3.74717057e-09 gen_lossU 1.24802355e-08 gen_lossW 2.4921155\n",
      "disc_lossW -2.84850836 disc_lossU 8.77330741e-09 gen_lossU 1.40640566e-07 gen_lossW 4.20401859\n",
      "disc_lossW -0.477451682 disc_lossU 4.7588569e-06 gen_lossU 3.8841792e-09 gen_lossW 3.97461\n",
      "disc_lossW 4.2298646 disc_lossU 1.96968131e-08 gen_lossU 1.65517781e-06 gen_lossW 0.552211\n",
      "disc_lossW 0.709946334 disc_lossU 3.86861672e-08 gen_lossU 3.13672857e-08 gen_lossW 1.32476735\n",
      "disc_lossW 0.52897656 disc_lossU 2.11544886e-07 gen_lossU 1.10752616e-07 gen_lossW 2.67282248\n",
      "disc_lossW -0.444251895 disc_lossU 3.07622031e-06 gen_lossU 2.48646074e-07 gen_lossW 2.35005498\n",
      "disc_lossW -1.02153397 disc_lossU 5.38653566e-08 gen_lossU 6.75969281e-10 gen_lossW 4.12535286\n",
      "disc_lossW -2.74396825 disc_lossU 6.22369356e-09 gen_lossU 9.77039605e-08 gen_lossW 3.34658599\n",
      "disc_lossW -0.175202489 disc_lossU 2.49060967e-08 gen_lossU 1.03553369e-07 gen_lossW 3.28792906\n",
      "disc_lossW 1.26755369 disc_lossU 8.72895782e-08 gen_lossU 2.46871634e-09 gen_lossW 3.29094744\n",
      "disc_lossW 2.78588867 disc_lossU 8.34810443e-09 gen_lossU 1.52471114e-09 gen_lossW 1.60878634\n",
      "disc_lossW -1.0761435 disc_lossU 2.2656925e-06 gen_lossU 2.06060164e-08 gen_lossW 4.37550735\n",
      "disc_lossW 3.49066353 disc_lossU 4.97700739e-07 gen_lossU 5.77029141e-07 gen_lossW 3.61626387\n",
      "disc_lossW -0.144746 disc_lossU 1.53975051e-07 gen_lossU 3.17261395e-09 gen_lossW 3.96059322\n",
      "disc_lossW -0.462445259 disc_lossU 2.27432764e-07 gen_lossU 5.60071323e-09 gen_lossW 4.5154686\n",
      "disc_lossW -1.17593801 disc_lossU 1.45723442e-08 gen_lossU 1.03280073e-09 gen_lossW 6.0528\n",
      "disc_lossW -0.519842505 disc_lossU 5.68525529e-08 gen_lossU 2.29621544e-09 gen_lossW 4.81918097\n",
      "disc_lossW -2.08092785 disc_lossU 1.68946045e-07 gen_lossU 2.35220132e-09 gen_lossW 6.12545872\n",
      "disc_lossW 0.536492169 disc_lossU 5.87060185e-07 gen_lossU 5.30693089e-09 gen_lossW 4.4930563\n",
      "disc_lossW 2.86193871 disc_lossU 2.11389093e-08 gen_lossU 2.48474552e-09 gen_lossW 1.56422687\n",
      "disc_lossW 1.90872335 disc_lossU 3.96793602e-08 gen_lossU 5.1604264e-08 gen_lossW 4.70082569\n",
      "disc_lossW 0.808145523 disc_lossU 8.69127e-09 gen_lossU 1.29067345e-07 gen_lossW 4.31458187\n",
      "disc_lossW 2.07858467 disc_lossU 9.0264173e-08 gen_lossU 9.37803613e-09 gen_lossW 3.14171481\n",
      "disc_lossW -0.616266847 disc_lossU 3.01006295e-08 gen_lossU 4.07762606e-08 gen_lossW 7.22576\n",
      "disc_lossW 2.66180563 disc_lossU 1.14157661e-08 gen_lossU 8.91907632e-08 gen_lossW 4.57227039\n",
      "disc_lossW -2.52591491 disc_lossU 8.41324322e-07 gen_lossU 6.24297636e-09 gen_lossW 7.56616259\n",
      "disc_lossW 2.83190823 disc_lossU 2.87736356e-07 gen_lossU 7.50148672e-08 gen_lossW 1.16864812\n",
      "disc_lossW 0.228455782 disc_lossU 6.66875835e-08 gen_lossU 1.20872542e-07 gen_lossW 3.86731434\n",
      "disc_lossW 1.28207278 disc_lossU 4.17850865e-09 gen_lossU 4.78501e-06 gen_lossW 2.74050856\n",
      "disc_lossW 3.55388832 disc_lossU 1.44948409e-08 gen_lossU 1.80672103e-07 gen_lossW 3.22876716\n",
      "disc_lossW -2.4486351 disc_lossU 2.44883665e-07 gen_lossU 5.7525579e-08 gen_lossW 7.29799461\n",
      "disc_lossW 2.68000269 disc_lossU 4.7588621e-08 gen_lossU 2.65692876e-07 gen_lossW 5.037292\n",
      "disc_lossW 1.40653825 disc_lossU 1.48999533e-08 gen_lossU 3.50015597e-07 gen_lossW 3.54042244\n",
      "disc_lossW 2.79912567 disc_lossU 2.72961564e-08 gen_lossU 4.51393043e-06 gen_lossW 3.30878592\n",
      "disc_lossW 1.73128152 disc_lossU 7.79517073e-09 gen_lossU 9.98990473e-08 gen_lossW 3.671031\n",
      "disc_lossW 0.865843415 disc_lossU 2.00865564e-08 gen_lossU 2.84300268e-06 gen_lossW 2.2267313\n",
      "disc_lossW -0.403558135 disc_lossU 2.12359708e-09 gen_lossU 1.13004171e-06 gen_lossW 2.8371048\n",
      "disc_lossW 3.17110634 disc_lossU 6.60434818e-09 gen_lossU 5.44835871e-07 gen_lossW 2.37147617\n",
      "disc_lossW -0.380398393 disc_lossU 1.01965805e-08 gen_lossU 7.567578e-06 gen_lossW 3.94071698\n",
      "disc_lossW 1.51292539 disc_lossU 1.16788899e-08 gen_lossU 1.90044307e-06 gen_lossW 2.83363438\n",
      "disc_lossW 2.50280333 disc_lossU 1.0667582e-07 gen_lossU 2.10334392e-07 gen_lossW 1.52045465\n",
      "disc_lossW 1.0646162 disc_lossU 5.62821825e-08 gen_lossU 1.6836e-05 gen_lossW 2.32538176\n",
      "disc_lossW -0.222079039 disc_lossU 9.79917587e-08 gen_lossU 3.86176362e-06 gen_lossW 3.6669035\n",
      "disc_lossW 1.81449401 disc_lossU 1.8037278e-07 gen_lossU 2.99510089e-06 gen_lossW 1.37748992\n",
      "disc_lossW -6.18954372 disc_lossU 1.60966707e-07 gen_lossU 2.13185672e-06 gen_lossW 8.12940121\n",
      "disc_lossW 3.91624308 disc_lossU 2.12583284e-08 gen_lossU 2.20698e-07 gen_lossW -0.481313229\n",
      "disc_lossW -0.348532319 disc_lossU 1.43580408e-07 gen_lossU 2.84346669e-07 gen_lossW 1.9436146\n",
      "disc_lossW -2.65530252 disc_lossU 1.49124606e-08 gen_lossU 3.1286163e-05 gen_lossW 5.42682314\n",
      "disc_lossW -2.60509443 disc_lossU 3.41765303e-06 gen_lossU 4.96313135e-07 gen_lossW 4.89203024\n",
      "disc_lossW 4.70304871 disc_lossU 4.72699426e-08 gen_lossU 5.24619645e-06 gen_lossW 1.63264203\n",
      "disc_lossW 2.08700037 disc_lossU 5.52331425e-09 gen_lossU 3.18956921e-07 gen_lossW 4.67901945\n",
      "disc_lossW 7.7525568 disc_lossU 1.37703182e-07 gen_lossU 2.32156566e-07 gen_lossW 3.16352296\n",
      "disc_lossW -0.408052266 disc_lossU 1.2098786e-08 gen_lossU 5.92304616e-07 gen_lossW 3.9939456\n",
      "disc_lossW -0.757253349 disc_lossU 7.92388221e-07 gen_lossU 4.51568837e-07 gen_lossW 3.88250494\n",
      "disc_lossW -1.16011214 disc_lossU 4.77046385e-08 gen_lossU 1.76757039e-05 gen_lossW 6.27757025\n",
      "disc_lossW 3.97232246 disc_lossU 1.91984428e-09 gen_lossU 4.37690659e-07 gen_lossW 3.38289237\n",
      "disc_lossW 1.03589153 disc_lossU 1.41486636e-08 gen_lossU 8.08219511e-07 gen_lossW 5.23993778\n",
      "disc_lossW 0.055290103 disc_lossU 3.82655791e-08 gen_lossU 2.3399042e-07 gen_lossW 5.74268961\n",
      "disc_lossW 2.39608669 disc_lossU 3.14126254e-08 gen_lossU 1.13099918e-06 gen_lossW 5.17944813\n",
      "disc_lossW -0.0431889296 disc_lossU 2.22377095e-09 gen_lossU 1.01077057e-06 gen_lossW 3.28367925\n",
      "disc_lossW 4.38625479 disc_lossU 2.69767888e-08 gen_lossU 3.10009582e-08 gen_lossW 3.99711\n",
      "disc_lossW 3.53345966 disc_lossU 4.13391e-08 gen_lossU 9.61897626e-07 gen_lossW 1.90771294\n",
      "disc_lossW 3.66355753 disc_lossU 1.56621915e-08 gen_lossU 6.66449296e-07 gen_lossW 3.71923447\n",
      "disc_lossW 3.09286857 disc_lossU 2.71550622e-08 gen_lossU 2.78863723e-07 gen_lossW 2.13813233\n",
      "disc_lossW 1.19044697 disc_lossU 3.43662379e-08 gen_lossU 1.58653631e-06 gen_lossW 3.75389862\n",
      "disc_lossW 2.10836697 disc_lossU 8.41546e-07 gen_lossU 1.10275709e-08 gen_lossW 4.29142284\n",
      "disc_lossW 6.19732761 disc_lossU 4.55736471e-09 gen_lossU 4.01261e-08 gen_lossW 0.24749437\n",
      "disc_lossW 0.643157125 disc_lossU 9.2923e-09 gen_lossU 8.74326211e-09 gen_lossW 5.92706776\n",
      "disc_lossW 0.0798734426 disc_lossU 4.49305126e-09 gen_lossU 1.01397667e-07 gen_lossW 5.44869518\n",
      "disc_lossW 4.35754681 disc_lossU 2.04121413e-08 gen_lossU 1.5222291e-08 gen_lossW 1.65956795\n",
      "disc_lossW -1.38711524 disc_lossU 3.28956751e-09 gen_lossU 2.66656848e-08 gen_lossW 5.55384398\n",
      "disc_lossW 3.40150642 disc_lossU 1.76412414e-08 gen_lossU 2.60096105e-08 gen_lossW 2.63129377\n",
      "disc_lossW 0.0156258345 disc_lossU 4.61942903e-07 gen_lossU 9.9784e-09 gen_lossW 4.08253527\n",
      "disc_lossW -0.779733837 disc_lossU 1.77196728e-08 gen_lossU 1.13749145e-07 gen_lossW 6.39046717\n",
      "disc_lossW 0.404055595 disc_lossU 6.97247629e-08 gen_lossU 4.05918055e-09 gen_lossW 5.61216974\n",
      "disc_lossW 0.10886395 disc_lossU 4.96607271e-08 gen_lossU 4.9910355e-08 gen_lossW 6.53786707\n",
      "disc_lossW -1.32150316 disc_lossU 3.61094692e-08 gen_lossU 1.44232049e-08 gen_lossW 6.49862671\n",
      "disc_lossW 3.06571054 disc_lossU 2.80157426e-08 gen_lossU 3.4415395e-09 gen_lossW 4.27118111\n",
      "disc_lossW 2.92602444 disc_lossU 2.66333615e-07 gen_lossU 1.6927979e-08 gen_lossW 5.23711\n",
      "disc_lossW 2.74193597 disc_lossU 1.13851604e-06 gen_lossU 1.02001552e-09 gen_lossW 5.66980457\n",
      "disc_lossW 0.388283 disc_lossU 2.69639369e-08 gen_lossU 2.44904914e-08 gen_lossW 4.9923\n",
      "disc_lossW 1.89846182 disc_lossU 2.21091234e-08 gen_lossU 2.496e-08 gen_lossW 6.05802679\n",
      "disc_lossW 0.638726473 disc_lossU 3.33820545e-08 gen_lossU 1.25859334e-09 gen_lossW 5.77569485\n",
      "disc_lossW -0.888974667 disc_lossU 4.30363976e-08 gen_lossU 1.5135313e-09 gen_lossW 8.2017765\n",
      "disc_lossW 2.4123621 disc_lossU 1.79061406e-08 gen_lossU 3.54918e-09 gen_lossW 6.23998165\n",
      "disc_lossW 1.18853939 disc_lossU 2.88212454e-09 gen_lossU 2.22918439e-09 gen_lossW 6.52193928\n",
      "disc_lossW 0.763747573 disc_lossU 4.42177281e-08 gen_lossU 7.82476661e-09 gen_lossW 8.32681942\n",
      "disc_lossW 3.71178913 disc_lossU 4.85047508e-07 gen_lossU 4.04479367e-10 gen_lossW 7.17502785\n",
      "disc_lossW 0.55444169 disc_lossU 2.48626321e-07 gen_lossU 2.8365886e-11 gen_lossW 6.43482304\n",
      "disc_lossW -0.164109945 disc_lossU 1.88954274e-08 gen_lossU 4.24493496e-10 gen_lossW 6.54416513\n",
      "disc_lossW 4.22574949 disc_lossU 1.31548272e-09 gen_lossU 9.55368229e-10 gen_lossW 7.52715111\n",
      "disc_lossW -0.668093681 disc_lossU 1.58793458e-08 gen_lossU 3.61948409e-08 gen_lossW 7.65539\n",
      "disc_lossW 0.294492424 disc_lossU 1.88661113e-06 gen_lossU 4.67364458e-09 gen_lossW 8.60176277\n",
      "disc_lossW -2.16201234 disc_lossU 1.28854944e-08 gen_lossU 2.376499e-11 gen_lossW 11.2009792\n",
      "disc_lossW 2.71102238 disc_lossU 1.54371534e-07 gen_lossU 3.33789024e-10 gen_lossW 6.43777943\n",
      "disc_lossW 3.43264031 disc_lossU 3.60301655e-08 gen_lossU 1.8057702e-09 gen_lossW 7.36170578\n",
      "disc_lossW 0.575388789 disc_lossU 9.90871e-08 gen_lossU 7.58739627e-10 gen_lossW 8.54769325\n",
      "disc_lossW 2.30565929 disc_lossU 1.13441978e-07 gen_lossU 6.16096674e-10 gen_lossW 6.51044178\n",
      "disc_lossW 0.608414412 disc_lossU 1.41520839e-08 gen_lossU 5.69910474e-09 gen_lossW 9.25521946\n",
      "disc_lossW 0.426878214 disc_lossU 1.21451947e-07 gen_lossU 2.08475126e-09 gen_lossW 8.25996113\n",
      "disc_lossW -0.245785475 disc_lossU 3.93644726e-08 gen_lossU 1.33503919e-09 gen_lossW 10.0980091\n",
      "disc_lossW -0.504614234 disc_lossU 7.05755e-10 gen_lossU 2.88055069e-09 gen_lossW 10.5347519\n",
      "disc_lossW 5.23527861 disc_lossU 3.20867e-08 gen_lossU 3.01988301e-08 gen_lossW 6.83362055\n",
      "disc_lossW -1.34316409 disc_lossU 3.54848702e-08 gen_lossU 1.70908199e-09 gen_lossW 9.3654213\n",
      "disc_lossW 2.14500856 disc_lossU 2.7132149e-08 gen_lossU 1.54439064e-07 gen_lossW 7.39920425\n",
      "disc_lossW -0.599515915 disc_lossU 1.34566336e-09 gen_lossU 1.03336451e-08 gen_lossW 7.30188751\n",
      "disc_lossW 1.51499784 disc_lossU 1.24988858e-07 gen_lossU 1.63031566e-09 gen_lossW 6.30631113\n",
      "disc_lossW -0.81612432 disc_lossU 3.13008464e-08 gen_lossU 4.51546533e-09 gen_lossW 8.16336632\n",
      "disc_lossW -0.846239 disc_lossU 2.95224041e-08 gen_lossU 2.29444908e-07 gen_lossW 8.77188587\n",
      "disc_lossW 0.110172749 disc_lossU 1.50129651e-07 gen_lossU 7.19192539e-10 gen_lossW 7.64520741\n",
      "disc_lossW 1.53627944 disc_lossU 6.29564738e-08 gen_lossU 1.29974422e-08 gen_lossW 9.16677094\n",
      "disc_lossW 2.20603085 disc_lossU 4.4140242e-07 gen_lossU 5.40305649e-08 gen_lossW 8.20244217\n",
      "disc_lossW 2.25445557 disc_lossU 1.79133139e-07 gen_lossU 9.86872628e-09 gen_lossW 7.56510639\n",
      "disc_lossW 1.23708308 disc_lossU 7.00337708e-08 gen_lossU 3.93011383e-08 gen_lossW 5.94341373\n",
      "disc_lossW -2.20662522 disc_lossU 2.59178933e-07 gen_lossU 2.41679956e-08 gen_lossW 7.8151288\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "x_train2 = np.expand_dims(x_train, axis=-1)\n",
    "x_train2 = (x_train2 - np.min(x_train2)) / (np.max(x_train2) - np.min(x_train2))\n",
    "train(x_train2, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "9vPp5AiDkrAF",
    "outputId": "3d848da5-6eb3-4359-fadf-ba4caf55a081",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = tf.random.normal(shape=(1,100))\n",
    "test = generator.predict(noise)\n",
    "plt.imshow(test.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {
    "id": "fhuRFc0ro5T_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(x_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOko51Vap7de58bTo0cUSQ9",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
