{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/WGAN-GP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-g36b6KfZQeH",
    "outputId": "03984b48-fc78-4337-c88b-38309c0704f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.8/dist-packages (0.21.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (23.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.8/dist-packages (0.10.0.post2)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.3.5)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.3.1)\n",
      "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from librosa) (4.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.8/dist-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.57.1)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.51.0->librosa) (0.40.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.51.0->librosa) (6.1.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from pooch<1.7,>=1.0->librosa) (2.22.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from pooch<1.7,>=1.0->librosa) (23.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.51.0->librosa) (3.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons\n",
    "#!pip install pydub\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kq4KEMDwylHv",
    "outputId": "c561625f-4dfd-4d33-d481-5f6d3be071ae"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UiZf7YAawUu",
    "outputId": "88fca3e1-8c6b-48e4-d860-5736e327b4fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 11:21:36.922321: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KZA4iSHlpHeW"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def downsample_array(arr, factor, axis):\n",
    "    return arr.take(np.arange(0, arr.shape[axis], factor), axis)\n",
    "\n",
    "def create_sliding_window(array, window_size, stride):\n",
    "    num_windows = (len(array) - window_size) // stride + 1\n",
    "    sliding_windows = np.lib.stride_tricks.sliding_window_view(array, (window_size,))\n",
    "\n",
    "    return sliding_windows[::stride]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PwZorlMxq2Rx"
   },
   "outputs": [],
   "source": [
    "reduction = 12\n",
    "wav_file = '/tmp/bass samples/NBKoanbandstuff.wav'\n",
    "audio, sr = librosa.load(wav_file, sr=None)\n",
    "audio_dev = np.std(audio)\n",
    "audio = audio/audio_dev\n",
    "result_array = create_sliding_window(audio,44100,100)\n",
    "result_array = np.expand_dims(result_array, axis=2)\n",
    "result_array = downsample_array(result_array,reduction,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "ZIzKgHj1sIvX",
    "outputId": "cfb075e1-4037-450e-a12a-66f34e7f1c44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "audio_data = result_array[100].ravel()\n",
    "# Play the audio within the Jupyter Notebook\n",
    "Audio(data=audio_data, rate=sr/reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3XZfCdyC7S_D",
    "outputId": "e5ac907d-e770-4c00-b945-b728bf89b812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3675, 1)\n"
     ]
    }
   ],
   "source": [
    "#x_train = noise\n",
    "y_train = result_array\n",
    "print(y_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MUFnmA-82FDv"
   },
   "outputs": [],
   "source": [
    "class ResNetBlock(layers.Layer):\n",
    "    def __init__(self, filters,kernel_size=3, strides=1,dilation_rate=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = layers.Conv1D(filters, kernel_size, strides=strides,dilation_rate=dilation_rate, padding='same')\n",
    "        self.conv2 = layers.Conv1D(filters, kernel_size, padding='same')\n",
    "\n",
    "        if strides != 1:\n",
    "            self.residual = layers.Conv1D(filters, 1, strides=strides)\n",
    "        else:\n",
    "            self.residual = lambda x: x\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        x = self.conv1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        r = self.residual(inputs)\n",
    "\n",
    "        x += r\n",
    "        \n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qBopciVs0FBC"
   },
   "outputs": [],
   "source": [
    "class ResNetBlockup(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=3, strides=1, dilation_rate=1):\n",
    "        super(ResNetBlockup, self).__init__()\n",
    "        self.conv1 = layers.Conv1DTranspose(filters, kernel_size, strides=strides, dilation_rate=dilation_rate, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv1DTranspose(filters, kernel_size, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        if strides != 1:\n",
    "            self.residual = layers.Conv1DTranspose(filters, 1, strides=strides)\n",
    "        else:\n",
    "            self.residual = lambda x: x\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        r = self.residual(inputs)\n",
    "\n",
    "        x += r\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gh2ORgUybdzt",
    "outputId": "b3c88a15-7f48-47f8-ba8e-13f2372a84aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 1)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 128)         640       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         65664     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 128)         65664     \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, None, 128)         65664     \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, None, 128)         65664     \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, None, 128)         65664     \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, None, 128)         65664     \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, None, 512)         262656    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 512)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690,177\n",
      "Trainable params: 690,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 11:21:39.461796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 11:21:39.465486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 11:21:39.465665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 11:21:39.466754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 11:21:39.466975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 11:21:39.467140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 11:21:39.977408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 11:21:39.977602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 11:21:39.977744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 11:21:39.977869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6093 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:0b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator():\n",
    "    input_series = layers.Input(shape=(None,1))\n",
    "\n",
    "\n",
    "    x = layers.Conv1D(128, 4, strides =1, padding='same')(input_series)\n",
    "    \n",
    "    x = layers.Conv1D(128, 4, strides =2, padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 4, strides =4, padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 4, strides =8, padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 4, strides =12, padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 4, strides =24, padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 4, strides =48, padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv1D(512, 4, strides =96, padding='same')(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Global pooling\n",
    "    pooled_output = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Dense layer\n",
    "    dense_output = layers.Dense(64, activation='relu')(pooled_output)\n",
    "\n",
    "    # Dense layer\n",
    "    dense_output = layers.Dense(1, activation='linear')(dense_output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_series, outputs=dense_output)\n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8q_-l7f8eC-i",
    "outputId": "999bad35-ec0a-4714-cc8e-10454b73b946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None, 1)]         0         \n",
      "                                                                 \n",
      " res_net_block (ResNetBlock)  (None, None, 128)        66304     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 128)         0         \n",
      "                                                                 \n",
      " res_net_block_1 (ResNetBloc  (None, None, 128)        131328    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 128)         0         \n",
      "                                                                 \n",
      " res_net_block_2 (ResNetBloc  (None, None, 128)        131328    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 128)         0         \n",
      "                                                                 \n",
      " res_net_block_3 (ResNetBloc  (None, None, 128)        131328    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 128)         0         \n",
      "                                                                 \n",
      " res_net_block_4 (ResNetBloc  (None, None, 128)        131328    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 128)         0         \n",
      "                                                                 \n",
      " res_net_block_5 (ResNetBloc  (None, None, 128)        131328    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " conv1d_20 (Conv1D)          (None, None, 1)           129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 723,073\n",
      "Trainable params: 723,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_generator():\n",
    "    input_series = layers.Input(shape=(None,1))\n",
    "\n",
    "    x = ResNetBlock(128,4,strides=1,dilation_rate=24)(input_series)\n",
    "    \n",
    "    x = layers.Dropout(.2)(x)\n",
    "\n",
    "    x = ResNetBlock(128,4,strides=1,dilation_rate=12)(x)\n",
    "    \n",
    "    x = layers.Dropout(.2)(x)\n",
    "\n",
    "    x = ResNetBlock(128,4,strides=1,dilation_rate=6)(x)\n",
    "    \n",
    "    x = layers.Dropout(.2)(x)\n",
    "\n",
    "    x = ResNetBlock(128,4,strides=1,dilation_rate=4)(x)\n",
    "    \n",
    "    x = layers.Dropout(.2)(x)\n",
    "\n",
    "    x = ResNetBlock(128,4,strides=1,dilation_rate=2)(x)\n",
    "    \n",
    "    x = layers.Dropout(.2)(x)\n",
    "\n",
    "    x = ResNetBlock(128,4,strides=1,dilation_rate=1)(x)\n",
    "\n",
    "    x = layers.Conv1D(1,1)(x)\n",
    "\n",
    "\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_series, outputs=x)\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aFFOfuDfbpD9"
   },
   "outputs": [],
   "source": [
    "# Compile models\n",
    "#generator_optimizer = tf.keras.optimizers.Adam(0.00004)\n",
    "#discriminator_optimizer = tf.keras.optimizers.Adam(0.00004)\n",
    "\n",
    "# Set your desired hyperparameters\n",
    "learnRateD = 0.0002\n",
    "learnRateG = 0.001\n",
    "decayRateD = 0.9\n",
    "decayRateG = 0.9\n",
    "epsilonD = 1e-10\n",
    "epsilonG = 1e-10\n",
    "\n",
    "# Create the optimizers with the set hyperparameters\n",
    "generator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learnRateG, rho=decayRateG, epsilon=epsilonG, centered=False)\n",
    "discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learnRateD, rho=decayRateD, epsilon=epsilonD, centered=False)\n",
    "#generator_optimizer = tf.keras.optimizers.experimental.SGD(1e-4)\n",
    "#discriminator_optimizer = tf.keras.optimizers.experimental.SGD(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DpBCjazdvotg"
   },
   "outputs": [],
   "source": [
    "def gradient_penalty(real, fake, discriminator):\n",
    "    batch_size = real.shape[0]\n",
    "    epsilon = tf.random.uniform([batch_size, 1, 1], 0.0, 1.0)\n",
    "    interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        pred = discriminator(interpolated, training=True)\n",
    "\n",
    "    gradients = tape.gradient(pred, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "T3PtShxlbn9c"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output, gradient_penalty):\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output) + gradient_penalty\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "a8RtXmHLZHWu"
   },
   "outputs": [],
   "source": [
    "def clip_discriminator_weights(discriminator):\n",
    "    for l in discriminator.layers:\n",
    "        weights = l.get_weights()\n",
    "        weights = [tf.clip_by_value(w, -0.01, 0.01) for w in weights]\n",
    "        l.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vAoSvqXLsQHq"
   },
   "outputs": [],
   "source": [
    "def print_img(generator_model):\n",
    "    # Generate and save sample images\n",
    "    noise = tf.random.normal([10, 100])\n",
    "    sampled_labels = tf.constant([[i % 10] for i in range(10)], dtype=tf.int32)\n",
    "    generated_images = generator_model.predict([noise, sampled_labels])\n",
    "    fig, axs = plt.subplots(1, 10, figsize=(10, 10))\n",
    "    for i in range(10):\n",
    "        axs[i].imshow(generated_images[i], cmap=\"gray\")\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAqGbZU0Z5yl",
    "outputId": "e0d5d88e-72db-48df-9c5f-d299dd423187"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 11:21:41.111911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "(1, 10000, 1)\n"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal(shape=(1,10000,1))\n",
    "\n",
    "test = generator.predict(noise)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wp5mnkMVVgPV",
    "outputId": "bdd629cb-03a7-462b-ff31-3239aaa226cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 11:21:42.231582: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x4299cd20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-08 11:21:42.231610: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 SUPER, Compute Capability 7.5\n",
      "2023-08-08 11:21:42.234803: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-08 11:21:42.323995: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f1e093c3310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f1e093c3310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "disc_loss -5.87054491 gen_loss 19.6036568\n",
      "disc_loss -874.223511 gen_loss 2509.10376\n",
      "disc_loss 0.330419242 gen_loss 0.652748704\n",
      "disc_loss 0.281469882 gen_loss 0.726069689\n",
      "disc_loss 0.226503074 gen_loss 0.764575303\n",
      "disc_loss 0.165609121 gen_loss 0.850236058\n",
      "disc_loss 0.100284338 gen_loss 0.91120851\n",
      "disc_loss 0.0121122599 gen_loss 1.01947248\n",
      "disc_loss -0.129227281 gen_loss 1.12867236\n",
      "disc_loss -0.252874255 gen_loss 1.25067163\n",
      "disc_loss -0.419613421 gen_loss 1.4767741\n",
      "disc_loss -0.640644431 gen_loss 1.86514699\n",
      "disc_loss -1.13830495 gen_loss 2.40947986\n",
      "disc_loss -1.52232993 gen_loss 3.04806471\n",
      "disc_loss -2.54595804 gen_loss 4.45854855\n",
      "disc_loss -4.23937702 gen_loss 7.65778828\n",
      "disc_loss -6.21899176 gen_loss 11.3234959\n",
      "disc_loss -5.7644062 gen_loss 10.8335505\n",
      "disc_loss -2.79889441 gen_loss 5.33838415\n",
      "disc_loss -0.155564427 gen_loss 0.977668405\n",
      "disc_loss 0.74966538 gen_loss 0.354519337\n",
      "disc_loss 0.527735054 gen_loss 0.316443771\n",
      "disc_loss -0.941367686 gen_loss 3.02097678\n",
      "disc_loss -2.69819927 gen_loss 6.46063757\n",
      "disc_loss -5.26068735 gen_loss 12.9964809\n",
      "disc_loss -5.58785391 gen_loss 11.2629833\n",
      "disc_loss -0.00937175751 gen_loss 0.68748045\n",
      "disc_loss 0.478272736 gen_loss 0.261160314\n",
      "disc_loss 0.769056797 gen_loss -8.89293879e-05\n",
      "disc_loss 0.53698355 gen_loss -0.00626320112\n",
      "disc_loss 0.389575273 gen_loss -0.031163644\n",
      "disc_loss 0.16741848 gen_loss 0.0639689416\n",
      "disc_loss -0.362257957 gen_loss 0.749751747\n",
      "disc_loss -2.1948657 gen_loss 3.14082956\n",
      "disc_loss -20.1225357 gen_loss 45.7056885\n",
      "disc_loss -10.2876034 gen_loss 17.7751236\n",
      "disc_loss -1.85927367 gen_loss 2.32036376\n",
      "disc_loss -0.794288754 gen_loss 1.17954457\n",
      "disc_loss 0.081332922 gen_loss 0.466873795\n",
      "disc_loss 0.340260297 gen_loss -0.0060923486\n",
      "disc_loss -0.360777259 gen_loss 0.127719462\n",
      "disc_loss -0.296218514 gen_loss 0.397523344\n",
      "disc_loss -0.554244936 gen_loss 0.358622253\n",
      "disc_loss -0.954185843 gen_loss 0.46603471\n",
      "disc_loss -0.436790466 gen_loss 0.727823138\n",
      "disc_loss -0.823572099 gen_loss 0.0482507162\n",
      "disc_loss -0.399756551 gen_loss 0.0607997291\n",
      "disc_loss -2.67639303 gen_loss 0.622528791\n",
      "disc_loss -4.27096033 gen_loss 3.6285224\n",
      "disc_loss -6.49672508 gen_loss 5.95711279\n",
      "disc_loss -10.3543434 gen_loss 11.9931631\n",
      "disc_loss -4.1271987 gen_loss 0.983014882\n",
      "disc_loss -1.99732375 gen_loss 0.43766135\n",
      "disc_loss -1.69039905 gen_loss 0.177656427\n",
      "disc_loss -1.85831535 gen_loss 0.974901795\n",
      "disc_loss -2.31923604 gen_loss 0.419815958\n",
      "disc_loss -5.08561516 gen_loss 3.390347\n",
      "disc_loss -6.45888662 gen_loss 5.20206881\n",
      "disc_loss -9.71563911 gen_loss 6.7502985\n",
      "disc_loss -8.18515205 gen_loss 0.72576791\n",
      "disc_loss -10.816328 gen_loss 1.2654562\n",
      "disc_loss -8.71140099 gen_loss 0.864931285\n",
      "disc_loss -6.55820274 gen_loss 1.16967893\n",
      "disc_loss -6.17499352 gen_loss 1.87871325\n",
      "disc_loss -8.50549412 gen_loss 2.23557568\n",
      "disc_loss -9.31439304 gen_loss 0.499672\n",
      "disc_loss -13.4452639 gen_loss 1.2475853\n",
      "disc_loss -12.018404 gen_loss 0.519566059\n",
      "disc_loss -14.7694283 gen_loss 0.818282604\n",
      "disc_loss -14.9643688 gen_loss 0.55054152\n",
      "disc_loss -14.3619928 gen_loss 0.764699817\n",
      "disc_loss -15.7596188 gen_loss 0.586403489\n",
      "disc_loss -18.5698891 gen_loss 0.6737\n",
      "disc_loss -18.1344261 gen_loss 0.0210418459\n",
      "disc_loss -18.7784901 gen_loss 0.579852223\n",
      "disc_loss -14.6555634 gen_loss 0.14787975\n",
      "disc_loss -11.4323444 gen_loss 2.13496351\n",
      "disc_loss -10.1736202 gen_loss 0.963878155\n",
      "disc_loss -12.8862085 gen_loss 0.346762955\n",
      "disc_loss -16.9756527 gen_loss 0.568366647\n",
      "disc_loss -21.1740627 gen_loss 0.49059692\n",
      "disc_loss -24.7832451 gen_loss -0.0343142785\n",
      "disc_loss -28.3898487 gen_loss 1.95641959\n",
      "disc_loss -26.1797276 gen_loss 0.555062\n",
      "disc_loss -23.1435852 gen_loss 0.63618803\n",
      "disc_loss -19.7687683 gen_loss 0.178874403\n",
      "disc_loss -24.9931526 gen_loss 0.0549517497\n",
      "disc_loss -26.4829655 gen_loss 0.600278199\n",
      "disc_loss -37.1502342 gen_loss 14.1376095\n",
      "disc_loss -34.7345963 gen_loss 0.063052088\n",
      "disc_loss -34.5283051 gen_loss -0.451540858\n",
      "disc_loss -40.4431839 gen_loss 1.11385691\n",
      "disc_loss -38.6974144 gen_loss -0.229379088\n",
      "disc_loss -33.8273506 gen_loss 4.52868891\n",
      "disc_loss -29.5425987 gen_loss -0.275064886\n",
      "disc_loss -32.4052086 gen_loss -0.732754111\n",
      "disc_loss -32.448143 gen_loss -0.463393062\n",
      "disc_loss -36.8243828 gen_loss -0.400111049\n",
      "disc_loss -39.0551682 gen_loss 2.54879379\n",
      "disc_loss -37.3930206 gen_loss -0.610312164\n",
      "disc_loss -35.0519 gen_loss -1.21797633\n",
      "disc_loss -30.9112 gen_loss -0.192747891\n",
      "disc_loss -28.6095123 gen_loss -2.05264592\n",
      "disc_loss -20.7037964 gen_loss -1.38476729\n",
      "disc_loss -28.6729984 gen_loss 22.7829151\n",
      "disc_loss -27.8100929 gen_loss 20.1825199\n",
      "disc_loss -24.8808708 gen_loss -0.480206341\n",
      "disc_loss -29.3055649 gen_loss -0.344304949\n",
      "disc_loss -30.9562359 gen_loss -0.759395599\n",
      "disc_loss -31.4912777 gen_loss 0.557857871\n",
      "disc_loss -22.1506691 gen_loss 0.394459903\n",
      "disc_loss -26.5659943 gen_loss -0.382054865\n",
      "disc_loss -32.4165344 gen_loss -0.277715862\n",
      "disc_loss -43.0771217 gen_loss 0.213687658\n",
      "disc_loss -46.3767242 gen_loss -0.397146225\n",
      "disc_loss -48.1818428 gen_loss -1.01355612\n",
      "disc_loss -31.7343388 gen_loss -0.739106536\n",
      "disc_loss -32.956234 gen_loss -1.66457963\n",
      "disc_loss -28.7471085 gen_loss -1.64043403\n",
      "disc_loss -21.2041302 gen_loss -2.72015834\n",
      "disc_loss -24.4147339 gen_loss -2.91325212\n",
      "disc_loss -23.0338707 gen_loss -6.16173458\n",
      "disc_loss -26.8826504 gen_loss 6.74031305\n",
      "disc_loss -692.369446 gen_loss 1262.40308\n",
      "disc_loss -19.5365696 gen_loss -3.10763979\n",
      "disc_loss -17.7967 gen_loss -3.59292865\n",
      "disc_loss -12.792449 gen_loss -2.70285559\n",
      "disc_loss -17.4804592 gen_loss -2.30152845\n",
      "disc_loss -21.5894775 gen_loss -2.22415042\n",
      "disc_loss -23.2977066 gen_loss -3.21937299\n",
      "disc_loss -25.6860065 gen_loss -4.78709316\n",
      "disc_loss -25.4002132 gen_loss -3.49458385\n",
      "disc_loss -27.8390312 gen_loss -3.93998456\n",
      "disc_loss -27.5344448 gen_loss -2.51388788\n",
      "disc_loss -28.809185 gen_loss -3.04394293\n",
      "disc_loss -24.4366493 gen_loss -5.32180166\n",
      "disc_loss -9.5741539 gen_loss -4.1385417\n",
      "disc_loss -9.12402 gen_loss -3.17528224\n",
      "disc_loss -9.11463737 gen_loss -2.78789282\n",
      "disc_loss -22.8637886 gen_loss 29.9332027\n",
      "disc_loss -33.5650749 gen_loss 43.4432602\n",
      "disc_loss -8.56334209 gen_loss -1.16446412\n",
      "disc_loss -10.9878483 gen_loss -1.95090795\n",
      "disc_loss -5.82928801 gen_loss -1.58214259\n",
      "disc_loss -4.34508228 gen_loss -1.23773468\n",
      "disc_loss -17.877243 gen_loss -5.53976107\n",
      "disc_loss -30.3572464 gen_loss -4.35923481\n",
      "disc_loss -31.8844452 gen_loss -7.42848873\n",
      "disc_loss -21.1934204 gen_loss -4.75188971\n",
      "disc_loss -21.2651024 gen_loss -5.51941204\n",
      "disc_loss -9.06504536 gen_loss -5.7931\n",
      "disc_loss -7.45908689 gen_loss -4.46657419\n",
      "disc_loss -9.48017216 gen_loss -1.88370037\n",
      "disc_loss -18.5638142 gen_loss -6.25999832\n",
      "disc_loss -27.6106834 gen_loss -8.67093086\n",
      "disc_loss -32.1239624 gen_loss -13.0927534\n",
      "disc_loss -15.2827139 gen_loss -7.31067181\n",
      "disc_loss -15.4624805 gen_loss 0.147840217\n",
      "disc_loss -98.3864822 gen_loss 141.084625\n",
      "disc_loss -150.651367 gen_loss 192.885895\n",
      "disc_loss -18.6820183 gen_loss -2.10964227\n",
      "disc_loss -19.4883232 gen_loss -1.80787969\n",
      "disc_loss -29.8574944 gen_loss -5.9917531\n",
      "disc_loss -51.543129 gen_loss -10.6717243\n",
      "disc_loss -36.3971634 gen_loss -4.85367203\n",
      "disc_loss -31.9282532 gen_loss -7.4279747\n",
      "disc_loss -33.4552078 gen_loss -2.49231625\n",
      "disc_loss -32.4278679 gen_loss -3.19729304\n",
      "disc_loss -25.9453621 gen_loss -6.06759405\n",
      "disc_loss -24.8679123 gen_loss -8.02016258\n",
      "disc_loss -35.5619812 gen_loss -5.94774246\n",
      "disc_loss -38.1253242 gen_loss -11.242671\n",
      "disc_loss -40.6878052 gen_loss -5.93486786\n",
      "disc_loss -54.2082 gen_loss -8.81670189\n",
      "disc_loss -41.5977097 gen_loss -8.99333668\n",
      "disc_loss -63.6327934 gen_loss -26.2839355\n",
      "disc_loss -69.9344788 gen_loss -25.7104092\n",
      "disc_loss -28.9095783 gen_loss -21.8125172\n",
      "disc_loss -140.339355 gen_loss 191.979767\n",
      "disc_loss -707.909424 gen_loss 1376.29651\n",
      "disc_loss -44.8574142 gen_loss 7.17122269\n",
      "disc_loss -48.9804344 gen_loss 3.97993922\n",
      "disc_loss -26.0058231 gen_loss -1.34019756\n",
      "disc_loss -21.1863022 gen_loss -5.6316576\n",
      "disc_loss -27.7594166 gen_loss -7.24946308\n",
      "disc_loss -35.0303955 gen_loss -4.60893583\n",
      "disc_loss -23.8715305 gen_loss -8.44194412\n",
      "disc_loss -23.0815144 gen_loss -7.02548504\n",
      "disc_loss -43.7220688 gen_loss -10.9181356\n",
      "disc_loss -37.8671303 gen_loss -7.6671257\n",
      "disc_loss -23.41782 gen_loss -6.50116873\n",
      "disc_loss -34.3673935 gen_loss -10.9868832\n",
      "disc_loss -22.909338 gen_loss -7.48232508\n",
      "disc_loss -35.7984772 gen_loss -6.08920145\n",
      "disc_loss -55.515274 gen_loss -10.5226803\n",
      "disc_loss -43.1541 gen_loss -14.8817854\n",
      "disc_loss -25.9523277 gen_loss -13.2056274\n",
      "disc_loss -17.6742973 gen_loss -9.40441\n",
      "disc_loss -12.5169573 gen_loss -11.3904142\n",
      "disc_loss -17.0208588 gen_loss 0.494477093\n",
      "disc_loss -20.0946388 gen_loss 3.65472412\n",
      "disc_loss -17.1955681 gen_loss -9.04543\n",
      "disc_loss -32.0713501 gen_loss 4.41983509\n",
      "disc_loss -98.3749542 gen_loss 127.294205\n",
      "disc_loss -103.7407 gen_loss 136.878571\n",
      "disc_loss -36.853447 gen_loss 11.6644716\n",
      "disc_loss -32.1697159 gen_loss -2.79470253\n",
      "disc_loss -29.0232544 gen_loss -2.22955608\n",
      "disc_loss -30.3473339 gen_loss -2.71376514\n",
      "disc_loss -47.5208664 gen_loss -8.32462502\n",
      "disc_loss -54.1036797 gen_loss -9.80951\n",
      "disc_loss -63.6297302 gen_loss -8.12496853\n",
      "disc_loss -64.5708313 gen_loss -12.2826147\n",
      "disc_loss -65.8514481 gen_loss -13.1950169\n",
      "disc_loss -57.554657 gen_loss -13.9535646\n",
      "disc_loss -63.8606796 gen_loss -8.07948875\n",
      "disc_loss -85.6347198 gen_loss -16.1651363\n",
      "disc_loss -87.6313477 gen_loss 48.5519905\n",
      "disc_loss -84.9745865 gen_loss -14.3171873\n",
      "disc_loss -70.2588272 gen_loss -8.0948391\n",
      "disc_loss -59.9584732 gen_loss -12.54669\n",
      "disc_loss -33.6685143 gen_loss 1.92439151\n",
      "disc_loss -25.9721813 gen_loss -11.4703321\n",
      "disc_loss -28.0338745 gen_loss -9.69340324\n",
      "disc_loss -27.9232388 gen_loss -10.3261967\n",
      "disc_loss -48.0374451 gen_loss 11.045413\n",
      "disc_loss -79.5587311 gen_loss 92.3476791\n",
      "disc_loss -33.4795647 gen_loss 8.91453743\n",
      "disc_loss -30.0159073 gen_loss -6.41575527\n",
      "disc_loss -34.2804565 gen_loss -3.50655985\n",
      "disc_loss -24.0579624 gen_loss -3.06944633\n",
      "disc_loss -30.6960373 gen_loss -8.35384083\n",
      "disc_loss -30.1153107 gen_loss -5.46327496\n",
      "disc_loss -32.968689 gen_loss -6.21929264\n",
      "disc_loss -31.0085449 gen_loss -11.4395695\n",
      "disc_loss -37.5633163 gen_loss -6.12257338\n",
      "disc_loss -42.3042831 gen_loss -4.4338789\n",
      "disc_loss -30.6860867 gen_loss -8.33531284\n",
      "disc_loss -33.7654457 gen_loss -8.08669\n",
      "disc_loss -21.6187057 gen_loss -1.47775245\n",
      "disc_loss -17.2031479 gen_loss -10.4699993\n",
      "disc_loss -18.6205654 gen_loss -8.79923534\n",
      "disc_loss -27.78022 gen_loss 1.51835632\n",
      "disc_loss -51.9731026 gen_loss 38.9738541\n",
      "disc_loss -73.2873383 gen_loss 76.581459\n",
      "disc_loss -33.2665253 gen_loss -5.18499517\n",
      "disc_loss -28.8921852 gen_loss -7.71561337\n",
      "disc_loss -20.8333969 gen_loss -4.22059393\n",
      "disc_loss -14.0716419 gen_loss -7.01890564\n",
      "disc_loss -12.9400597 gen_loss -4.67971659\n",
      "disc_loss -12.4154339 gen_loss -9.69734287\n",
      "disc_loss -11.0261593 gen_loss -4.02999496\n",
      "disc_loss -14.7673912 gen_loss -4.61868811\n",
      "disc_loss -14.7342739 gen_loss -4.98252106\n",
      "disc_loss -21.6761856 gen_loss 2.7731092\n",
      "disc_loss -7.710145 gen_loss -2.66615558\n",
      "disc_loss -14.8679705 gen_loss -7.69776392\n",
      "disc_loss -18.4814224 gen_loss -9.30859184\n",
      "disc_loss -26.4165306 gen_loss -15.3831539\n",
      "disc_loss -20.2812481 gen_loss -3.27498865\n",
      "disc_loss -35.3226624 gen_loss 1.62407911\n",
      "disc_loss -16.5973778 gen_loss -8.08841896\n",
      "disc_loss -18.1421204 gen_loss -17.8283882\n",
      "disc_loss -18.9390812 gen_loss -1.16158545\n",
      "disc_loss -25.8693466 gen_loss 20.2506256\n",
      "disc_loss -52.029007 gen_loss 66.1465683\n",
      "disc_loss -43.1381073 gen_loss 36.2390671\n",
      "disc_loss -20.2974434 gen_loss -3.451545\n",
      "disc_loss -25.3632622 gen_loss -4.79868937\n",
      "disc_loss -21.0391388 gen_loss -4.02634811\n",
      "disc_loss -22.5995312 gen_loss -7.69308853\n",
      "disc_loss -14.2954321 gen_loss -3.07713556\n",
      "disc_loss -11.3362265 gen_loss -0.353346348\n",
      "disc_loss -10.6855106 gen_loss -6.15703583\n",
      "disc_loss -20.0015678 gen_loss 5.90619183\n",
      "disc_loss -17.8411903 gen_loss 2.34835124\n",
      "disc_loss -23.0242577 gen_loss -0.677018762\n",
      "disc_loss -19.3080578 gen_loss -11.3498287\n",
      "disc_loss -22.8918037 gen_loss -5.80230474\n",
      "disc_loss -18.174202 gen_loss -8.27796268\n",
      "disc_loss -10.3475952 gen_loss 5.31859446\n",
      "disc_loss -14.4386806 gen_loss 12.2996635\n",
      "disc_loss -15.9495449 gen_loss 15.4798431\n",
      "disc_loss -10.6737 gen_loss 11.5781422\n",
      "disc_loss -1.52328551 gen_loss 0.817295432\n",
      "disc_loss -3.30976319 gen_loss 1.31611621\n",
      "disc_loss -6.72809124 gen_loss -2.16142845\n",
      "disc_loss -9.65300655 gen_loss 1.60818195\n",
      "disc_loss -4.77510357 gen_loss -3.68382335\n",
      "disc_loss -2.25391722 gen_loss -2.46838307\n",
      "disc_loss -11.3556976 gen_loss 1.11549318\n",
      "disc_loss -17.9413319 gen_loss -3.80571556\n",
      "disc_loss -13.4141903 gen_loss 6.24483585\n",
      "disc_loss -16.0915394 gen_loss 2.1928153\n",
      "disc_loss -12.727375 gen_loss 2.86704779\n",
      "disc_loss -5.05818129 gen_loss 0.534906805\n",
      "disc_loss -5.69506693 gen_loss 1.5854969\n",
      "disc_loss -5.40581799 gen_loss 5.13678837\n",
      "disc_loss -7.91313887 gen_loss -0.0546104424\n",
      "disc_loss -12.7279549 gen_loss -2.02505231\n",
      "disc_loss -10.4683504 gen_loss 0.653790116\n",
      "disc_loss -9.46655273 gen_loss 0.857289314\n",
      "disc_loss -9.84742928 gen_loss -0.791596711\n",
      "disc_loss -11.5511837 gen_loss -0.124449924\n",
      "disc_loss -20.1672516 gen_loss -1.66637576\n",
      "disc_loss -9.86708927 gen_loss 0.380508155\n",
      "disc_loss -10.443367 gen_loss 2.38400722\n",
      "disc_loss -32.6705322 gen_loss 23.1202641\n",
      "disc_loss -23.3011417 gen_loss -0.951408744\n",
      "disc_loss -16.9732742 gen_loss -1.73597217\n",
      "disc_loss -20.4264908 gen_loss -1.29297292\n",
      "disc_loss -14.5304089 gen_loss 0.702150822\n",
      "disc_loss -17.718338 gen_loss 1.67421222\n",
      "disc_loss -23.023735 gen_loss -1.09015393\n",
      "disc_loss -19.4798431 gen_loss -4.30206251\n",
      "disc_loss -19.894783 gen_loss -4.34363556\n",
      "disc_loss -22.9471359 gen_loss 0.761487365\n",
      "disc_loss -14.3083591 gen_loss 2.38959074\n",
      "disc_loss -52.9021835 gen_loss 2.78668737\n",
      "disc_loss -38.4675903 gen_loss -12.345068\n",
      "disc_loss -43.4799728 gen_loss -12.8543854\n",
      "disc_loss -41.5457077 gen_loss 15.7830324\n",
      "disc_loss -54.986 gen_loss 34.3777771\n",
      "disc_loss -26.1725063 gen_loss -6.01910925\n",
      "disc_loss -25.9601307 gen_loss -5.70960665\n",
      "disc_loss -48.8063965 gen_loss -12.6609459\n",
      "disc_loss -50.0740089 gen_loss -19.4317665\n",
      "disc_loss -17.9783287 gen_loss 0.912549138\n",
      "disc_loss -38.5869141 gen_loss -3.84051251\n",
      "disc_loss -49.0175323 gen_loss -14.7770205\n",
      "disc_loss -49.141716 gen_loss 16.5105267\n",
      "disc_loss -41.9696236 gen_loss 19.9814873\n",
      "disc_loss -33.4970436 gen_loss 9.59060097\n",
      "disc_loss -43.4444504 gen_loss 0.183418468\n",
      "disc_loss -34.1174469 gen_loss -10.0960217\n",
      "disc_loss -26.0102863 gen_loss -5.87028313\n",
      "disc_loss -19.9475918 gen_loss -8.66862297\n",
      "disc_loss -24.5301228 gen_loss -8.45369148\n",
      "disc_loss -34.3741646 gen_loss -11.0895023\n",
      "disc_loss -51.5181961 gen_loss -1.3073597\n",
      "disc_loss -66.1813049 gen_loss 37.4792404\n",
      "disc_loss -30.9899826 gen_loss 7.67380667\n",
      "disc_loss -24.0319195 gen_loss -6.73530054\n",
      "disc_loss -13.8970881 gen_loss -1.63543534\n",
      "disc_loss -12.5017452 gen_loss 1.57151103\n",
      "disc_loss -8.51128387 gen_loss -2.25355768\n",
      "disc_loss -23.3579273 gen_loss -0.85672152\n",
      "disc_loss -15.2814054 gen_loss -7.99895096\n",
      "disc_loss -27.69631 gen_loss 3.59076\n",
      "disc_loss -19.0025082 gen_loss 17.677084\n",
      "disc_loss -26.9762611 gen_loss 5.86736917\n",
      "disc_loss -16.0698967 gen_loss 12.5376835\n",
      "disc_loss -26.7273 gen_loss 5.72445\n",
      "disc_loss -35.0316315 gen_loss -2.72808647\n",
      "disc_loss -36.5551529 gen_loss -8.46830654\n",
      "disc_loss -38.4187508 gen_loss -6.20016098\n",
      "disc_loss -49.4130745 gen_loss -14.5839558\n",
      "disc_loss -38.7358322 gen_loss 38.952858\n",
      "disc_loss -112.810532 gen_loss 77.3951\n",
      "disc_loss -38.4732437 gen_loss 0.0147388456\n",
      "disc_loss -39.6567116 gen_loss -18.5718765\n",
      "disc_loss -59.0781212 gen_loss -7.40162659\n",
      "disc_loss -52.6407623 gen_loss -10.5241127\n",
      "disc_loss -71.6397171 gen_loss -15.5589828\n",
      "disc_loss -46.0173569 gen_loss -22.3223515\n",
      "disc_loss -17.5303402 gen_loss 6.95035553\n",
      "disc_loss -23.9450016 gen_loss -9.51417732\n",
      "disc_loss -19.2945309 gen_loss -15.957983\n",
      "disc_loss -19.6752129 gen_loss 2.43847585\n",
      "disc_loss -17.3378754 gen_loss -19.7741699\n",
      "disc_loss -31.7232151 gen_loss 2.77905154\n",
      "disc_loss -33.5348701 gen_loss -10.9951172\n",
      "disc_loss -17.8826351 gen_loss -10.390646\n",
      "disc_loss -22.7445145 gen_loss 2.09916353\n",
      "disc_loss -49.0539932 gen_loss 42.2832947\n",
      "disc_loss -32.6413498 gen_loss 8.2292757\n",
      "disc_loss -26.6959629 gen_loss 2.5595808\n",
      "disc_loss -27.4706173 gen_loss 6.31774044\n",
      "disc_loss -23.3781929 gen_loss -0.629245758\n",
      "disc_loss -11.4379721 gen_loss -7.61873484\n",
      "disc_loss -19.8265457 gen_loss -0.686449528\n",
      "disc_loss -16.4825554 gen_loss -6.86716747\n",
      "disc_loss -18.3029327 gen_loss 5.12111139\n",
      "disc_loss -9.65403938 gen_loss 2.31043935\n",
      "disc_loss -14.3262157 gen_loss -2.55089235\n",
      "disc_loss -13.4686422 gen_loss -2.05001569\n",
      "disc_loss -19.4037743 gen_loss 13.9763279\n",
      "disc_loss -20.736248 gen_loss 5.83206701\n",
      "disc_loss -8.48687649 gen_loss -3.02278399\n",
      "disc_loss -12.9299603 gen_loss 14.0806046\n",
      "disc_loss -18.1654701 gen_loss 3.18700409\n",
      "disc_loss -15.3191967 gen_loss 18.6645222\n",
      "disc_loss -15.6561737 gen_loss 12.7801027\n",
      "disc_loss -11.1723814 gen_loss 3.15481353\n",
      "disc_loss -9.14353943 gen_loss 1.78842199\n",
      "disc_loss -7.43079472 gen_loss 0.880210519\n",
      "disc_loss -8.13180447 gen_loss 2.07327032\n",
      "disc_loss -13.8638496 gen_loss -0.214868739\n",
      "disc_loss -20.827549 gen_loss 1.70837235\n",
      "disc_loss -20.0189819 gen_loss 0.103441909\n",
      "disc_loss -7.50768948 gen_loss 2.8015\n",
      "disc_loss -12.0472288 gen_loss 1.24093699\n",
      "disc_loss -23.6174736 gen_loss -5.00209856\n",
      "disc_loss -30.1458035 gen_loss -9.30804157\n",
      "disc_loss -20.1658745 gen_loss -6.5340209\n",
      "disc_loss -21.6685448 gen_loss -3.014256\n",
      "disc_loss -25.6403713 gen_loss 0.973982692\n",
      "disc_loss -22.0890083 gen_loss 0.156897739\n",
      "disc_loss -25.0824699 gen_loss -1.36633408\n",
      "disc_loss -18.0687332 gen_loss 0.83586657\n",
      "disc_loss -12.1218872 gen_loss -4.5401\n",
      "disc_loss -9.28949928 gen_loss -1.50202012\n",
      "disc_loss -7.64770651 gen_loss -2.95825982\n",
      "disc_loss -13.0097771 gen_loss 9.46495\n",
      "disc_loss -24.0501842 gen_loss 20.9671383\n",
      "disc_loss -24.6233101 gen_loss 34.6216927\n",
      "disc_loss -12.0233612 gen_loss 2.89598322\n",
      "disc_loss -16.2604561 gen_loss 5.92154837\n",
      "disc_loss -11.3352957 gen_loss 0.569236338\n",
      "disc_loss -8.783288 gen_loss -1.10447693\n",
      "disc_loss -7.06435633 gen_loss -2.55078959\n",
      "disc_loss -11.9752436 gen_loss -1.6658653\n",
      "disc_loss -14.1265411 gen_loss -4.63103247\n",
      "disc_loss -17.8124847 gen_loss -1.58174491\n",
      "disc_loss -14.2368412 gen_loss -4.72445869\n",
      "disc_loss -7.43125153 gen_loss -2.63516974\n",
      "disc_loss -3.95557761 gen_loss 0.85114634\n",
      "disc_loss -4.37430382 gen_loss 0.734598\n",
      "disc_loss -1.12650466 gen_loss -1.35433078\n",
      "disc_loss -4.1860137 gen_loss 2.29175973\n",
      "disc_loss -3.10125208 gen_loss 0.701802909\n",
      "disc_loss -3.35308051 gen_loss 1.1415087\n",
      "disc_loss -6.5989027 gen_loss 5.68787289\n",
      "disc_loss -10.2205372 gen_loss 13.6902103\n",
      "disc_loss -5.35085487 gen_loss 5.17797565\n",
      "disc_loss -12.9379482 gen_loss 1.19927621\n",
      "disc_loss -16.8544788 gen_loss -1.60626864\n",
      "disc_loss -19.2840919 gen_loss 0.572049797\n",
      "disc_loss -13.2136354 gen_loss 0.616783\n",
      "disc_loss -7.42783165 gen_loss 3.81293106\n",
      "disc_loss -9.92657852 gen_loss 2.38486338\n",
      "disc_loss -5.36997461 gen_loss 0.43024531\n",
      "disc_loss -7.59099865 gen_loss 0.260426223\n",
      "disc_loss -12.181283 gen_loss -3.79962611\n",
      "disc_loss -16.2139378 gen_loss -4.41537142\n",
      "disc_loss -13.681757 gen_loss -5.35291815\n",
      "disc_loss -12.9275875 gen_loss -0.0839783698\n",
      "disc_loss -12.5508347 gen_loss -0.199935481\n",
      "disc_loss -14.181901 gen_loss -1.65010107\n",
      "disc_loss -20.6507092 gen_loss -3.27972841\n",
      "disc_loss -26.2479534 gen_loss 19.460659\n",
      "disc_loss -27.9335041 gen_loss 14.6171007\n",
      "disc_loss -15.1815767 gen_loss -2.33643603\n",
      "disc_loss -30.6746292 gen_loss -4.12514782\n",
      "disc_loss -19.9060707 gen_loss -2.4104321\n",
      "disc_loss -16.9834652 gen_loss -5.62290668\n",
      "disc_loss -25.5783043 gen_loss -3.9952457\n",
      "disc_loss -22.5262527 gen_loss -3.14121222\n",
      "disc_loss -20.4870186 gen_loss -5.54350805\n",
      "disc_loss -19.4595871 gen_loss 8.24884\n",
      "disc_loss -29.0574074 gen_loss 2.29870462\n",
      "disc_loss -34.2442741 gen_loss 8.30074215\n",
      "disc_loss -18.965456 gen_loss 4.74907398\n",
      "disc_loss -33.1788483 gen_loss -5.39796925\n",
      "disc_loss -26.088768 gen_loss -4.28851843\n",
      "disc_loss -38.5157852 gen_loss -4.46269274\n",
      "disc_loss -21.4747448 gen_loss 0.236427695\n",
      "disc_loss -30.7853775 gen_loss -8.87785816\n",
      "disc_loss -22.0522919 gen_loss -11.3579512\n",
      "disc_loss -31.1189442 gen_loss -7.83797\n",
      "disc_loss -46.869194 gen_loss 35.6079712\n",
      "disc_loss -58.0107269 gen_loss 36.3942871\n",
      "disc_loss -14.9292164 gen_loss -4.05290842\n",
      "disc_loss -31.9438763 gen_loss -10.3373766\n",
      "disc_loss -46.5422974 gen_loss -17.2128639\n",
      "disc_loss -26.7163734 gen_loss -4.5003953\n",
      "disc_loss -18.759304 gen_loss -13.2360153\n",
      "disc_loss -24.6140366 gen_loss -11.2536402\n",
      "disc_loss -18.0469856 gen_loss 1.37867296\n",
      "disc_loss -27.9691772 gen_loss -3.44122386\n",
      "disc_loss -34.6923904 gen_loss 5.05109882\n",
      "disc_loss -31.8367786 gen_loss -6.02397108\n",
      "disc_loss -31.2601929 gen_loss -0.639799476\n",
      "disc_loss -40.978138 gen_loss 5.19486523\n",
      "disc_loss -41.604702 gen_loss 3.27085495\n",
      "disc_loss -20.7661209 gen_loss -0.108195305\n",
      "disc_loss -27.8275528 gen_loss -11.1224194\n",
      "disc_loss -26.0231018 gen_loss -5.83009052\n",
      "disc_loss -12.7758799 gen_loss 2.30725932\n",
      "disc_loss -17.503933 gen_loss 7.916471\n",
      "disc_loss -16.7442646 gen_loss 5.75795794\n",
      "disc_loss -24.2347908 gen_loss 12.1706429\n",
      "disc_loss -23.6625519 gen_loss -2.37822652\n",
      "disc_loss -23.3070641 gen_loss 7.63467884\n",
      "disc_loss -7.60054445 gen_loss 3.24614859\n",
      "disc_loss -14.9016552 gen_loss 21.2098045\n",
      "disc_loss -35.1877174 gen_loss 9.55430317\n",
      "disc_loss -23.3020267 gen_loss 5.38066626\n",
      "disc_loss -27.3240471 gen_loss -5.58998251\n",
      "disc_loss -33.8025932 gen_loss 4.84212637\n",
      "disc_loss -47.671196 gen_loss -6.31949615\n",
      "disc_loss -36.4442329 gen_loss -9.28747082\n",
      "disc_loss -49.2003479 gen_loss -6.18796825\n",
      "disc_loss -36.4393158 gen_loss -12.8447113\n",
      "disc_loss -41.5507126 gen_loss 5.33147573\n",
      "disc_loss -46.8541489 gen_loss -4.33588266\n",
      "disc_loss -55.8428726 gen_loss 44.6525955\n",
      "disc_loss -91.4943695 gen_loss 41.3082657\n",
      "disc_loss -51.8752518 gen_loss -14.6194687\n",
      "disc_loss -37.2377968 gen_loss -21.4512177\n",
      "disc_loss -39.7761078 gen_loss -8.83151817\n",
      "disc_loss -19.2315331 gen_loss -4.69477606\n",
      "disc_loss -17.5955963 gen_loss -5.88863277\n",
      "disc_loss -23.093235 gen_loss -5.4603529\n",
      "disc_loss -25.7678566 gen_loss -23.6993351\n",
      "disc_loss -22.7805901 gen_loss -14.116086\n",
      "disc_loss -32.7920303 gen_loss 12.8095417\n",
      "disc_loss -27.8846684 gen_loss -5.20815134\n",
      "disc_loss -23.2084808 gen_loss -11.3518553\n",
      "disc_loss -23.0935459 gen_loss 5.00361538\n",
      "disc_loss -17.3575344 gen_loss 6.81910706\n",
      "disc_loss -32.6987801 gen_loss 48.7198524\n",
      "disc_loss -13.2864075 gen_loss -5.48623657\n",
      "disc_loss -11.8390856 gen_loss 1.68387163\n",
      "disc_loss -9.3038311 gen_loss -1.79657745\n",
      "disc_loss -24.8060322 gen_loss -0.702030957\n",
      "disc_loss -15.0084 gen_loss 8.03883934\n",
      "disc_loss -11.3022022 gen_loss -0.859569073\n",
      "disc_loss -9.13023567 gen_loss 3.77232504\n",
      "disc_loss -15.4475174 gen_loss 2.2683208\n",
      "disc_loss -10.5589027 gen_loss 4.29053259\n",
      "disc_loss -8.60030651 gen_loss 3.90278244\n",
      "disc_loss -9.52152252 gen_loss 1.97171247\n",
      "disc_loss -11.8668213 gen_loss -1.23640037\n",
      "disc_loss -20.6290722 gen_loss -5.55638266\n",
      "disc_loss -8.86120224 gen_loss 2.65188575\n",
      "disc_loss -0.150069237 gen_loss 5.06850672\n",
      "disc_loss -15.2372141 gen_loss 3.00230479\n",
      "disc_loss -12.6051311 gen_loss -5.3629837\n",
      "disc_loss -14.9621067 gen_loss -2.53024936\n",
      "disc_loss -10.6737013 gen_loss -2.53106737\n",
      "disc_loss -10.7750502 gen_loss -0.298620611\n",
      "disc_loss -13.3350391 gen_loss 1.12377989\n",
      "disc_loss -10.9777775 gen_loss 0.339242756\n",
      "disc_loss -13.003088 gen_loss -4.72948694\n",
      "disc_loss -16.2644958 gen_loss 14.2395077\n",
      "disc_loss -17.5544968 gen_loss 13.0296831\n",
      "disc_loss -13.1340914 gen_loss -0.120069124\n",
      "disc_loss -20.7472515 gen_loss -6.61872244\n",
      "disc_loss -23.9892273 gen_loss 0.892366767\n",
      "disc_loss -16.6958332 gen_loss -9.77845\n",
      "disc_loss -22.9905319 gen_loss -5.01372528\n",
      "disc_loss -17.5885887 gen_loss 7.10557556\n",
      "disc_loss -12.8347559 gen_loss -6.64223\n",
      "disc_loss -10.4374533 gen_loss -4.20274115\n",
      "disc_loss -7.45574379 gen_loss 0.387645543\n",
      "disc_loss -2.70916605 gen_loss 5.98185635\n",
      "disc_loss -9.63147926 gen_loss 9.47091198\n",
      "disc_loss -23.6374817 gen_loss -2.65184259\n",
      "disc_loss -15.5910349 gen_loss 9.62009048\n",
      "disc_loss -12.6013756 gen_loss -2.67329168\n",
      "disc_loss -12.3464794 gen_loss -2.50496149\n",
      "disc_loss -14.2654457 gen_loss -3.45245314\n",
      "disc_loss -6.02904034 gen_loss 4.88917065\n",
      "disc_loss -13.6599503 gen_loss 2.56031\n",
      "disc_loss -15.9461985 gen_loss -2.13177037\n",
      "disc_loss -15.7826605 gen_loss 6.61731052\n",
      "disc_loss -18.6252937 gen_loss 6.76266861\n",
      "disc_loss -33.0720177 gen_loss 14.1417408\n",
      "disc_loss -11.4829407 gen_loss 0.478898615\n",
      "disc_loss -7.33659697 gen_loss -1.11119902\n",
      "disc_loss -10.1609516 gen_loss 1.80746233\n",
      "disc_loss -8.45446396 gen_loss 1.23981082\n",
      "disc_loss -1.58457112 gen_loss 1.7671032\n",
      "disc_loss -3.68171549 gen_loss 0.910474181\n",
      "disc_loss -5.01296139 gen_loss 5.69450951\n",
      "disc_loss -6.59693146 gen_loss 2.33439779\n",
      "disc_loss -4.29310083 gen_loss 6.85542\n",
      "disc_loss -4.22580147 gen_loss 3.74034762\n",
      "disc_loss -0.107283294 gen_loss 1.58314657\n",
      "disc_loss -15.3106813 gen_loss -0.433802605\n",
      "disc_loss -20.7678108 gen_loss -4.58741474\n",
      "disc_loss -23.4115601 gen_loss -1.38458896\n",
      "disc_loss -17.1065178 gen_loss -2.91867018\n",
      "disc_loss -8.11168671 gen_loss -1.59219527\n",
      "disc_loss -8.24064255 gen_loss 0.581924498\n",
      "disc_loss -11.059639 gen_loss -1.21157527\n",
      "disc_loss -5.08932257 gen_loss 1.89130235\n",
      "disc_loss -12.320858 gen_loss 0.371804059\n",
      "disc_loss -18.933876 gen_loss -0.58932668\n",
      "disc_loss -17.762991 gen_loss -5.69106865\n",
      "disc_loss -11.6805191 gen_loss -3.59660268\n",
      "disc_loss -7.60962868 gen_loss -1.50690234\n",
      "disc_loss -10.4976902 gen_loss 5.29516745\n",
      "disc_loss -25.5998116 gen_loss 7.00083447\n",
      "disc_loss -27.1412621 gen_loss 14.9158154\n",
      "disc_loss -14.5567122 gen_loss 3.27577972\n",
      "disc_loss -18.6384506 gen_loss 2.64333487\n",
      "disc_loss -23.2818413 gen_loss -5.6399\n",
      "disc_loss -31.371006 gen_loss -1.74992251\n",
      "disc_loss -32.660862 gen_loss -8.24271202\n",
      "disc_loss -26.9009247 gen_loss -7.3584609\n",
      "disc_loss -20.9471054 gen_loss -3.93559957\n",
      "disc_loss -26.6691322 gen_loss -11.0895138\n",
      "disc_loss -8.72175503 gen_loss 0.431423098\n",
      "disc_loss -20.388443 gen_loss -3.36428976\n",
      "disc_loss -35.5647507 gen_loss -3.8607533\n",
      "disc_loss -31.6383 gen_loss 0.631872952\n",
      "disc_loss -16.4927807 gen_loss -0.63174516\n",
      "disc_loss -16.4214935 gen_loss -6.07660675\n",
      "disc_loss -25.5578842 gen_loss -1.38643324\n",
      "disc_loss -40.0772438 gen_loss -7.7503252\n",
      "disc_loss -62.6076698 gen_loss 27.4139462\n",
      "disc_loss -49.3193855 gen_loss 10.3854952\n",
      "disc_loss -32.4452095 gen_loss -8.11708736\n",
      "disc_loss -35.1662 gen_loss -4.06664371\n",
      "disc_loss -25.2036629 gen_loss -1.29634404\n",
      "disc_loss -15.6919651 gen_loss -1.38627839\n",
      "disc_loss -12.2620716 gen_loss -1.20643508\n",
      "disc_loss -39.824585 gen_loss -14.0875416\n",
      "disc_loss -23.9965057 gen_loss 4.03671408\n",
      "disc_loss -22.4075203 gen_loss -8.19668579\n",
      "disc_loss -17.1954 gen_loss 2.44491053\n",
      "disc_loss -29.2922554 gen_loss -3.97905302\n",
      "disc_loss -12.7202263 gen_loss -5.49634171\n",
      "disc_loss -26.4428959 gen_loss -8.69784355\n",
      "disc_loss -11.9718914 gen_loss 9.2029047\n",
      "disc_loss -25.4516983 gen_loss -3.22723842\n",
      "disc_loss -10.9596491 gen_loss -7.07861614\n",
      "disc_loss -16.3377838 gen_loss 9.14353752\n",
      "disc_loss -26.2153358 gen_loss 30.0020638\n",
      "disc_loss -19.634346 gen_loss 6.75512695\n",
      "disc_loss -18.7923889 gen_loss -2.39473128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@tf.function\n",
    "def train_step(target_audios):\n",
    "\n",
    "\n",
    "  for i in range(5):\n",
    "\n",
    "      # Get the shape of the target_audios tensor\n",
    "      shape = tf.shape(target_audios)\n",
    "\n",
    "\n",
    "      # Generate noise using tf.random.normal()\n",
    "      noise = tf.random.normal((shape))\n",
    "      generated_audio = generator(noise, training=True)\n",
    "      with tf.GradientTape() as disc_tape:\n",
    "\n",
    "          real_output = discriminator(target_audios, training=True)\n",
    "          fake_output = discriminator(generated_audio, training=True)\n",
    "          gp = gradient_penalty(target_audios, generated_audio, discriminator)\n",
    "          disc_loss = discriminator_loss(real_output, fake_output, gp)\n",
    "\n",
    "      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "      #clip_discriminator_weights(discriminator)\n",
    "\n",
    "      if i ==0:\n",
    "          weights = discriminator.get_weights()\n",
    "\n",
    "\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "    noise = tf.random.normal(shape=(target_audios.shape))\n",
    "    generated_audio = generator(noise, training=True)\n",
    "    fake_output = discriminator(generated_audio, training=True)\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "  discriminator.set_weights(weights)\n",
    "\n",
    "\n",
    "  tf.print(\"disc_loss\",disc_loss,'gen_loss',gen_loss)\n",
    "\n",
    "\n",
    "def train(generator, discriminator\n",
    "          , epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(len(y_train) // batch_size):\n",
    "            #images = x_train[batch * batch_size: (batch+1) * batch_size]\n",
    "            target_audios = y_train[batch * batch_size: (batch+1) * batch_size]\n",
    "\n",
    "            train_step(target_audios)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            test = generator.predict(noise)\n",
    "            audio_data = test.ravel()\n",
    "            # Play the audio within the Jupyter Notebook\n",
    "            Audio(data=audio_data, rate=sr/reduction)\n",
    "\n",
    "\n",
    "# Train the GAN\n",
    "EPOCHS = 2000000\n",
    "BATCH_SIZE = 10\n",
    "num_unrolling_steps = 20  # Set the desired number of unrolling steps\n",
    "train(generator, discriminator, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "mrmv1Zri-jjc",
    "outputId": "2664ddc0-4025-48b9-bb57-3fbb83a922cc"
   },
   "outputs": [],
   "source": [
    "test = generator.predict(noise)\n",
    "audio_data = test.ravel()\n",
    "# Play the audio within the Jupyter Notebook\n",
    "Audio(data=audio_data, rate=sr/reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Overview of Colaboratory Features",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
