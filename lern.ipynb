{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/lern.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2370Z9yrygMt",
        "outputId": "2a944cd1-49fa-4318-af83-967defbe4415"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas-ta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_9Y93vMykBd",
        "outputId": "825cf2d2-1212-428a-a9b1-0ede36ad7ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas-ta in /usr/local/lib/python3.10/dist-packages (0.3.14b0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pandas-ta) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas-ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas-ta) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas-ta) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->pandas-ta) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "from scipy.stats import boxcox\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "WdAXBSQwyyZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define functions**"
      ],
      "metadata": {
        "id": "zp8skWb9K0du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_largest_value_array(Data, columns):\n",
        "    # Subset the data frame to only include the specified columns\n",
        "    sub_data = Data[columns]\n",
        "\n",
        "    # Use the apply() function with the max() function to find the maximum value in each row of the subsetted data frame\n",
        "    max_values = sub_data.apply(max, axis=1)\n",
        "\n",
        "    # Create a new array that takes the maximum value from step 2 for each row of the original data frame\n",
        "    max_array = np.array(max_values)\n",
        "    \n",
        "    return max_array\n",
        "  \n",
        "def get_smallest_value_array(Data, columns):\n",
        "    # Subset the data frame to only include the specified columns\n",
        "    sub_data = Data[columns]\n",
        "\n",
        "    # Use the min() function to find the minimum value in each row of the subsetted data frame\n",
        "    min_values = sub_data.min(axis=1)\n",
        "\n",
        "    # Create a new array that takes the minimum value from step 2 for each row of the original data frame\n",
        "    min_array = np.array(min_values)\n",
        "    \n",
        "    return min_array\n",
        "\n",
        "def get_array_bo_greater_than_bc(Data):\n",
        "    # Use np.where() to compare the \"BO\" and \"BC\" columns, return 1 if \"BO\" < \"BC\", and 0 otherwise\n",
        "    result_array = np.where(Data[\"BO\"] < Data[\"BC\"], 1, 0)\n",
        "    \n",
        "    return result_array\n",
        "\n",
        "def generate_dataframe(func, start, end, step, data, name):\n",
        "    result_df = pd.DataFrame()\n",
        "\n",
        "    for param in np.arange(start, end, step):\n",
        "        result_array = func(param, data)\n",
        "        temp_df = pd.DataFrame(result_array, columns=[name+str(param)])\n",
        "        result_df = pd.concat([result_df, temp_df], axis=1)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "def equalize_dataframes(df1, df2):\n",
        "    row_count_diff = len(df1) - len(df2)\n",
        "    \n",
        "    if row_count_diff > 0:\n",
        "        # df1 has more rows, remove the first rows from df1\n",
        "        df1 = df1.iloc[row_count_diff:]\n",
        "    elif row_count_diff < 0:\n",
        "        # df2 has more rows, remove the first rows from df2\n",
        "        df2 = df2.iloc[-row_count_diff:]\n",
        "    \n",
        "    return df1, df2\n",
        "def drop_rows_with_nan(df):\n",
        "    # drop any row with NaN values in the specified columns\n",
        "    df.dropna(subset=df.columns, inplace=True)\n",
        "    \n",
        "    # reset the index after dropping rows\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    # return the cleaned data frame\n",
        "    return df"
      ],
      "metadata": {
        "id": "ssMSTnooIvRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data**"
      ],
      "metadata": {
        "id": "nyKLo8j9986v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data = pd.read_csv('/content/drive/MyDrive/eurusd_hour.csv')\n",
        "Data['Date'] = pd.to_datetime(Data['Date'])\n",
        "Data['Time'] = pd.to_datetime(Data['Time'])\n",
        "print(Data)"
      ],
      "metadata": {
        "id": "vKH7LWXjy7fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract time information**"
      ],
      "metadata": {
        "id": "3-TkkeCc9yWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Date = pd.DataFrame()\n",
        "\n",
        "Date['day'] = Data['Date'].dt.day\n",
        "Date['month'] = Data['Date'].dt.month\n",
        "Date['weekday'] = Data['Date'].dt.weekday\n",
        "Date['hour_of_day'] = Data['Time'].dt.hour\n",
        "\n",
        "print(Date)"
      ],
      "metadata": {
        "id": "EFY_8-AP0FAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Candle stick and other chart info**"
      ],
      "metadata": {
        "id": "4HqfCY9rTs9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Shape = pd.DataFrame()\n",
        "\n",
        "abs_diff_array = np.abs(Data['BH'] - Data['BL'])\n",
        "\n",
        "updown_array = get_array_bo_greater_than_bc(Data)\n",
        "\n",
        "max_array = get_largest_value_array(Data,[\"BC\", \"BO\"])\n",
        "min_array = get_smallest_value_array(Data,[\"BC\", \"BO\"])\n",
        "\n",
        "tp_array = (Data['BH']-min_array)/abs_diff_array\n",
        "bp_array = (Data['BH']-max_array)/abs_diff_array\n",
        "\n",
        "RelChange = Data['BC'] - Data['BO']\n",
        "\n",
        "Shape[\"Direction\"] = updown_array\n",
        "Shape[\"Bottom_Point\"] = bp_array\n",
        "Shape[\"Top_Point\"] = tp_array\n",
        "Shape[\"Relative_Change\"] = RelChange\n",
        "Shape[\"Size\"] = abs_diff_array\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(Shape)"
      ],
      "metadata": {
        "id": "0A0q2FME9xnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Technical analysis**"
      ],
      "metadata": {
        "id": "lTIu3koWWoMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "OHLC = Data[[\"BC\",\"BL\",\"BH\",\"BO\"]]\n",
        "\n",
        "\n",
        "def sma_func(hyperparameter, data):\n",
        "    # Calculate the Simple Moving Average (SMA) using the ta.sma() function\n",
        "    close = data[\"BC\"]\n",
        "    sma_result = ta.sma(close, length=hyperparameter)-close\n",
        "\n",
        "    # Convert the resulting Series to a NumPy array\n",
        "    result_array = sma_result.to_numpy()\n",
        "\n",
        "    return result_array\n",
        "\n",
        "def stdev_func(hyperparameter, data):\n",
        "    close = data[\"BC\"]\n",
        "    # Calculate the Simple Moving Average (SMA) using the ta.sma() function\n",
        "    sma_result = ta.stdev(close, length=hyperparameter)\n",
        "\n",
        "    # Convert the resulting Series to a NumPy array\n",
        "    result_array = sma_result.to_numpy()\n",
        "\n",
        "    return result_array\n",
        "\n",
        "def rsi_func(hyperparameter, data):\n",
        "    close = data[\"BC\"]\n",
        "    # Calculate the Simple Moving Average (SMA) using the ta.sma() function\n",
        "    sma_result = ta.rsi(close, length=hyperparameter)\n",
        "\n",
        "    # Convert the resulting Series to a NumPy array\n",
        "    result_array = sma_result.to_numpy()\n",
        "\n",
        "    return result_array\n",
        "\n",
        "def cci_func(hyperparameter, data):\n",
        "    close = data[\"BC\"]\n",
        "    high = data[\"BH\"]\n",
        "    low = data[\"BL\"]\n",
        "    # Calculate the Simple Moving Average (SMA) using the ta.sma() function\n",
        "    sma_result = ta.cci(high,low,close, length=hyperparameter)\n",
        "\n",
        "    # Convert the resulting Series to a NumPy array\n",
        "    result_array = sma_result.to_numpy()\n",
        "\n",
        "    return result_array\n",
        "\n",
        "\n",
        "rsi = generate_dataframe(rsi_func,10,100,2,OHLC,\"RSI_\")\n",
        "\n",
        "sma1 = generate_dataframe(sma_func,10,100,5,OHLC,\"SMA_\")\n",
        "sma2 = generate_dataframe(sma_func,100,1000,10,OHLC,\"SMA_\")\n",
        "\n",
        "stdev = generate_dataframe(stdev_func,10,300,5,OHLC,\"STDEV_\")\n",
        "\n",
        "cci = generate_dataframe(cci_func,10,200,20,OHLC,\"CCI_\")\n",
        "\n",
        "\n",
        "Technicals = pd.concat([sma1, sma2, stdev,rsi,cci], axis=1)\n",
        "\n",
        "#print(stdev1)\n",
        "\n",
        "plt.plot(cci)\n",
        "plt.xlim(2000,2500)\n",
        "#plt.ylim(-.05,.05)"
      ],
      "metadata": {
        "id": "NUr16CliWmoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combine dataframes and prepare data**"
      ],
      "metadata": {
        "id": "zg52WHlz9qtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fulldata = pd.concat([Date,Shape,Technicals,],axis=1)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(fulldata)\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=fulldata.columns)\n",
        "\n",
        "pt = preprocessing.PowerTransformer(method='box-cox',standardize=True)\n",
        "\n",
        "def boxcox_transform(df):\n",
        "    pt = preprocessing.PowerTransformer(method='yeo-johnson',standardize=True)\n",
        "    df_boxcox = pd.DataFrame(pt.fit_transform(df), columns=df.columns)\n",
        "    return df_boxcox\n",
        "\n",
        "\n",
        "newshoe = boxcox_transform(fulldata)\n",
        "\n",
        "#print(fulldata)\n",
        "#plt.plot(scaled_df)\n",
        "hist = newshoe.hist(bins=20, figsize=(12, 8), grid=False, color='#86bf91', rwidth=0.9)\n",
        "#plt.plot(newshoe)\n",
        "plt.xlim(5000,5100)\n",
        "plt.ylim(-5,5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2meJx2v29oxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create target then remove nans from all data rows**"
      ],
      "metadata": {
        "id": "--LdOKgpClXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Target = pd.DataFrame()\n",
        "Target[\"Target\"] = Data['BC'] - Data['BO']\n",
        "\n",
        "Target[\"Target\"] = Target[\"Target\"].shift(-1)\n",
        "\n",
        "\n",
        "combine = pd.concat([Target,newshoe],axis=1)\n",
        "\n",
        "combine = drop_rows_with_nan(combine)\n",
        "\n",
        "Target = combine.pop(\"Target\")\n",
        "\n",
        "tester = Target\n",
        "\n",
        "# Create MinMaxScaler object with desired scale range (-1, 1)\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "# Scale the DataFrame\n",
        "scaled_data = scaler.fit_transform(Target.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Convert the scaled data back to a DataFrame\n",
        "Target = pd.DataFrame(data=scaled_data, columns=['Target'])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(combine, Target, test_size=0.33, random_state=42,shuffle=False)\n",
        "\n",
        "#plt.plot(tester*1000)\n",
        "plt.plot(X_train)\n",
        "plt.xlim(1000,1100)\n",
        "print(X_train)\n"
      ],
      "metadata": {
        "id": "_rQ_l4mlCoHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regulizer = l2(.001)\n",
        "\n",
        "# create a sequential model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Dense(230, activation='gelu'))\n",
        "model.add(Dropout(.2))\n",
        "model.add(Dense(50, activation='gelu',))\n",
        "model.add(Dropout(.1))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# compile the model with categorical crossentropy loss and Adam optimizer\n",
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "metadata": {
        "id": "QEogdpUhc_7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model on your data\n",
        "history = model.fit(X_train, y_train, epochs=10,batch_size=32,validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "F3vQ8itEdDtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')"
      ],
      "metadata": {
        "id": "6BdlwFXULyKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "9aIUC1iZd7Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# set the index to start at 0\n",
        "y_test.index = range(y_test.shape[0])\n",
        "plt.plot(y_test)\n",
        "plt.plot(predictions)\n",
        "plt.xlim(9000,10500)\n",
        "plt.ylim(-0.2,.02)\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "LU7cWkt2eTt6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}