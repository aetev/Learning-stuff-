{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/3%20action%20deep%20q%20regularized%20(2)%20(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWAdRqLdGEyf",
        "outputId": "75d2c4a1-9c4a-4ef5-cfb5-f50c08794af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIPBOzgXLKp5",
        "outputId": "ec34f8fd-5e5a-4c79-94ad-4f90b6e36764",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas_ta in /usr/local/lib/python3.10/dist-packages (0.3.14b0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pandas_ta) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->pandas_ta) (1.16.0)\n",
            "Requirement already satisfied: tf_agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.20.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.20.1)\n",
            "Requirement already satisfied: rlds in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: dm-reverb~=0.12.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.12.0)\n",
            "Requirement already satisfied: tensorflow~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.13.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.12.0->tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.12.0->tf_agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (0.33.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.20.1->tf_agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf_agents[reverb]) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2.3.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.12.0->tf_agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas_ta\n",
        "!pip install tf_agents[reverb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mknrqfmnKQSP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_ta as ta\n",
        "import matplotlib.pyplot as plt\n",
        "import reverb\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import layers\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import epsilon_greedy_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies.epsilon_greedy_policy import EpsilonGreedyPolicy\n",
        "from tf_agents.drivers import dynamic_episode_driver\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "81IQ0kMGZEBK"
      },
      "outputs": [],
      "source": [
        "num_iterations = 2000000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 1000  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 3  # @param {type:\"integer\"}\n",
        "eval_interval = 10000  # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vaIxTZWCHydp"
      },
      "outputs": [],
      "source": [
        "def get_relative_change(df, column_name):\n",
        "    relative_changes = []\n",
        "    for i in range(len(df)):\n",
        "        if i == 0:\n",
        "            relative_changes.append(0)  # First element has no previous value\n",
        "        else:\n",
        "            relative_change = (df.iloc[i] - df.iloc[i-1]) / df.iloc[i-1]\n",
        "            relative_changes.append(relative_change)\n",
        "    return pd.DataFrame({column_name: relative_changes})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iwSR4EWTLbDK"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/eurusd_hour.csv')\n",
        "close = df['AC']\n",
        "open = df['AO']\n",
        "high = df['AH']\n",
        "low = df['AL']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szXaEp64McUL"
      },
      "outputs": [],
      "source": [
        "RSI = ta.rsi(close,14,scalar=1)\n",
        "AROON = ta.aroon(high,low,14,scalar = 1)\n",
        "AROON_UP = AROON['AROOND_14']\n",
        "AROON_DOWN = AROON['AROONU_14']\n",
        "CCI = ta.cci(high,low,close,14)\n",
        "CCI = CCI.multiply(.001)\n",
        "CCI = CCI.add(.5)\n",
        "RVI = ta.rvi(close,high,low,14,scalar=1)\n",
        "CHANGE = get_relative_change(close,\"Change\")\n",
        "\n",
        "sma = ta.sma(close,20,offset = -20)\n",
        "sma2 = ta.sma(close,30,offset = -20)\n",
        "DIFF = sma-sma2\n",
        "DIFF = DIFF/np.std(DIFF)\n",
        "DIFF.name = 'DIFF'\n",
        "\n",
        "INDICATORS = pd.concat([CHANGE,RSI,AROON_UP,AROON_DOWN,CCI,RVI,DIFF],axis=1)\n",
        "\n",
        "INDICATORS = INDICATORS.dropna(axis=0)\n",
        "INDICATORS = INDICATORS.reset_index()\n",
        "\n",
        "change = INDICATORS.pop('Change')\n",
        "diff = INDICATORS.pop('DIFF')\n",
        "INDICATORS.pop('index')\n",
        "\n",
        "INDICATORS = INDICATORS.to_numpy()\n",
        "change = change.to_numpy()\n",
        "change_dev = change.std()\n",
        "change = change/change_dev\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlim(20000,20200)\n",
        "plt.ylim(-4,4)\n",
        "print(diff.shape)\n",
        "plt.plot(np.cumsum(change))\n",
        "plt.plot(diff)"
      ],
      "metadata": {
        "id": "vn2ZV_zEhVxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggc0UJ3vbWgf"
      },
      "outputs": [],
      "source": [
        "print(INDICATORS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XMF1wKV6e_X"
      },
      "outputs": [],
      "source": [
        "plt.plot(INDICATORS)\n",
        "plt.xlim(5400,5500)\n",
        "#print(INDICATORS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcjEVE3Rh_5l"
      },
      "outputs": [],
      "source": [
        "def test_environment(env, num_steps):\n",
        "    # Reset the environment\n",
        "    observation = env.reset()\n",
        "\n",
        "    # Get the upper bound of the action range\n",
        "    action_spec = env.action_spec().maximum\n",
        "\n",
        "    # Loop through the time steps and take random actions\n",
        "    for _ in range(num_steps):\n",
        "        # Select a random action from the range of valid values\n",
        "        action = random.randint(0, action_spec)\n",
        "\n",
        "        # Execute the action and get the next observation, reward, done, and info\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # Print the current time step\n",
        "        np.set_printoptions(precision=4)\n",
        "        print(reward,action,info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQreCJAi-KAU"
      },
      "outputs": [],
      "source": [
        "max_ep_len = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoVru2TRfnAz"
      },
      "outputs": [],
      "source": [
        "class TradingEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int64, minimum=0, maximum=3, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(8,), dtype=np.float32, minimum=-100,maximum=100, name='observation')\n",
        "\n",
        "\n",
        "    self._state = [0,0,0,0,0,0,0,0]\n",
        "    self._episode_ended = False\n",
        "    self._count = random.randint(0,60000)\n",
        "    self._end_ep = self._count+max_ep_len\n",
        "    self._balance = 0\n",
        "    self._actions = [0,0,0,0]\n",
        "    self.trade_info = {\n",
        "    'value': 0,\n",
        "    'time': 0,\n",
        "    'direction': 0,\n",
        "    }\n",
        "\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "\n",
        "    self._state = [0,0,0,0,0,0,0,0]\n",
        "    self._episode_ended = False\n",
        "    self._count = random.randint(0,60000)\n",
        "    self._end_ep = self._count+max_ep_len\n",
        "    self._balance = 0\n",
        "    self._actions = [0,0,0,0]\n",
        "    self.trade_info = {\n",
        "    'value': 0,\n",
        "    'time': 0,\n",
        "    'direction': 0,\n",
        "    }\n",
        "\n",
        "    return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "  def _step(self, action):\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "    profit = 0\n",
        "    cur_direction = self.trade_info['direction']\n",
        "    hold_penelty = 0\n",
        "    hold_reward = 0\n",
        "\n",
        "\n",
        "    if action == 0:\n",
        "      self._actions[0] = 1\n",
        "      profit = self.trade_info['value']-.05\n",
        "      self.trade_info['value'] = change[self._count]\n",
        "      self.trade_info['time'] = 0\n",
        "      self.trade_info['direction'] = 1\n",
        "      hold_reward = (diff[self._count]*1)*.25\n",
        "    elif action == 1:\n",
        "      self._actions[1] = 1\n",
        "      hold_reward = -.05\n",
        "      profit = self.trade_info['value']-.05\n",
        "      self.trade_info['value'] = -change[self._count]\n",
        "      self.trade_info['time'] = 0\n",
        "      self.trade_info['direction'] = -1\n",
        "      hold_reward = (diff[self._count]*-1)*.25\n",
        "    elif action == 2:\n",
        "      self._actions[2] = 1\n",
        "      profit = self.trade_info['value']\n",
        "      hold_reward = (-abs(diff[self._count])+.5)*.25\n",
        "      self.trade_info['value'] = 0\n",
        "      self.trade_info['time'] = 0 if cur_direction != 0 else self.trade_info['time'] + .01\n",
        "      self.trade_info['direction'] = 0\n",
        "    elif action == 3:\n",
        "      self._actions[3] = 1\n",
        "      if cur_direction != 0:\n",
        "        hold_reward = diff[self._count]*cur_direction\n",
        "      else:\n",
        "        hold_reward = (-abs(diff[self._count])+.5)*.25\n",
        "      hold_penelty = -.05 if cur_direction == 0 else 0\n",
        "      self.trade_info['value'] += change[self._count]*cur_direction\n",
        "      self.trade_info['time'] += .01\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    self._balance += profit\n",
        "\n",
        "    info_list = [\n",
        "        self.trade_info['value'],\n",
        "        self.trade_info['time'],\n",
        "        self.trade_info['direction']\n",
        "    ]\n",
        "\n",
        "\n",
        "    #update state\n",
        "    self._state = np.append(INDICATORS[self._count],info_list)\n",
        "\n",
        "    if self._count >= self._end_ep:\n",
        "      self._episode_ended = True\n",
        "\n",
        "    self._count += 1\n",
        "\n",
        "    if self._episode_ended:\n",
        "      reward = self._balance+self.trade_info['value']\n",
        "      return ts.termination(np.array(self._state, dtype=np.float32), reward=reward)\n",
        "    else:\n",
        "      reward = hold_reward\n",
        "      return ts.transition(\n",
        "          np.array(self._state, dtype=np.float32), reward=reward, discount=.999)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cal = -.1\n",
        "print(cal +.2)"
      ],
      "metadata": {
        "id": "u1AyLWw1XtNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3gkt6dv_0e_"
      },
      "outputs": [],
      "source": [
        "env = TradingEnv()\n",
        "\n",
        "utils.validate_py_environment(env, episodes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3YHkyIzjE3h",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "test_environment(env,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMb4Gnsh_ktX"
      },
      "outputs": [],
      "source": [
        "train_py_env = env\n",
        "eval_py_env = env\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cumpJmHpHJk"
      },
      "outputs": [],
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZGBmV3KI0HG"
      },
      "outputs": [],
      "source": [
        "fc_layer_params = (20, 5)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IspNjyYpK7uX"
      },
      "outputs": [],
      "source": [
        "def activity_regularizer(multiplier):\n",
        "    def regularizer(y_pred):\n",
        "        mean = tf.reduce_mean(y_pred)\n",
        "        variance = tf.math.reduce_variance(y_pred)\n",
        "        return multiplier * (tf.square(mean - 0) + tf.square(variance - 1))\n",
        "    return regularizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smmf6KQZMjwZ"
      },
      "outputs": [],
      "source": [
        "def activity_regularizer(multiplier, decay=0.99):\n",
        "    # Initialize moving averages for mean and variance\n",
        "    mean_avg = tf.Variable(0.0, trainable=False, name='mean_avg')\n",
        "    variance_avg = tf.Variable(1.0, trainable=False, name='variance_avg')\n",
        "\n",
        "    def regularizer(y_pred):\n",
        "        # Calculate the current mean and variance\n",
        "        current_mean = tf.reduce_mean(y_pred)\n",
        "        current_variance = tf.math.reduce_variance(y_pred)\n",
        "\n",
        "        # Update moving averages\n",
        "        update_mean = tf.compat.v1.assign(mean_avg, decay * mean_avg + (1 - decay) * current_mean)\n",
        "        update_variance = tf.compat.v1.assign(variance_avg, decay * variance_avg + (1 - decay) * current_variance)\n",
        "\n",
        "        with tf.control_dependencies([update_mean, update_variance]):\n",
        "            # Use the moving averages for regularization\n",
        "            mean = mean_avg\n",
        "            variance = variance_avg\n",
        "\n",
        "        return multiplier * (tf.square(mean - 0) + tf.square(variance - 1))\n",
        "\n",
        "    return regularizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh7MRqDdA9Wi"
      },
      "outputs": [],
      "source": [
        "drop_val = .2\n",
        "lars_coefficient = .01\n",
        "q_net = sequential.Sequential([\n",
        "    layers.Dense(100,activation='gelu',activity_regularizer=activity_regularizer(0.01)),\n",
        "    layers.Dropout(drop_val),\n",
        "    layers.Dense(50,activation='gelu',activity_regularizer=activity_regularizer(0.01)),\n",
        "    layers.Dropout(drop_val),\n",
        "    layers.Dense(20,activation='gelu',activity_regularizer=activity_regularizer(0.01)),\n",
        "    layers.Dropout(drop_val),\n",
        "    layers.Dense(num_actions,activation=None,bias_regularizer=regularizers.l2(0.01))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIE-FaVlI_8g"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    epsilon_greedy = .1,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XToINtJGJBEf"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywATNsoeJDxb"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRBz--oy5Nxy"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return_print(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for episode in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "\n",
        "      # Print the episode number and observation at each step\n",
        "      print(f\"Episode {episode + 1}: Observation = {time_step.observation.numpy()}\")\n",
        "\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMR3n88lJJYF"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4aKSFxFZJL7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "compute_avg_return(eval_env, eval_policy, num_eval_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6G_uzI2JMWn"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmcyLyvdJO_j"
      },
      "outputs": [],
      "source": [
        "agent.collect_data_spec\n",
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UKOKJyiJR_v",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8yCryO8Jn2u"
      },
      "outputs": [],
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqH4-VBJJrOn"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzbFW84FJyM8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEgCdK7lkuFA"
      },
      "outputs": [],
      "source": [
        "avg_return = compute_avg_return_print(eval_env, collect_policy, num_eval_episodes)\n",
        "print(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSG1pY7R5Nx0"
      },
      "outputs": [],
      "source": [
        "avg_return = compute_avg_return_print(eval_env, eval_policy, num_eval_episodes)\n",
        "print(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9Waz9CaPhVM"
      },
      "outputs": [],
      "source": [
        "weights = q_net.get_weights()\n",
        "print(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv_BrtKqfRuI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}