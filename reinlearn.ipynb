{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/reinlearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-agents[reverb]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avnZVDph6XDI",
        "outputId": "8f95117f-d15a-4f33-a041-e6fc146d7fa2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (8.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.19.0)\n",
            "Requirement already satisfied: rlds in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: dm-reverb~=0.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.11.0)\n",
            "Requirement already satisfied: tensorflow~=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.11.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.11.0->tf-agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (0.32.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.19.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.12.0->tf-agents[reverb]) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.12.0->tf-agents[reverb]) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.12.0->tf-agents[reverb]) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzGKkz097Pha",
        "outputId": "9430ee9b-609b-49ac-c6a7-7567649b9605"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import reverb\n",
        "\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.utils import common\n",
        "import tensorflow as tf\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.policies import random_tf_policy"
      ],
      "metadata": {
        "id": "Qq7mHcz_6Znf"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "PDBocCSIMGmO"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data.csv')\n",
        "df.pop(\"index\")\n",
        "del df[df. columns[0]]\n",
        "df = df.astype('float64')\n",
        "min = df.min()\n",
        "obsShape = len(df. columns)\n",
        "\n",
        "dfarray = df.to_numpy()\n",
        "dflist = dfarray.tolist()\n",
        "\n",
        "length = len(dflist)\n",
        "\n",
        "print(dflist[1][3])\n",
        "print(df)\n",
        "print(np.asarray(dflist).shape)\n",
        "#print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMUkx9L17ZOZ",
        "outputId": "3cf1f2b1-0902-46d1-b05b-ff9388352da6"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.0001999999999999\n",
            "       Direction  Bottom_Point  Top_Point  Relative_Change     Size   day  \\\n",
            "0            0.0      0.000000   0.666667         -0.00080  0.00120   2.0   \n",
            "1            0.0      0.444444   0.666667         -0.00020  0.00090   2.0   \n",
            "2            1.0      0.230769   0.846154          0.00080  0.00130   2.0   \n",
            "3            0.0      0.888889   0.888889          0.00000  0.00090   2.0   \n",
            "4            1.0      0.400000   0.700000          0.00030  0.00100   2.0   \n",
            "...          ...           ...        ...              ...      ...   ...   \n",
            "93075        1.0      0.575000   0.962500          0.00093  0.00240  29.0   \n",
            "93076        1.0      0.457317   0.682927          0.00037  0.00164  29.0   \n",
            "93077        0.0      0.422680   0.824742         -0.00039  0.00097  29.0   \n",
            "93078        1.0      0.634146   0.780488          0.00006  0.00041  29.0   \n",
            "93079        1.0      0.515152   0.969697          0.00015  0.00033  29.0   \n",
            "\n",
            "       month  weekday  hour_of_day  \n",
            "0        5.0      0.0          0.0  \n",
            "1        5.0      0.0          1.0  \n",
            "2        5.0      0.0          2.0  \n",
            "3        5.0      0.0          3.0  \n",
            "4        5.0      0.0          4.0  \n",
            "...      ...      ...          ...  \n",
            "93075    4.0      2.0         18.0  \n",
            "93076    4.0      2.0         19.0  \n",
            "93077    4.0      2.0         20.0  \n",
            "93078    4.0      2.0         21.0  \n",
            "93079    4.0      2.0         22.0  \n",
            "\n",
            "[93080 rows x 9 columns]\n",
            "(93080, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,9), dtype=np.float32, name='observation')\n",
        "    self.counter = 0\n",
        "    self.reward = 0\n",
        "    self._state = dflist[0]\n",
        "    self._episode_ended = False\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state = dflist[0]\n",
        "    self._episode_ended = False\n",
        "    self.reward = 0\n",
        "    self.counter = 0\n",
        "    return ts.restart(np.array([self._state], dtype=np.float32))\n",
        "\n",
        "  def _step(self, action):\n",
        "\n",
        "    self.counter += 1\n",
        "\n",
        "    self._state = dflist[self.counter]\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "    # Buy Sell reward.\n",
        "    if action == 1:\n",
        "      self.reward += dflist[self.counter][3] \n",
        "    elif action == 0:\n",
        "      self.reward -= dflist[self.counter][3]\n",
        "    else:\n",
        "      raise ValueError('`action` should be 0 or 1.')\n",
        "\n",
        "\n",
        "    if self.counter == 1000:\n",
        "      self._episode_ended = True\n",
        "\n",
        "\n",
        "    if self._episode_ended:\n",
        "      return ts.termination(np.array([self._state], dtype=np.float32), self.reward)\n",
        "    else:\n",
        "      return ts.transition(\n",
        "          np.array([self._state], dtype=np.float32), reward=self.reward, discount=0)"
      ],
      "metadata": {
        "id": "yE8avAL96b4a"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environment = TradingEnv()\n",
        "utils.validate_py_environment(environment, episodes=2)"
      ],
      "metadata": {
        "id": "JaISMkaZ6cs-"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Buy = np.array(0, dtype=np.int32)\n",
        "end_round_action = np.array(1, dtype=np.int32)\n",
        "\n",
        "environment = TradingEnv()\n",
        "time_step = environment.reset()\n",
        "print(time_step)\n",
        "cumulative_reward = time_step.reward\n",
        "\n",
        "for _ in range(12):\n",
        "  time_step = environment.step(Buy)\n",
        "  print(time_step)\n",
        "  cumulative_reward += time_step.reward\n",
        "\n",
        "time_step = environment.step(end_round_action)\n",
        "print(time_step)\n",
        "cumulative_reward += time_step.reward\n",
        "print('Final Reward = ', cumulative_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSAH6tJz6e3w",
        "outputId": "85cba8dc-4939-468f-ce36-aa596ba0e833"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[ 0.000000e+00,  0.000000e+00,  6.666667e-01, -8.000000e-04,\n",
            "         1.200000e-03,  2.000000e+00,  5.000000e+00,  0.000000e+00,\n",
            "         0.000000e+00]], dtype=float32),\n",
            " 'reward': array(0., dtype=float32),\n",
            " 'step_type': array(0, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[ 0.0000000e+00,  4.4444445e-01,  6.6666669e-01, -1.9999999e-04,\n",
            "         8.9999998e-04,  2.0000000e+00,  5.0000000e+00,  0.0000000e+00,\n",
            "         1.0000000e+00]], dtype=float32),\n",
            " 'reward': array(0.0002, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[1.0000000e+00, 2.3076923e-01, 8.4615386e-01, 7.9999998e-04,\n",
            "        1.3000000e-03, 2.0000000e+00, 5.0000000e+00, 0.0000000e+00,\n",
            "        2.0000000e+00]], dtype=float32),\n",
            " 'reward': array(-0.0006, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[0.000000e+00, 8.888889e-01, 8.888889e-01, 0.000000e+00,\n",
            "        9.000000e-04, 2.000000e+00, 5.000000e+00, 0.000000e+00,\n",
            "        3.000000e+00]], dtype=float32),\n",
            " 'reward': array(-0.0006, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[1.e+00, 4.e-01, 7.e-01, 3.e-04, 1.e-03, 2.e+00, 5.e+00, 0.e+00,\n",
            "        4.e+00]], dtype=float32),\n",
            " 'reward': array(-0.0009, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[0.e+00, 8.e-01, 8.e-01, 0.e+00, 5.e-04, 2.e+00, 5.e+00, 0.e+00,\n",
            "        5.e+00]], dtype=float32),\n",
            " 'reward': array(-0.0009, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[1.000e+00, 1.875e-01, 7.500e-01, 4.500e-04, 8.000e-04, 2.000e+00,\n",
            "        5.000e+00, 0.000e+00, 6.000e+00]], dtype=float32),\n",
            " 'reward': array(-0.00135, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[ 0.0000000e+00,  2.2222222e-01,  5.5555558e-01, -3.0000001e-04,\n",
            "         8.9999998e-04,  2.0000000e+00,  5.0000000e+00,  0.0000000e+00,\n",
            "         7.0000000e+00]], dtype=float32),\n",
            " 'reward': array(-0.00105, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[1.0000000e+00, 2.9166666e-01, 1.0000000e+00, 8.5000001e-04,\n",
            "        1.2000001e-03, 2.0000000e+00, 5.0000000e+00, 0.0000000e+00,\n",
            "        8.0000000e+00]], dtype=float32),\n",
            " 'reward': array(-0.0019, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[1.000000e+00, 0.000000e+00, 6.666667e-01, 4.000000e-04,\n",
            "        6.000000e-04, 2.000000e+00, 5.000000e+00, 0.000000e+00,\n",
            "        9.000000e+00]], dtype=float32),\n",
            " 'reward': array(-0.0023, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[1.000000e+00, 2.777778e-01, 7.777778e-01, 4.500000e-04,\n",
            "        9.000000e-04, 2.000000e+00, 5.000000e+00, 0.000000e+00,\n",
            "        1.000000e+01]], dtype=float32),\n",
            " 'reward': array(-0.00275, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[ 0.0000000e+00,  6.4516127e-02,  8.7096775e-01, -1.2500000e-03,\n",
            "         1.5500000e-03,  2.0000000e+00,  5.0000000e+00,  0.0000000e+00,\n",
            "         1.1000000e+01]], dtype=float32),\n",
            " 'reward': array(-0.0015, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[ 0.0000000e+00,  8.5714287e-01,  9.2857140e-01, -9.9999997e-05,\n",
            "         1.4000000e-03,  2.0000000e+00,  5.0000000e+00,  0.0000000e+00,\n",
            "         1.2000000e+01]], dtype=float32),\n",
            " 'reward': array(-0.0014, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(0., dtype=float32),\n",
            " 'observation': array([[1.0000000e+00, 4.1860464e-01, 8.8372093e-01, 3.9999999e-04,\n",
            "        8.5999997e-04, 2.0000000e+00, 5.0000000e+00, 0.0000000e+00,\n",
            "        1.3000000e+01]], dtype=float32),\n",
            " 'reward': array(-0.001, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "Final Reward =  -0.01605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = tf_py_environment.TFPyEnvironment(environment)\n",
        "print(isinstance(env, tf_environment.TFEnvironment))\n",
        "print(\"TimeStep Specs:\", env.time_step_spec())\n",
        "print(\"Action Specs:\", env.action_spec())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13qrk1n_4H_z",
        "outputId": "43272a62-e90a-424c-b268-e30626efe7ff"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "TimeStep Specs: TimeStep(\n",
            "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
            " 'observation': BoundedTensorSpec(shape=(1, 9), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
            " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
            " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
            "Action Specs: BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goBS7If5GXfS",
        "outputId": "152007b7-57bc-43fc-ef24-c215e57b3971"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Spec:\n",
            "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [Flatten(), q_values_layer])"
      ],
      "metadata": {
        "id": "ZofbGbSo7190"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = .001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    env.time_step_spec(),\n",
        "    env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "metadata": {
        "id": "BSdaaX9C4H1P"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ],
      "metadata": {
        "id": "ZfOExBEMKj59"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
        "                                                env.action_spec())"
      ],
      "metadata": {
        "id": "OR33rDg0K0IB"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_avg_return(env, random_policy, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl4OYindKpqL",
        "outputId": "822c7d2b-9fd2-4283-f186-dc4ab06448d8"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-4.2019286"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ],
      "metadata": {
        "id": "11BqzPszLk_x"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.collect_data_spec"
      ],
      "metadata": {
        "id": "a5UXcE6rMfoa",
        "outputId": "363d35a3-758b-4428-db18-ed5ae8450709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Trajectory(\n",
              "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': BoundedTensorSpec(shape=(1, 9), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.collect_data_spec._fields"
      ],
      "metadata": {
        "id": "c-oMHNlTMoyP",
        "outputId": "a10d00a3-f326-4832-b6cb-05aa7931ae1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(env.reset())"
      ],
      "metadata": {
        "id": "2i9LJjK_M7pW",
        "outputId": "635bd669-72f1-4c6f-bd51-a47ff9accc84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TimeStep(\n",
              " {'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
              "  'observation': <tf.Tensor: shape=(1, 1, 9), dtype=float32, numpy=\n",
              " array([[[ 0.0000000e+00,  4.4444445e-01,  8.8888890e-01, -3.9999999e-04,\n",
              "           8.9999998e-04,  6.0000000e+00,  5.0000000e+00,  4.0000000e+00,\n",
              "           4.0000000e+00]]], dtype=float32)>,\n",
              "  'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.00856], dtype=float32)>,\n",
              "  'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}),\n",
              " ())"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "q7pnHNXINVxv",
        "outputId": "b472a579-e910-4819-a8f8-f53d7c8df5c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(Trajectory(\n",
              "{'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'observation': TensorSpec(shape=(64, 2, 1, 9), dtype=tf.float32, name=None),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ],
      "metadata": {
        "id": "2L8BeN5kNY8V",
        "outputId": "b9009a68-a7b7-4120-83dc-e757bb4e29bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fee2f40da80>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "metadata": {
        "id": "30LGeFO-NeHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}