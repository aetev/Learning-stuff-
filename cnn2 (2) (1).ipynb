{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/cnn2%20(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-g36b6KfZQeH",
    "outputId": "b1bf8e6e-046e-42e4-9d3f-c4dace11ac86"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UiZf7YAawUu",
    "outputId": "9740caeb-10ca-4fec-cf94-a4426308eadb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yn4y2vCkggU1",
    "outputId": "9ec92beb-a6eb-4e0c-c819-55e0e2afefa5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images to [-1, 1]\n",
    "#x_train = (x_train.astype(\"float32\") - 127.5) / 127.5\n",
    "#y_train = y_train.reshape(-1, 1)\n",
    "x_train = (x_train.astype(\"float32\")) / np.max(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "GwT02EZkqZQW",
    "outputId": "6921d11f-0688-41f8-f85e-5f47afb9a9d8"
   },
   "outputs": [],
   "source": [
    "new_images = []\n",
    "new_labels = []\n",
    "\n",
    "for digit in set(y_train):\n",
    "    digit_indices = np.where(y_train == digit)[0][:10]\n",
    "    for index in digit_indices:\n",
    "        image = x_train[index]\n",
    "        label = y_train[index]\n",
    "        new_images.append(image)\n",
    "        new_labels.append(label)\n",
    "\n",
    "new_images = np.array(new_images)\n",
    "new_labels = np.array(new_labels)\n",
    "\n",
    "# Create an array with indices from 0 to length of your arrays\n",
    "indices = np.arange(new_images.shape[0])\n",
    "\n",
    "# Shuffle the indices\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle your arrays\n",
    "new_images = new_images[indices]\n",
    "new_labels = new_labels[indices]\n",
    "\n",
    "print(\"New dataset shape:\", new_images.shape)\n",
    "\n",
    "# Visualize the new dataset\n",
    "plt.imshow(new_images[30], cmap=\"gray\")\n",
    "plt.title(\"Label: {}\".format(new_labels[30]))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "e5q1TuKbSRIY",
    "outputId": "deb9ea1d-1c04-45ab-85c2-8f75df44b85f"
   },
   "outputs": [],
   "source": [
    "def random_rotate_images(images, min_angle=-90):\n",
    "    size = tf.shape(images)[0]\n",
    "    random_angles = tf.random.uniform(shape=(size,), minval=min_angle, maxval=-min_angle, dtype=tf.float32)\n",
    "    rotated_images = tfa.image.rotate(images, random_angles / 90)\n",
    "    return rotated_images\n",
    "\n",
    "# Select a batch of images\n",
    "batch_size = 10\n",
    "batch_images = x_train[:batch_size]\n",
    "batch_images = np.expand_dims(batch_images, axis=-1)\n",
    "rotated_images = random_rotate_images(batch_images)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=batch_size, figsize=(10, 4))\n",
    "for i in range(batch_size):\n",
    "    axes[0][i].imshow(batch_images[i], cmap='gray')\n",
    "    axes[0][i].axis('off')\n",
    "    axes[1][i].imshow(rotated_images[i], cmap='gray')\n",
    "    axes[1][i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vyOx5JqbrffI",
    "outputId": "651e90ea-b2ad-4ddb-a5f5-bd5d99c66416"
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    noise_shape = (100,)\n",
    "    noise = layers.Input(shape=noise_shape)\n",
    "    input_digit = layers.Input(shape=(1,), dtype=\"int32\")\n",
    "    digit_embedding = layers.Embedding(input_dim=10, output_dim=64, input_length=1)(input_digit)\n",
    "    digit_embedding = layers.Flatten()(digit_embedding)\n",
    "\n",
    "    x = layers.Concatenate()([noise, digit_embedding])\n",
    "\n",
    "    # Transform the concatenated vector into a 7x7x256 tensor\n",
    "    x = layers.Dense(7 * 7, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape((7, 7, 1))(x)\n",
    "\n",
    "    # Upsample to 14x14\n",
    "    x = layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    # Conv 14x14\n",
    "    x = layers.Conv2D(64, (5, 5), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    # Upsample to 28x28\n",
    "    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    # Final output layer\n",
    "    output = layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='hard_sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[noise, input_digit], outputs=output)\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gh2ORgUybdzt"
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    input_image = layers.Input(shape=(28, 28, 1))\n",
    "    digit_input = layers.Input(shape=(1,), dtype=\"int32\")\n",
    "    digit_embedding_1d = layers.Embedding(input_dim=10, output_dim=64, input_length=1)(digit_input)\n",
    "    digit_embedding_1d = layers.Flatten()(digit_embedding_1d)\n",
    "    digit_embedding_2d = layers.Embedding(input_dim=10, output_dim=28*28, input_length=1)(digit_input)\n",
    "    digit_embedding_2d = layers.Reshape((28,28,1))(digit_embedding_2d)\n",
    "\n",
    "    merged_input = layers.Concatenate()([input_image, digit_embedding_2d])\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = layers.Conv2D(128, (2, 2), strides=(2, 2), padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01))(merged_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.8)(x)\n",
    "\n",
    "    x = layers.Conv2D(248, (5, 5), strides=(2, 2), padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.8)(x)\n",
    "\n",
    "    x = layers.Conv2D(248, (5, 5), strides=(2, 2), padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.8)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Concatenate()([x,digit_embedding_1d])\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(300, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.8)(x)\n",
    "    x = layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_image, digit_input], outputs=x)\n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator_copy = build_discriminator()\n",
    "#discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFFOfuDfbpD9"
   },
   "outputs": [],
   "source": [
    "# Compile models\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0004)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002)\n",
    "copy_optimizer = tf.keras.optimizers.Adam(0.0004)\n",
    "\n",
    "#generator_optimizer = tf.keras.optimizers.experimental.SGD(1e-4)\n",
    "#discriminator_optimizer = tf.keras.optimizers.experimental.SGD(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3PtShxlbn9c"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return tf.keras.losses.BinaryCrossentropy()(tf.ones_like(fake_output), fake_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAoSvqXLsQHq"
   },
   "outputs": [],
   "source": [
    "def print_img(generator_model):\n",
    "    # Generate and save sample images\n",
    "    noise = tf.random.normal([10, 100])\n",
    "    sampled_labels = tf.constant([[i % 10] for i in range(10)], dtype=tf.int32)\n",
    "    generated_images = generator_model.predict([noise, sampled_labels])\n",
    "    fig, axs = plt.subplots(1, 10, figsize=(10, 10))\n",
    "    for i in range(10):\n",
    "        axs[i].imshow(generated_images[i], cmap=\"gray\")\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CK4j3K7qtz47",
    "outputId": "00f69584-3e25-4597-e708-75bae2583e01"
   },
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def train_step(images, labels):\n",
    "\n",
    "    batch_size = images.shape[0]\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "    generated_images = generator([noise, labels], training=True)\n",
    "\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "\n",
    "      real_output = discriminator([images, labels], training=True)\n",
    "      fake_output = discriminator([generated_images, labels], training=True)\n",
    "\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "    # Get the weights of model1\n",
    "    weights = discriminator.get_weights()\n",
    "\n",
    "    # Set the weights of model2 to be the same as model1\n",
    "    discriminator_copy.set_weights(weights)\n",
    "\n",
    "    for i in range(5):\n",
    "      with tf.GradientTape() as disc_copy_tape:\n",
    "        noise = tf.random.normal([batch_size, 100])\n",
    "        generated_images = generator([noise, labels], training=True)\n",
    "\n",
    "        real_output = discriminator_copy([images, labels], training=True)\n",
    "        fake_output = discriminator_copy([generated_images, labels], training=True)\n",
    "\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "      gradients_of_discriminator = disc_copy_tape.gradient(disc_loss, discriminator_copy.trainable_variables)\n",
    "      copy_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_copy.trainable_variables))\n",
    "\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "      noise = tf.random.normal([batch_size, 100])\n",
    "      generated_images = generator([noise, labels], training=True)\n",
    "      fake_output = discriminator_copy([generated_images, labels], training=True)\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "\n",
    "    tf.print(\"disc_loss\",disc_loss,'gen_loss',gen_loss)\n",
    "\n",
    "\n",
    "def train(generator, discriminator\n",
    "          , epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(len(new_images) // batch_size):\n",
    "            images = new_images[batch * batch_size: (batch+1) * batch_size]\n",
    "            labels = new_labels[batch * batch_size: (batch+1) * batch_size]\n",
    "\n",
    "            train_step(images, labels)\n",
    "\n",
    "        # Output training progress\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print_img(generator)\n",
    "\n",
    "# Train the GAN\n",
    "EPOCHS = 2000000\n",
    "BATCH_SIZE = 50\n",
    "num_unrolling_steps = 5  # Set the desired number of unrolling steps\n",
    "train(generator, discriminator, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Overview of Colaboratory Features",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
