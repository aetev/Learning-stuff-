{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/destroys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-agents[reverb]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avnZVDph6XDI",
        "outputId": "9c59d7af-41da-48e0-f145-25ac63d4ea31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf-agents[reverb]\n",
            "  Downloading tf_agents-0.16.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents[reverb])\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (8.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
            "Collecting pygame==2.1.3 (from tf-agents[reverb])\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-probability~=0.19.0 (from tf-agents[reverb])\n",
            "  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rlds (from tf-agents[reverb])\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb~=0.11.0 (from tf-agents[reverb])\n",
            "  Downloading dm_reverb-0.11.0-cp310-cp310-manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.11.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.11.0->tf-agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf-agents[reverb]) (0.32.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.19.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.12.0->tf-agents[reverb]) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.12.0->tf-agents[reverb]) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.12.0->tf-agents[reverb]) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-agents[reverb]) (3.2.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697641 sha256=a06edbc73e1541929674a50ed6eb8999685f6f0606199bf755eaabaa6a041b6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: tensorflow-probability, rlds, pygame, gym, dm-reverb, tf-agents\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.20.1\n",
            "    Uninstalling tensorflow-probability-0.20.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.20.1\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed dm-reverb-0.11.0 gym-0.23.0 pygame-2.1.3 rlds-0.1.8 tensorflow-probability-0.19.0 tf-agents-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzGKkz097Pha",
        "outputId": "812ba9c3-d321-4417-9bb4-1d41c9c43439"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import reverb\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.utils import common\n",
        "import tensorflow as tf\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.policies import random_tf_policy"
      ],
      "metadata": {
        "id": "Qq7mHcz_6Znf"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 2  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "PDBocCSIMGmO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data.csv')\n",
        "df.pop(\"index\")\n",
        "del df[df. columns[0]]\n",
        "df = df.astype('float64')\n",
        "min = df.min()\n",
        "obsShape = len(df. columns)\n",
        "\n",
        "dfarray = df.to_numpy()\n",
        "dflist = dfarray.tolist()\n",
        "\n",
        "length = len(dflist)\n",
        "\n",
        "print(dflist[1][3])\n",
        "print(df)\n",
        "print(np.asarray(dflist).shape)\n",
        "#print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMUkx9L17ZOZ",
        "outputId": "97f50f79-98c2-46af-8318-da94f3e5e8e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.0001999999999999\n",
            "       Direction  Bottom_Point  Top_Point  Relative_Change     Size   day  \\\n",
            "0            0.0      0.000000   0.666667         -0.00080  0.00120   2.0   \n",
            "1            0.0      0.444444   0.666667         -0.00020  0.00090   2.0   \n",
            "2            1.0      0.230769   0.846154          0.00080  0.00130   2.0   \n",
            "3            0.0      0.888889   0.888889          0.00000  0.00090   2.0   \n",
            "4            1.0      0.400000   0.700000          0.00030  0.00100   2.0   \n",
            "...          ...           ...        ...              ...      ...   ...   \n",
            "93075        1.0      0.575000   0.962500          0.00093  0.00240  29.0   \n",
            "93076        1.0      0.457317   0.682927          0.00037  0.00164  29.0   \n",
            "93077        0.0      0.422680   0.824742         -0.00039  0.00097  29.0   \n",
            "93078        1.0      0.634146   0.780488          0.00006  0.00041  29.0   \n",
            "93079        1.0      0.515152   0.969697          0.00015  0.00033  29.0   \n",
            "\n",
            "       month  weekday  hour_of_day  \n",
            "0        5.0      0.0          0.0  \n",
            "1        5.0      0.0          1.0  \n",
            "2        5.0      0.0          2.0  \n",
            "3        5.0      0.0          3.0  \n",
            "4        5.0      0.0          4.0  \n",
            "...      ...      ...          ...  \n",
            "93075    4.0      2.0         18.0  \n",
            "93076    4.0      2.0         19.0  \n",
            "93077    4.0      2.0         20.0  \n",
            "93078    4.0      2.0         21.0  \n",
            "93079    4.0      2.0         22.0  \n",
            "\n",
            "[93080 rows x 9 columns]\n",
            "(93080, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  \n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,9), dtype=np.float32, name='observation')\n",
        "    self.counter = 0\n",
        "    self.reward = 0\n",
        "    self._state = dflist[0]\n",
        "    self._episode_ended = False\n",
        "    self._reward_spec = array_spec.BoundedArraySpec(\n",
        "    shape=(1,), dtype=np.float32, minimum=-100.0, maximum=100.0, name='reward')\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state = dflist[0]\n",
        "    self._episode_ended = False\n",
        "    self.reward = 0\n",
        "    self.counter = 0\n",
        "    return ts.restart(np.array([self._state], dtype=np.float32))\n",
        "\n",
        "  def _step(self, action):\n",
        "\n",
        "    self.counter += 1\n",
        "\n",
        "    self._state = dflist[self.counter]\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "    # Buy Sell reward.\n",
        "    if action == 1:\n",
        "      self.reward += dflist[self.counter][3] \n",
        "    elif action == 0:\n",
        "      self.reward -= dflist[self.counter][3]\n",
        "    else:\n",
        "      raise ValueError('`action` should be 0 or 1.')\n",
        "\n",
        "\n",
        "    if self.counter == 1000:\n",
        "      self._episode_ended = True\n",
        "\n",
        "\n",
        "    if self._episode_ended:\n",
        "      return ts.termination(np.array([self._state], dtype=np.float32), reward=self.reward)\n",
        "    else:\n",
        "      return ts.transition(\n",
        "          np.array([self._state], dtype=np.float32), reward=self.reward, discount=1)"
      ],
      "metadata": {
        "id": "yE8avAL96b4a"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environment = TradingEnv()\n",
        "utils.validate_py_environment(environment, episodes=2)"
      ],
      "metadata": {
        "id": "JaISMkaZ6cs-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reward Spec:')\n",
        "print(environment.time_step_spec().reward)"
      ],
      "metadata": {
        "id": "jsyHF34swMEE",
        "outputId": "728a8c85-4931-4ccd-d316-256dc8bc8367",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward Spec:\n",
            "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Buy = np.array(0, dtype=np.int32)\n",
        "end_round_action = np.array(1, dtype=np.int32)\n",
        "\n",
        "environment = TradingEnv()\n",
        "time_step = environment.reset()\n",
        "print(time_step)\n",
        "cumulative_reward = time_step.reward\n",
        "\n",
        "for _ in range(12):\n",
        "  time_step = environment.step(Buy)\n",
        "  print(time_step)\n",
        "  cumulative_reward += time_step.reward\n",
        "\n",
        "time_step = environment.step(end_round_action)\n",
        "print(time_step)\n",
        "cumulative_reward += time_step.reward\n",
        "print('Final Reward = ', cumulative_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSAH6tJz6e3w",
        "outputId": "342e4475-5dfe-48f9-fd89-5247c2b7ab57"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[ 0.000000e+00,  0.000000e+00,  6.666667e-01, -8.000000e-04,\n",
            "         1.200000e-03,  2.000000e+00,  5.000000e+00,  0.000000e+00,\n",
            "         0.000000e+00]], dtype=float32),\n",
            " 'reward': array(0., dtype=float32),\n",
            " 'step_type': array(0, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[ 0.0000000e+00,  4.4444445e-01,  6.6666669e-01, -1.9999999e-04,\n",
            "         8.9999998e-04,  2.0000000e+00,  5.0000000e+00,  0.0000000e+00,\n",
            "         1.0000000e+00]], dtype=float32),\n",
            " 'reward': array(0.0002, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[1.0000000e+00, 2.3076923e-01, 8.4615386e-01, 7.9999998e-04,\n",
            "        1.3000000e-03, 2.0000000e+00, 5.0000000e+00, 0.0000000e+00,\n",
            "        2.0000000e+00]], dtype=float32),\n",
            " 'reward': array(-0.0006, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[0.000000e+00, 8.888889e-01, 8.888889e-01, 0.000000e+00,\n",
            "        9.000000e-04, 2.000000e+00, 5.000000e+00, 0.000000e+00,\n",
            "        3.000000e+00]], dtype=float32),\n",
            " 'reward': array(-0.0006, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[1.e+00, 4.e-01, 7.e-01, 3.e-04, 1.e-03, 2.e+00, 5.e+00, 0.e+00,\n",
            "        4.e+00]], dtype=float32),\n",
            " 'reward': array(-0.0009, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[0.e+00, 8.e-01, 8.e-01, 0.e+00, 5.e-04, 2.e+00, 5.e+00, 0.e+00,\n",
            "        5.e+00]], dtype=float32),\n",
            " 'reward': array(-0.0009, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[1.000e+00, 1.875e-01, 7.500e-01, 4.500e-04, 8.000e-04, 2.000e+00,\n",
            "        5.000e+00, 0.000e+00, 6.000e+00]], dtype=float32),\n",
            " 'reward': array(-0.00135, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[ 0.0000000e+00,  2.2222222e-01,  5.5555558e-01, -3.0000001e-04,\n",
            "         8.9999998e-04,  2.0000000e+00,  5.0000000e+00,  0.0000000e+00,\n",
            "         7.0000000e+00]], dtype=float32),\n",
            " 'reward': array(-0.00105, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[1.0000000e+00, 2.9166666e-01, 1.0000000e+00, 8.5000001e-04,\n",
            "        1.2000001e-03, 2.0000000e+00, 5.0000000e+00, 0.0000000e+00,\n",
            "        8.0000000e+00]], dtype=float32),\n",
            " 'reward': array(-0.0019, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[1.000000e+00, 0.000000e+00, 6.666667e-01, 4.000000e-04,\n",
            "        6.000000e-04, 2.000000e+00, 5.000000e+00, 0.000000e+00,\n",
            "        9.000000e+00]], dtype=float32),\n",
            " 'reward': array(-0.0023, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[1.000000e+00, 2.777778e-01, 7.777778e-01, 4.500000e-04,\n",
            "        9.000000e-04, 2.000000e+00, 5.000000e+00, 0.000000e+00,\n",
            "        1.000000e+01]], dtype=float32),\n",
            " 'reward': array(-0.00275, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[ 0.0000000e+00,  6.4516127e-02,  8.7096775e-01, -1.2500000e-03,\n",
            "         1.5500000e-03,  2.0000000e+00,  5.0000000e+00,  0.0000000e+00,\n",
            "         1.1000000e+01]], dtype=float32),\n",
            " 'reward': array(-0.0015, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[ 0.0000000e+00,  8.5714287e-01,  9.2857140e-01, -9.9999997e-05,\n",
            "         1.4000000e-03,  2.0000000e+00,  5.0000000e+00,  0.0000000e+00,\n",
            "         1.2000000e+01]], dtype=float32),\n",
            " 'reward': array(-0.0014, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([[1.0000000e+00, 4.1860464e-01, 8.8372093e-01, 3.9999999e-04,\n",
            "        8.5999997e-04, 2.0000000e+00, 5.0000000e+00, 0.0000000e+00,\n",
            "        1.3000000e+01]], dtype=float32),\n",
            " 'reward': array(-0.001, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n",
            "Final Reward =  -0.01605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = tf_py_environment.TFPyEnvironment(environment,isolation=True)\n",
        "print(isinstance(env, tf_environment.TFEnvironment))\n",
        "print(\"TimeStep Specs:\", env.time_step_spec())\n",
        "print(\"Action Specs:\", env.action_spec())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13qrk1n_4H_z",
        "outputId": "22a193d6-55b8-4bbf-a6d4-fd234800c1c2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "TimeStep Specs: TimeStep(\n",
            "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
            " 'observation': BoundedTensorSpec(shape=(1, 9), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
            " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
            " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
            "Action Specs: BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goBS7If5GXfS",
        "outputId": "8a5d5e49-8fdf-4ea9-8337-cf3ce8dd91b2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Spec:\n",
            "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [Flatten(), q_values_layer])"
      ],
      "metadata": {
        "id": "ZofbGbSo7190"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = .001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    env.time_step_spec(),\n",
        "    env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "metadata": {
        "id": "BSdaaX9C4H1P"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ],
      "metadata": {
        "id": "ZfOExBEMKj59"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
        "                                                env.action_spec())"
      ],
      "metadata": {
        "id": "OR33rDg0K0IB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_avg_return(env, random_policy, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl4OYindKpqL",
        "outputId": "5024ae23-d103-4fdc-aa2e-789539752ad8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-31.585495"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ],
      "metadata": {
        "id": "11BqzPszLk_x"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.collect_data_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5UXcE6rMfoa",
        "outputId": "aacb5b1d-d17b-41ee-e944-89555249433c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Trajectory(\n",
              "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': BoundedTensorSpec(shape=(1, 9), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.collect_data_spec._fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-oMHNlTMoyP",
        "outputId": "81eb7946-bbab-4be0-d357-51798973fb32"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(env.reset())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i9LJjK_M7pW",
        "outputId": "6a0f012d-6682-43d5-afdc-4fe72adb4f41"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TimeStep(\n",
              " {'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
              "  'observation': <tf.Tensor: shape=(1, 1, 9), dtype=float32, numpy=\n",
              " array([[[ 0.0000000e+00,  4.4444445e-01,  8.8888890e-01, -3.9999999e-04,\n",
              "           8.9999998e-04,  6.0000000e+00,  5.0000000e+00,  4.0000000e+00,\n",
              "           4.0000000e+00]]], dtype=float32)>,\n",
              "  'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.01356], dtype=float32)>,\n",
              "  'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}),\n",
              " ())"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=1,\n",
        "    sample_batch_size=1,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7pnHNXINVxv",
        "outputId": "22d37eb2-8d6f-491c-9a2a-ee60f69a74a3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(Trajectory(\n",
              "{'action': TensorSpec(shape=(1, 2), dtype=tf.int32, name=None),\n",
              " 'discount': TensorSpec(shape=(1, 2), dtype=tf.float32, name=None),\n",
              " 'next_step_type': TensorSpec(shape=(1, 2), dtype=tf.int32, name=None),\n",
              " 'observation': TensorSpec(shape=(1, 2, 1, 9), dtype=tf.float32, name=None),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(1, 2), dtype=tf.float32, name=None),\n",
              " 'step_type': TensorSpec(shape=(1, 2), dtype=tf.int32, name=None)}), SampleInfo(key=TensorSpec(shape=(1, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(1, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(1, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(1, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(1, 2), dtype=tf.int32, name=None)))>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L8BeN5kNY8V",
        "outputId": "ffd7872a-0c27-4ea8-85af-eaad200286dd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f4fc8132320>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Ensure that time_step has a single batch dimension\n",
        "  time_step = tf.nest.map_structure(lambda t: tf.squeeze(t, axis=0), time_step)\n",
        "\n",
        "  # Check if batch size of time_step matches policy's batch size\n",
        "  batch_size = tf.compat.v1.shape(agent.policy.get_initial_state(batch_size=1)).numpy()[0]\n",
        "  if hasattr(time_step, 'reward'):\n",
        "      time_step_batch_size = tf.compat.v1.shape(time_step.reward)[0]\n",
        "  else:\n",
        "      time_step = time_step._replace(reward=tf.constant([0.0], dtype=tf.float32))\n",
        "      time_step_batch_size = 1\n",
        "  if time_step_batch_size != batch_size:\n",
        "      time_step = tf.nest.map_structure(lambda t: tf.repeat(t, repeats=batch_size, axis=0), time_step)\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "30LGeFO-NeHR",
        "outputId": "3c267928-0704-4427-d52a-ebbfbb0c9030"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-24666ba79dbb>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mtime_step_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7261\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7262\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}