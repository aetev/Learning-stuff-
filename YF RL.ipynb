{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/YF%20RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_agents[reverb]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so8O99AzB95T",
        "outputId": "45e3e38a-809c-4101-b2e1-53f68e974651"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf_agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.22.0)\n",
            "Requirement already satisfied: rlds in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: dm-reverb~=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.13.0)\n",
            "Requirement already satisfied: tensorflow~=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.13.0->tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.13.0->tf_agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf_agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->tf_agents[reverb]) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (3.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.13.0->tf_agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Concatenate\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from tf_agents.networks import network\n",
        "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
        "from tf_agents.environments import py_environment, tf_py_environment, utils\n",
        "from tf_agents.specs import array_spec, tensor_spec\n",
        "from tf_agents.trajectories import trajectory, time_step as ts\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.utils import common"
      ],
      "metadata": {
        "id": "R8uczkI2c8cJ"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64 # @param {type:\"integer\"}\n",
        "EVAL_EPISODES = 3 # @param {type:\"integer\"}\n",
        "NUMBER_EPISODES = 10000 # @param {type:\"integer\"}\n",
        "COLLECTION_STEPS = 1 # @param {type:\"integer\"}\n",
        "EVAL_INTERVAL =  100 # @param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "mJzdcerJK3Ti"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='purple'><center>**-------------------Create Data-------------------**</font>"
      ],
      "metadata": {
        "id": "2IlkLhMwK4kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the date 730 days ago\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=729)\n",
        "\n",
        "# Download the data\n",
        "data = yf.download('EURUSD=X', start=start_date, end=end_date, interval='1h')\n",
        "\n",
        "# Print the data\n",
        "print(data[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uHZHAsUc_cw",
        "outputId": "74f46dc1-1a7c-4c11-8772-6d5fb0164dca"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "                               Open      High       Low     Close  Adj Close  \\\n",
            "Datetime                                                                       \n",
            "2021-10-18 07:00:00+01:00  1.157943  1.158346  1.157675  1.158212   1.158212   \n",
            "2021-10-18 08:00:00+01:00  1.157943  1.159958  1.157943  1.159689   1.159689   \n",
            "\n",
            "                           Volume  \n",
            "Datetime                           \n",
            "2021-10-18 07:00:00+01:00       0  \n",
            "2021-10-18 08:00:00+01:00       0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "High = data['High'].to_numpy()\n",
        "Close = data['Close'].to_numpy()\n",
        "Low = data['Low'].to_numpy()\n",
        "Std = np.std(np.diff(Close))\n",
        "Close_Diff = np.diff(Close)/Std\n",
        "High_Adj = (High[1:]-Close[1:])/Std\n",
        "Low_Adj = (Close[1:]-Low[1:])/Std\n",
        "# Get the sum of adjacent values\n",
        "sum_array = Close_Diff[:-1] + Close_Diff[1:]\n",
        "print(sum_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pteR2bFg43J",
        "outputId": "99244351-9888-4f8f-f9d3-e3f876d9eafe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.21484273 -0.86061168  0.86042079 ... -0.44600625 -0.17838341\n",
            " -0.17828797]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stack = np.stack((Close_Diff, High_Adj, Low_Adj), axis=1)\n",
        "print(stack)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phmsBJRwEQAX",
        "outputId": "55cc97c3-a072-4e6c-a1b5-1e3c5173bd00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.18292349  0.21541539  1.39767077]\n",
            " [-0.96808076  1.29125156  0.        ]\n",
            " [ 0.10746909  0.43016267  0.42968546]\n",
            " ...\n",
            " [-0.17838341  0.17838341  0.26743195]\n",
            " [ 0.          0.2675274   0.17828797]\n",
            " [-0.17828797  0.17828797  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 50\n",
        "window_view = np.lib.stride_tricks.sliding_window_view(stack, (window_size,),axis=0)"
      ],
      "metadata": {
        "id": "THv9cmvhmfEG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Forex_Data = window_view[:-1]\n",
        "Forex_Data = Forex_Data.astype('float32')\n",
        "target = Close_Diff[window_size:]"
      ],
      "metadata": {
        "id": "h_w0wsB0EGOL"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='red'><center>**-------------------Create Environment-------------------**</font>"
      ],
      "metadata": {
        "id": "Inm6-iBTDdHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int64, minimum=0, maximum=3, name='action')\n",
        "\n",
        "    self._observation_spec = (\n",
        "        array_spec.BoundedArraySpec(shape=(3,50), dtype=np.float32, name='observation1'),\n",
        "        array_spec.BoundedArraySpec(shape=(5,), dtype=np.float32, name='observation2')\n",
        "    )\n",
        "\n",
        "    self._state = (Forex_Data[0], np.array([20, 20, 20, 20, 20], dtype=np.float32))\n",
        "\n",
        "    self._episode_ended = False\n",
        "    self._count = 1\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def get_balance(self):\n",
        "    return self._balance\n",
        "\n",
        "  def _reset(self):\n",
        "\n",
        "    self._state = (Forex_Data[0], np.array([20, 20, 20, 20, 20], dtype=np.float32))\n",
        "    self._episode_ended = False\n",
        "    self._count = 1\n",
        "\n",
        "    return ts.restart(self._state)\n",
        "\n",
        "  def _step(self, action):\n",
        "    if self._episode_ended:\n",
        "      return self.reset()\n",
        "\n",
        "    reward = 10\n",
        "\n",
        "    self._state = (Forex_Data[self._count], np.array([20, 20, 20, 20, 20], dtype=np.float32))\n",
        "\n",
        "    if self._count >= 100:\n",
        "      self._episode_ended = True\n",
        "\n",
        "    self._count += 1\n",
        "\n",
        "    if self._episode_ended:\n",
        "      return ts.termination(self._state, reward=reward)\n",
        "    else:\n",
        "      return ts.transition(self._state, reward=reward, discount=.999)\n",
        "\n",
        "\n",
        "\n",
        "env = TradingEnv()\n",
        "utils.validate_py_environment(env, episodes=2)\n",
        "\n",
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)\n",
        "\n",
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)\n",
        "\n",
        "print('Action Spec:')\n",
        "print(env.action_spec())\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(env)\n",
        "evaluation_env = tf_py_environment.TFPyEnvironment(env)"
      ],
      "metadata": {
        "id": "FJhuVNVjSirl",
        "outputId": "b9c5f8b8-756a-4369-dd17-78d42a0ba189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Spec:\n",
            "(BoundedArraySpec(shape=(3, 50), dtype=dtype('float32'), name='observation1', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38), BoundedArraySpec(shape=(5,), dtype=dtype('float32'), name='observation2', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38))\n",
            "Reward Spec:\n",
            "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
            "Action Spec:\n",
            "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='blue'><center>**-------------------Create Network-------------------**</font>"
      ],
      "metadata": {
        "id": "VE8qjYImGIEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomQNetwork(network.Network):\n",
        "    def __init__(self, input_tensor_spec, action_spec, name='CustomQNetwork'):\n",
        "        super(CustomQNetwork, self).__init__(input_tensor_spec=input_tensor_spec,\n",
        "                                             state_spec=(),\n",
        "                                             name=name)\n",
        "        # Define your custom layers and architecture here\n",
        "        self.LSTM1 = LSTM(units=100, activation='gelu', return_sequences=False)\n",
        "        self.Concat = Concatenate()\n",
        "        self.Dense1 = Dense(100,activation='gelu')\n",
        "        self.q_values_layer = Dense(action_spec.maximum - action_spec.minimum + 1,\n",
        "                                                    activation=None)\n",
        "\n",
        "    def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "        # Split the observation into two arrays\n",
        "        observation1, observation2 = observation\n",
        "\n",
        "        x = self.LSTM1(observation1)\n",
        "        x = self.Concat([x,observation2])\n",
        "        x = self.Dense1(x)\n",
        "        q_values = self.q_values_layer(x)\n",
        "        return q_values, network_state"
      ],
      "metadata": {
        "id": "p-oj9MnyBRRT"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalQNetwork(network.Network):\n",
        "    def __init__(self, input_tensor_spec, action_spec, num_atoms=51, preprocessing_layers=None,\n",
        "                 preprocessing_combiner=None, conv_layer_params=None, fc_layer_params=None,\n",
        "                 activation_fn=tf.nn.relu, name='CategoricalQNetwork'):\n",
        "        super(CategoricalQNetwork, self).__init__(input_tensor_spec=input_tensor_spec,\n",
        "                                                  state_spec=(),\n",
        "                                                  name=name)\n",
        "        # Check if action_spec is a BoundedTensorSpec\n",
        "        if not isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n",
        "            raise TypeError('action_spec must be a BoundedTensorSpec. Got: %s' % (action_spec,))\n",
        "\n",
        "        self._num_actions = action_spec.maximum - action_spec.minimum + 1\n",
        "        self._num_atoms = num_atoms\n",
        "\n",
        "        # Create a BoundedTensorSpec for the q_network action\n",
        "        q_network_action_spec = tensor_spec.BoundedTensorSpec((), tf.int32,\n",
        "                                                              minimum=0,\n",
        "                                                              maximum=self._num_actions * num_atoms - 1)\n",
        "\n",
        "        # Create the q_network\n",
        "        self._q_network = CustomQNetwork(input_tensor_spec=input_tensor_spec,\n",
        "                                             action_spec=q_network_action_spec,\n",
        "                                             )\n",
        "    def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "        logits, network_state = self._q_network(observation, step_type, network_state, training=training)\n",
        "        logits = tf.reshape(logits, [-1, self._num_actions, self._num_atoms])\n",
        "        return logits, network_state\n",
        "    @property\n",
        "    def num_atoms(self):\n",
        "        return self._num_atoms"
      ],
      "metadata": {
        "id": "D5B8riShDDw9"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation_spec = train_env.observation_spec()\n",
        "action_spec = train_env.action_spec()\n",
        "num_atoms = 51\n",
        "custom_network = CategoricalQNetwork(observation_spec,action_spec, num_atoms)"
      ],
      "metadata": {
        "id": "lfIpj9ALG_IN"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=.01)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "C51_agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    categorical_q_network=custom_network,\n",
        "    optimizer=optimizer,\n",
        "    min_q_value=-5,\n",
        "    max_q_value=5,\n",
        "    n_step_update=2,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=1,\n",
        "    train_step_counter=train_step_counter)\n",
        "C51_agent.initialize()"
      ],
      "metadata": {
        "id": "9ebSdMkaHHZD"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='green'><center>**-------------------Create Replay Buffer And Training Loop-------------------**</font>"
      ],
      "metadata": {
        "id": "-mEDhh1OH2Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplay(object):\n",
        "    def __init__(self, agent, enviroment):\n",
        "        self._replay_buffer = TFUniformReplayBuffer(\n",
        "            data_spec=agent.collect_data_spec,\n",
        "            batch_size=enviroment.batch_size,\n",
        "            max_length=10000)\n",
        "\n",
        "        self._random_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                enviroment.action_spec())\n",
        "\n",
        "        self._fill_buffer(train_env, self._random_policy, steps=100)\n",
        "\n",
        "        self.dataset = self._replay_buffer.as_dataset(\n",
        "            num_parallel_calls=tf.data.AUTOTUNE,\n",
        "            sample_batch_size=BATCH_SIZE,\n",
        "            num_steps=2).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        self.iterator = iter(self.dataset)\n",
        "\n",
        "\n",
        "\n",
        "    def _fill_buffer(self, enviroment, policy, steps):\n",
        "        for _ in range(steps):\n",
        "            self.timestamp_data(enviroment, policy)\n",
        "\n",
        "    def timestamp_data(self, environment, policy):\n",
        "        time_step = environment.current_time_step()\n",
        "        action_step = policy.action(time_step)\n",
        "        next_time_step = environment.step(action_step.action)\n",
        "        timestamp_trajectory = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "        self._replay_buffer.add_batch(timestamp_trajectory)"
      ],
      "metadata": {
        "id": "9mBwHUGVIHlA"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_average_reward(environment, policy, episodes=10):\n",
        "\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        time_step = environment.reset()\n",
        "        episode_reward = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = environment.step(action_step.action)\n",
        "        episode_reward += time_step.reward\n",
        "\n",
        "    total_reward += episode_reward\n",
        "    avg_reward = total_reward / episodes\n",
        "\n",
        "    return avg_reward.numpy()[0]"
      ],
      "metadata": {
        "id": "uXLDTJawKBqv"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(agent):\n",
        "    experience_replay = ExperienceReplay(agent, train_env)\n",
        "\n",
        "    agent.train_step_counter.assign(0)\n",
        "\n",
        "    avg_reward = get_average_reward(evaluation_env, agent.policy, EVAL_EPISODES)\n",
        "    rewards = [avg_reward]\n",
        "\n",
        "    for _ in range(NUMBER_EPISODES):\n",
        "\n",
        "        for _ in range(COLLECTION_STEPS):\n",
        "            experience_replay.timestamp_data(train_env, agent.collect_policy)\n",
        "\n",
        "        experience, info = next(experience_replay.iterator)\n",
        "        train_loss = agent.train(experience).loss\n",
        "\n",
        "        if agent.train_step_counter.numpy() % EVAL_INTERVAL == 0:\n",
        "            avg_reward = get_average_reward(evaluation_env, agent.policy, EVAL_EPISODES)\n",
        "            print('Episode {0} - Average reward = {1}, Loss = {2}.'.format(\n",
        "\t\t\t\t\tagent.train_step_counter.numpy(), avg_reward, train_loss))\n",
        "            rewards.append(avg_reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "print(\"**********************************\")\n",
        "print(\"Training DDQN\")\n",
        "print(\"**********************************\")\n",
        "C51_reward = train(C51_agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUKOKFyTJm-U",
        "outputId": "5ad66ff0-2635-4105-8d53-e7a94ee6348e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************\n",
            "Training DDQN\n",
            "**********************************\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}