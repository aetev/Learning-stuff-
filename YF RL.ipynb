{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/YF%20RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_agents[reverb]"
      ],
      "metadata": {
        "id": "so8O99AzB95T",
        "outputId": "45e3e38a-809c-4101-b2e1-53f68e974651",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf_agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.22.0)\n",
            "Requirement already satisfied: rlds in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: dm-reverb~=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.13.0)\n",
            "Requirement already satisfied: tensorflow~=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.13.0->tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.13.0->tf_agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf_agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf_agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->tf_agents[reverb]) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (3.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.13.0->tf_agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf_agents[reverb]) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from tf_agents.networks import network\n",
        "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
        "from tf_agents.environments import py_environment, tf_py_environment, utils\n",
        "from tf_agents.specs import array_spec, tensor_spec\n",
        "from tf_agents.trajectories import trajectory, time_step as ts\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.utils import common"
      ],
      "metadata": {
        "id": "R8uczkI2c8cJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the date 730 days ago\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=729)\n",
        "\n",
        "# Download the data\n",
        "data = yf.download('EURUSD=X', start=start_date, end=end_date, interval='1h')\n",
        "\n",
        "# Print the data\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uHZHAsUc_cw",
        "outputId": "3d3f2636-0ec6-4e12-ae71-9678a46b4f70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "                               Open      High       Low     Close  Adj Close  \\\n",
            "Datetime                                                                       \n",
            "2021-10-18 07:00:00+01:00  1.157943  1.158346  1.157675  1.158212   1.158212   \n",
            "2021-10-18 08:00:00+01:00  1.157943  1.159958  1.157943  1.159689   1.159689   \n",
            "2021-10-18 09:00:00+01:00  1.159689  1.160093  1.158480  1.158480   1.158480   \n",
            "2021-10-18 10:00:00+01:00  1.158614  1.159152  1.158078  1.158614   1.158614   \n",
            "2021-10-18 11:00:00+01:00  1.158480  1.159958  1.158078  1.159555   1.159555   \n",
            "...                             ...       ...       ...       ...        ...   \n",
            "2023-10-17 01:00:00+01:00  1.055855  1.056189  1.055855  1.055855   1.055855   \n",
            "2023-10-17 02:00:00+01:00  1.055743  1.055855  1.055298  1.055520   1.055520   \n",
            "2023-10-17 03:00:00+01:00  1.055520  1.055520  1.054964  1.055298   1.055298   \n",
            "2023-10-17 04:00:00+01:00  1.055298  1.055632  1.055075  1.055298   1.055298   \n",
            "2023-10-17 05:00:00+01:00  1.055298  1.055298  1.055075  1.055075   1.055075   \n",
            "\n",
            "                           Volume  \n",
            "Datetime                           \n",
            "2021-10-18 07:00:00+01:00       0  \n",
            "2021-10-18 08:00:00+01:00       0  \n",
            "2021-10-18 09:00:00+01:00       0  \n",
            "2021-10-18 10:00:00+01:00       0  \n",
            "2021-10-18 11:00:00+01:00       0  \n",
            "...                           ...  \n",
            "2023-10-17 01:00:00+01:00       0  \n",
            "2023-10-17 02:00:00+01:00       0  \n",
            "2023-10-17 03:00:00+01:00       0  \n",
            "2023-10-17 04:00:00+01:00       0  \n",
            "2023-10-17 05:00:00+01:00       0  \n",
            "\n",
            "[12394 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "High = data['High'].to_numpy()\n",
        "Close = data['Close'].to_numpy()\n",
        "Low = data['Low'].to_numpy()\n",
        "Std = np.std(np.diff(Close))\n",
        "Close_Diff = np.diff(Close)/Std\n",
        "High_Adj = (High[1:]-Close[1:])/Std\n",
        "Low_Adj = (Close[1:]-Low[1:])/Std\n",
        "# Get the sum of adjacent values\n",
        "sum_array = Close_Diff[:-1] + Close_Diff[1:]\n",
        "print(sum_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pteR2bFg43J",
        "outputId": "99244351-9888-4f8f-f9d3-e3f876d9eafe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.21484273 -0.86061168  0.86042079 ... -0.44600625 -0.17838341\n",
            " -0.17828797]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stack = np.stack((Close_Diff, High_Adj, Low_Adj), axis=1)\n",
        "print(stack)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phmsBJRwEQAX",
        "outputId": "55cc97c3-a072-4e6c-a1b5-1e3c5173bd00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.18292349  0.21541539  1.39767077]\n",
            " [-0.96808076  1.29125156  0.        ]\n",
            " [ 0.10746909  0.43016267  0.42968546]\n",
            " ...\n",
            " [-0.17838341  0.17838341  0.26743195]\n",
            " [ 0.          0.2675274   0.17828797]\n",
            " [-0.17828797  0.17828797  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 50\n",
        "window_view = np.lib.stride_tricks.sliding_window_view(stack, (window_size,),axis=0)"
      ],
      "metadata": {
        "id": "THv9cmvhmfEG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = window_view[:-1]\n",
        "target = Close_Diff[window_size:]"
      ],
      "metadata": {
        "id": "h_w0wsB0EGOL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='red'><center>**-------------------Create Environment-------------------**</font>"
      ],
      "metadata": {
        "id": "Inm6-iBTDdHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int64, minimum=0, maximum=3, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(12,), dtype=np.float32, minimum=-100,maximum=100, name='observation')\n",
        "\n",
        "\n",
        "    self._state = [0] * 12\n",
        "    self._episode_ended = False\n",
        "    self._count = np.random.randint(10000,60000)\n",
        "    self._end_ep = self._count+max_ep_len\n",
        "    self._scaler = np.sum(np.abs(change[self._count:self._end_ep]))\n",
        "    self._balance = 0\n",
        "    self._prev_dff = 0\n",
        "    self._returns = np.array([])\n",
        "    self.trade_info = {\n",
        "    'time': 0,\n",
        "    'value': 0,\n",
        "    'direction': 0,\n",
        "    'max_val': 0,\n",
        "    'min_val': 0,\n",
        "    'max_time': 0,\n",
        "    'min_time': 0\n",
        "    }\n",
        "\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def get_balance(self):\n",
        "    return self._balance\n",
        "\n",
        "  def _reset(self):\n",
        "\n",
        "    self._state = [0] * 12\n",
        "    self._episode_ended = False\n",
        "    self._count = np.random.randint(10000,60000)\n",
        "    self._end_ep = self._count+max_ep_len\n",
        "    self._scaler = np.sum(np.abs(change[self._count:self._end_ep]))\n",
        "    self._returns = np.array([])\n",
        "    self._balance = 0\n",
        "    self._prev_diff = 0\n",
        "    self.trade_info = {\n",
        "    'time': 0,\n",
        "    'value': 0,\n",
        "    'direction': 0,\n",
        "    'max_val': 0,\n",
        "    'min_val': 0,\n",
        "    'max_time': 0,\n",
        "    'min_time': 0\n",
        "    }\n",
        "\n",
        "    return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "  def _step(self, action):\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "\n",
        "    mult = 10\n",
        "    diff_hold = DIFF_hold[self._count]\n",
        "    profit = 0\n",
        "    reward = 0\n",
        "    cur_change = change[self._count+1]/mult\n",
        "\n",
        "    cur_direction = self.trade_info['direction']\n",
        "\n",
        "    if action == 0:\n",
        "      profit = self.trade_info['value']-.005\n",
        "      reward = profit\n",
        "      self.trade_info['value'] = cur_change\n",
        "      self.trade_info['direction'] = 1\n",
        "      self.trade_info['time'] = 0\n",
        "      self.trade_info['max_val'] = cur_change\n",
        "      self.trade_info['min_val'] = cur_change\n",
        "      self.trade_info['max_time'] = 0\n",
        "      self.trade_info['min_time'] = 0\n",
        "    elif action == 1:\n",
        "      profit = self.trade_info['value']-.005\n",
        "      reward = profit\n",
        "      self.trade_info['value'] = -cur_change\n",
        "      self.trade_info['direction'] = -1\n",
        "      self.trade_info['time'] = 0\n",
        "      self.trade_info['max_val'] = -cur_change\n",
        "      self.trade_info['min_val'] = -cur_change\n",
        "      self.trade_info['max_time'] = 0\n",
        "      self.trade_info['min_time'] = 0\n",
        "      #reward = profit if profit > 0 else profit/2\n",
        "    elif action == 2:\n",
        "      profit = self.trade_info['value']\n",
        "      reward = profit\n",
        "      self.trade_info['value'] = 0\n",
        "      self.trade_info['direction'] = 0\n",
        "      self.trade_info['time'] = 0 if cur_direction != 0 else self.trade_info['time'] + .01\n",
        "      self.trade_info['max_val'] = 0\n",
        "      self.trade_info['min_val'] = 0\n",
        "      self.trade_info['max_time'] = 0\n",
        "      self.trade_info['min_time'] = 0\n",
        "    else:\n",
        "      self.trade_info['value'] += cur_change*cur_direction\n",
        "      self.trade_info['time'] += .01\n",
        "      self.trade_info['max_val'] = max(self.trade_info['value'],self.trade_info['max_val'])\n",
        "      self.trade_info['min_val'] = min(self.trade_info['value'],self.trade_info['min_val'])\n",
        "      self.trade_info['max_time'] = 0 if self.trade_info['max_val']==self.trade_info['value'] else self.trade_info['max_time']+.01\n",
        "      self.trade_info['min_time'] = 0 if self.trade_info['min_val']==self.trade_info['value'] else self.trade_info['min_time']+.01\n",
        "\n",
        "\n",
        "\n",
        "    self._balance += profit\n",
        "    if profit != 0:\n",
        "        self._returns = np.append(self._returns, profit)\n",
        "\n",
        "    info_list = [\n",
        "        self.trade_info['value'],\n",
        "        self.trade_info['direction'],\n",
        "        self.trade_info['time'],\n",
        "        self.trade_info['max_val'],\n",
        "        self.trade_info['min_val'],\n",
        "        self.trade_info['max_time'],\n",
        "        self.trade_info['min_time']\n",
        "    ]\n",
        "\n",
        "\n",
        "    #update state\n",
        "    self._state = np.append(INDICATORS[self._count],info_list)\n",
        "\n",
        "    if self._count >= self._end_ep:\n",
        "      self._episode_ended = True\n",
        "      self._returns.append(self.trade_info['value'])\n",
        "\n",
        "    self._count += 1\n",
        "\n",
        "    if self._episode_ended:\n",
        "      #self._returns = [x for x in self._returns if x >= 0]\n",
        "      #reward = np.sum(self._returns)/self._scaler if np.sum(self._returns) != 0 else -1\n",
        "      #reward = np.mean(self._returns) if np.mean(self._returns) != 0 else -10\n",
        "      reward = SortinoRatio(self._returns,0) if np.sum(self._returns) != 0 else -10\n",
        "      return ts.termination(np.array(self._state, dtype=np.float32), reward=reward)\n",
        "    else:\n",
        "      return ts.transition(\n",
        "          np.array(self._state, dtype=np.float32), reward=reward, discount=.999)"
      ],
      "metadata": {
        "id": "-1CmsW97E5gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TradingEnv()\n",
        "utils.validate_py_environment(env, episodes=2)\n",
        "\n",
        "print('Observation Spec:')\n",
        "print(train_env.time_step_spec().observation)\n",
        "\n",
        "print('Reward Spec:')\n",
        "print(train_env.time_step_spec().reward)\n",
        "\n",
        "print('Action Spec:')\n",
        "print(train_env.action_spec())\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(env)\n",
        "evaluation_env = tf_py_environment.TFPyEnvironment(env)"
      ],
      "metadata": {
        "id": "SUTrZ_cnF4ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='blue'><center>**-------------------Create Network-------------------**</font>"
      ],
      "metadata": {
        "id": "VE8qjYImGIEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomQNetwork(network.Network):\n",
        "    def __init__(self, input_tensor_spec, action_spec, name='CustomQNetwork'):\n",
        "        super(CustomQNetwork, self).__init__(input_tensor_spec=input_tensor_spec,\n",
        "                                             state_spec=(),\n",
        "                                             name=name)\n",
        "        # Define your custom layers and architecture here\n",
        "        self.dense1 = layers.Dense(100,activation='gelu')\n",
        "        self.drop1 = layers.Dropout(.2)\n",
        "        self.dense2 = layers.Dense(100,activation='gelu')\n",
        "        self.drop2 = layers.Dropout(.2)\n",
        "        self.dense2 = layers.Dense(100,activation='gelu')\n",
        "        self.drop2 = layers.Dropout(.2)\n",
        "        self.q_values_layer = tf.keras.layers.Dense(action_spec.maximum - action_spec.minimum + 1,\n",
        "                                                    activation=None)\n",
        "\n",
        "    def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "        # Pass the observation through your custom layers\n",
        "        x = self.dense1(observation)\n",
        "        x = self.drop1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.drop2(x)\n",
        "        q_values = self.q_values_layer(x)\n",
        "        return q_values, network_state"
      ],
      "metadata": {
        "id": "p-oj9MnyBRRT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalQNetwork(network.Network):\n",
        "    def __init__(self, input_tensor_spec, action_spec, num_atoms=51, preprocessing_layers=None,\n",
        "                 preprocessing_combiner=None, conv_layer_params=None, fc_layer_params=None,\n",
        "                 activation_fn=tf.nn.relu, name='CategoricalQNetwork'):\n",
        "        super(CategoricalQNetwork, self).__init__(input_tensor_spec=input_tensor_spec,\n",
        "                                                  state_spec=(),\n",
        "                                                  name=name)\n",
        "        # Check if action_spec is a BoundedTensorSpec\n",
        "        if not isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n",
        "            raise TypeError('action_spec must be a BoundedTensorSpec. Got: %s' % (action_spec,))\n",
        "\n",
        "        self._num_actions = action_spec.maximum - action_spec.minimum + 1\n",
        "        self._num_atoms = num_atoms\n",
        "\n",
        "        # Create a BoundedTensorSpec for the q_network action\n",
        "        q_network_action_spec = tensor_spec.BoundedTensorSpec((), tf.int32,\n",
        "                                                              minimum=0,\n",
        "                                                              maximum=self._num_actions * num_atoms - 1)\n",
        "\n",
        "        # Create the q_network\n",
        "        self._q_network = CustomQNetwork(input_tensor_spec=input_tensor_spec,\n",
        "                                             action_spec=q_network_action_spec,\n",
        "                                             )\n",
        "    def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "        logits, network_state = self._q_network(observation, step_type, network_state, training=training)\n",
        "        logits = tf.reshape(logits, [-1, self._num_actions, self._num_atoms])\n",
        "        return logits, network_state\n",
        "    @property\n",
        "    def num_atoms(self):\n",
        "        return self._num_atoms"
      ],
      "metadata": {
        "id": "D5B8riShDDw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation_spec = train_env.observation_spec()\n",
        "action_spec = train_env.action_spec()\n",
        "num_atoms = 51\n",
        "custom_network = CategoricalQNetwork(observation_spec,action_spec, num_atoms)"
      ],
      "metadata": {
        "id": "lfIpj9ALG_IN",
        "outputId": "d1e663f4-accd-45e3-c541-d75570cbac56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ace5c3556023>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobservation_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maction_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_atoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcustom_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategoricalQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_atoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_env' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=.01)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "C51_agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    categorical_q_network=custom_network,\n",
        "    optimizer=optimizer,\n",
        "    min_q_value=-5,\n",
        "    max_q_value=5,\n",
        "    n_step_update=2,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=1,\n",
        "    train_step_counter=train_step_counter)\n",
        "C51_agent.initialize()"
      ],
      "metadata": {
        "id": "9ebSdMkaHHZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='green'><center>**-------------------Create Replay Buffer And Training Loop-------------------**</font>"
      ],
      "metadata": {
        "id": "-mEDhh1OH2Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_average_reward(environment, policy, episodes=10):\n",
        "\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        time_step = environment.reset()\n",
        "        episode_reward = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = environment.step(action_step.action)\n",
        "        episode_reward += time_step.reward\n",
        "\n",
        "    total_reward += episode_reward\n",
        "    avg_reward = total_reward / episodes\n",
        "\n",
        "    return avg_reward.numpy()[0]"
      ],
      "metadata": {
        "id": "uXLDTJawKBqv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplay(object):\n",
        "    def __init__(self, agent, enviroment):\n",
        "        self._replay_buffer = TFUniformReplayBuffer(\n",
        "            data_spec=agent.collect_data_spec,\n",
        "            batch_size=enviroment.batch_size,\n",
        "            max_length=10000)\n",
        "\n",
        "        self._random_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                enviroment.action_spec())\n",
        "\n",
        "        self._fill_buffer(train_env, self._random_policy, steps=100)\n",
        "\n",
        "        self.dataset = self._replay_buffer.as_dataset(\n",
        "            num_parallel_calls=tf.data.AUTOTUNE,\n",
        "            sample_batch_size=BATCH_SIZE,\n",
        "            num_steps=2).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        self.iterator = iter(self.dataset)\n",
        "\n",
        "\n",
        "\n",
        "    def _fill_buffer(self, enviroment, policy, steps):\n",
        "        for _ in range(steps):\n",
        "            self.timestamp_data(enviroment, policy)\n",
        "\n",
        "    def timestamp_data(self, environment, policy):\n",
        "        time_step = environment.current_time_step()\n",
        "        action_step = policy.action(time_step)\n",
        "        next_time_step = environment.step(action_step.action)\n",
        "        timestamp_trajectory = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "        self._replay_buffer.add_batch(timestamp_trajectory)"
      ],
      "metadata": {
        "id": "9mBwHUGVIHlA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(agent):\n",
        "    experience_replay = ExperienceReplay(agent, train_env)\n",
        "\n",
        "    agent.train_step_counter.assign(0)\n",
        "\n",
        "    avg_reward = get_average_reward(evaluation_env, agent.policy, EVAL_EPISODES)\n",
        "    rewards = [avg_reward]\n",
        "\n",
        "    for _ in range(NUMBER_EPOSODES):\n",
        "\n",
        "        for _ in range(COLLECTION_STEPS):\n",
        "            experience_replay.timestamp_data(train_env, agent.collect_policy)\n",
        "\n",
        "        experience, info = next(experience_replay.iterator)\n",
        "        train_loss = agent.train(experience).loss\n",
        "\n",
        "        if agent.train_step_counter.numpy() % EVAL_INTERVAL == 0:\n",
        "            avg_reward = get_average_reward(evaluation_env, agent.policy, EVAL_EPISODES)\n",
        "            print('Episode {0} - Average reward = {1}, Loss = {2}.'.format(\n",
        "\t\t\t\t\tagent.train_step_counter.numpy(), avg_reward, train_loss))\n",
        "            rewards.append(avg_reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "print(\"**********************************\")\n",
        "print(\"Training DDQN\")\n",
        "print(\"**********************************\")\n",
        "C51_reward = train(C51_agent)"
      ],
      "metadata": {
        "id": "EUKOKFyTJm-U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}