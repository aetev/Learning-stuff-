{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/GET%20REAl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWAdRqLdGEyf",
        "outputId": "b1cc2b89-f5f9-4420-c5cf-f5a625028e14",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIPBOzgXLKp5",
        "outputId": "165af880-dedf-4524-a52b-251ad4c9c0a5",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas_ta in /usr/local/lib/python3.10/dist-packages (0.3.14b0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pandas_ta) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->pandas_ta) (1.16.0)\n",
            "Requirement already satisfied: tf_agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.20.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.20.1)\n",
            "Requirement already satisfied: rlds in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: dm-reverb~=0.12.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.12.0)\n",
            "Requirement already satisfied: tensorflow~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.13.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.12.0->tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.12.0->tf_agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf_agents[reverb]) (0.33.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.20.1->tf_agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf_agents[reverb]) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2.3.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.12.0->tf_agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf_agents[reverb]) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas_ta\n",
        "!pip install tf_agents[reverb]\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install line_profiler"
      ],
      "metadata": {
        "id": "miwvqJBWNbfs",
        "outputId": "aecfb114-1796-4fad-bd41-9c6ad81dcf5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: line_profiler in /usr/local/lib/python3.10/dist-packages (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fbSGyTvWHsrH"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mknrqfmnKQSP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_ta as ta\n",
        "import matplotlib.pyplot as plt\n",
        "import reverb\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import layers\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import epsilon_greedy_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies.epsilon_greedy_policy import EpsilonGreedyPolicy\n",
        "from tf_agents.drivers import dynamic_episode_driver\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.environments.batched_py_environment import BatchedPyEnvironment\n",
        "from tf_agents.networks import categorical_q_network\n",
        "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK-NO8kyHsrJ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Disable all GPUS\n",
        "    tf.config.set_visible_devices([], 'GPU')\n",
        "    visible_devices = tf.config.get_visible_devices()\n",
        "    for device in visible_devices:\n",
        "        assert device.device_type != 'GPU'\n",
        "except:\n",
        "    # Invalid device or cannot modify virtual devices once initialized.\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81IQ0kMGZEBK"
      },
      "outputs": [],
      "source": [
        "num_iterations = 20000000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 1000  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 3  # @param {type:\"integer\"}\n",
        "eval_interval = 100000  # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaIxTZWCHydp"
      },
      "outputs": [],
      "source": [
        "def get_relative_change(df, column_name):\n",
        "    relative_changes = []\n",
        "    for i in range(len(df)):\n",
        "        if i == 0:\n",
        "            relative_changes.append(0)  # First element has no previous value\n",
        "        else:\n",
        "            relative_change = (df.iloc[i] - df.iloc[i-1]) / df.iloc[i-1]\n",
        "            relative_changes.append(relative_change)\n",
        "    return pd.DataFrame({column_name: relative_changes})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwSR4EWTLbDK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/eurusd_hour.csv')\n",
        "close = df['AC']\n",
        "open = df['AO']\n",
        "high = df['AH']\n",
        "low = df['AL']\n",
        "achange = df['ACh']\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szXaEp64McUL"
      },
      "outputs": [],
      "source": [
        "RSI = ta.rsi(close,14,scalar=1)\n",
        "AROON = ta.aroon(high,low,14,scalar = 1)\n",
        "AROON_UP = AROON['AROOND_14']\n",
        "AROON_DOWN = AROON['AROONU_14']\n",
        "CCI = ta.cci(high,low,close,14)\n",
        "CCI = CCI.multiply(.001)\n",
        "CCI = CCI.add(.5)\n",
        "RVI = ta.rvi(close,high,low,14,scalar=1)\n",
        "CHANGE = achange\n",
        "CHANGE.name = 'Change'\n",
        "lookback = 10\n",
        "DIFF_enter = ta.sma(achange/np.std(achange),lookback,offset = -lookback)\n",
        "DIFF_enter.name = 'DIFF_enter'\n",
        "\n",
        "DIFF_hold = ta.sma(achange/np.std(achange),lookback,offset = -(lookback/2))\n",
        "DIFF_hold.name = 'DIFF_hold'\n",
        "\n",
        "INDICATORS = pd.concat([CHANGE,RSI,AROON_UP,AROON_DOWN,CCI,RVI,DIFF_enter,DIFF_hold],axis=1)\n",
        "\n",
        "INDICATORS = INDICATORS.dropna(axis=0)\n",
        "INDICATORS = INDICATORS.reset_index()\n",
        "\n",
        "change = INDICATORS.pop('Change')\n",
        "DIFF_enter = INDICATORS.pop('DIFF_enter')\n",
        "DIFF_hold = INDICATORS.pop('DIFF_hold')\n",
        "INDICATORS.pop('index')\n",
        "\n",
        "INDICATORS = INDICATORS.to_numpy()\n",
        "change = change.to_numpy()\n",
        "change_dev = change.std()\n",
        "change = change/change_dev\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(DIFF_hold))"
      ],
      "metadata": {
        "id": "--kfiDJdwq_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XMF1wKV6e_X"
      },
      "outputs": [],
      "source": [
        "plt.plot(INDICATORS)\n",
        "plt.xlim(5400,5500)\n",
        "#print(INDICATORS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcjEVE3Rh_5l"
      },
      "outputs": [],
      "source": [
        "def custom_formatter(x):\n",
        "  return f'{x:.4f}'\n",
        "\n",
        "\n",
        "def test_environment(env, num_steps):\n",
        "    # Reset the environment\n",
        "    observation = env.reset()\n",
        "\n",
        "    # Get the upper bound of the action range\n",
        "    action_spec = env.action_spec().maximum\n",
        "\n",
        "    # Loop through the time steps and take random actions\n",
        "    for _ in range(num_steps):\n",
        "        # Select a random action from the range of valid values\n",
        "        action = random.randint(0, action_spec)\n",
        "\n",
        "        # Execute the action and get the next observation, reward, done, and info\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # Print the current time step\n",
        "        np.set_printoptions(formatter={'float_kind':custom_formatter})\n",
        "        print(action,\"{:.4f}\".format(reward),info[5:12])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiJi0KTZ4iOU"
      },
      "outputs": [],
      "source": [
        "def cross(cur_diff,prev_diff):\n",
        "  if cur_diff > 0 and prev_diff <= 0:\n",
        "    return 5\n",
        "  elif cur_diff < 0 and prev_diff >= 0:\n",
        "    return 5\n",
        "  else:\n",
        "    return 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ2nDR1p1e9U"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQreCJAi-KAU"
      },
      "outputs": [],
      "source": [
        "max_ep_len = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHXn3fUdHsrO"
      },
      "outputs": [],
      "source": [
        "def SortinoRatio(df, T):\n",
        "    \"\"\"Calculates the Sortino ratio from univariate excess returns.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        df ([float]): The dataframe or pandas series of univariate excess returns.\n",
        "        T ([integer]): The targeted return.\n",
        "    \"\"\"\n",
        "\n",
        "    #downside deviation:\n",
        "\n",
        "    #temp = np.minimum(0, df - T)**2\n",
        "    temp = np.square(np.minimum(0, df - T))\n",
        "    temp_expectation = np.mean(temp)\n",
        "    downside_dev = np.sqrt(temp_expectation)\n",
        "\n",
        "    if downside_dev == 0:\n",
        "      return 0\n",
        "\n",
        "    #Sortino ratio:\n",
        "\n",
        "    sortino_ratio = np.mean(df - T) / downside_dev\n",
        "\n",
        "    return(sortino_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext line_profiler"
      ],
      "metadata": {
        "id": "kc0S_Bp-NsAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = np.random.normal(loc=0, scale=1, size=100)\n",
        "T = 0"
      ],
      "metadata": {
        "id": "_u6GZdpOM-Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%lprun -f SortinoRatio SortinoRatio(df,T)"
      ],
      "metadata": {
        "id": "voSmIuZ7MkfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions = {\n",
        "    0: lambda cur_change: {\n",
        "        'value': cur_change,\n",
        "        'direction': 1,\n",
        "        'time': 0,\n",
        "        'max_val': cur_change,\n",
        "        'min_val': cur_change,\n",
        "        'max_time': 0,\n",
        "        'min_time': 0\n",
        "    },\n",
        "    1: lambda cur_change: {\n",
        "        'value': -cur_change,\n",
        "        'direction': -1,\n",
        "        'time': 0,\n",
        "        'max_val': -cur_change,\n",
        "        'min_val': -cur_change,\n",
        "        'max_time': 0,\n",
        "        'min_time': 0\n",
        "    },\n",
        "    2: lambda cur_change: {\n",
        "        'value': 0,\n",
        "        'direction': 0,\n",
        "        'time': 0,\n",
        "        'max_val': 0,\n",
        "        'min_val': 0,\n",
        "        'max_time': 0,\n",
        "        'min_time': 0\n",
        "    }\n",
        "    }"
      ],
      "metadata": {
        "id": "08mFJHAybh-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int64, minimum=0, maximum=3, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(12,), dtype=np.float32, minimum=-100,maximum=100, name='observation')\n",
        "\n",
        "\n",
        "    self._state = np.zeros(12)\n",
        "    self._episode_ended = False\n",
        "    self._count = random.randint(10000,60000)\n",
        "    self._end_ep = self._count+max_ep_len\n",
        "    self._balance = 0\n",
        "    self._returns = np.zeros(0)\n",
        "    self.trade_info = {\n",
        "    'time': 0,\n",
        "    'value': 0,\n",
        "    'direction': 0,\n",
        "    'max_val': 0,\n",
        "    'min_val': 0,\n",
        "    'max_time': 0,\n",
        "    'min_time': 0\n",
        "    }\n",
        "\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def get_balance(self):\n",
        "    return self._balance\n",
        "\n",
        "  def _reset(self):\n",
        "\n",
        "    self._state = np.zeros(12)\n",
        "    self._episode_ended = False\n",
        "    self._count = random.randint(10000,60000)\n",
        "    self._end_ep = self._count+max_ep_len\n",
        "    self._returns = np.zeros(0)\n",
        "    self._balance = 0\n",
        "    self.trade_info = {\n",
        "    'time': 0,\n",
        "    'value': 0,\n",
        "    'direction': 0,\n",
        "    'max_val': 0,\n",
        "    'min_val': 0,\n",
        "    'max_time': 0,\n",
        "    'min_time': 0\n",
        "    }\n",
        "\n",
        "    return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "  def _step(self, action):\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "\n",
        "    mult = 10\n",
        "    diff_hold = DIFF_hold[self._count]\n",
        "    profit = 0\n",
        "    reward = 0\n",
        "    cur_change = change[self._count+1]/mult\n",
        "    cur_direction = self.trade_info['direction']\n",
        "    action_func = actions.get(int(action), None)\n",
        "\n",
        "    if action_func:\n",
        "      profit = self.trade_info['value']\n",
        "      reward = profit if profit >= 0 else max(-profit-.5,0)\n",
        "      action_details = action_func(cur_change)\n",
        "      self.trade_info.update(action_details)\n",
        "    else:\n",
        "      self.trade_info['value'] += cur_change * cur_direction\n",
        "      self.trade_info['time'] += 0.01 if cur_direction != 0 else 0\n",
        "      self.trade_info['max_val'] = max(self.trade_info['value'], self.trade_info['max_val'])\n",
        "      self.trade_info['min_val'] = min(self.trade_info['value'], self.trade_info['min_val'])\n",
        "      self.trade_info['max_time'] = 0 if self.trade_info['max_val'] == self.trade_info['value'] else self.trade_info['max_time'] + 0.01\n",
        "      self.trade_info['min_time'] = 0 if self.trade_info['min_val'] == self.trade_info['value'] else self.trade_info['min_time'] + 0.01\n",
        "      #reward = diff_hold*cur_direction\n",
        "\n",
        "\n",
        "    self._balance += profit\n",
        "    if profit != 0:\n",
        "        self._returns = np.append(self._returns, profit)\n",
        "\n",
        "    #info_list = np.fromiter(self.trade_info.values(), dtype=float)\n",
        "    info_list = list(self.trade_info.values())\n",
        "\n",
        "    #update state\n",
        "    self._state = np.append(INDICATORS[self._count],info_list)\n",
        "\n",
        "    if self._count >= self._end_ep:\n",
        "      self._episode_ended = True\n",
        "      np.append(self._returns,self.trade_info['value'])\n",
        "\n",
        "    self._count += 1\n",
        "\n",
        "    if self._episode_ended:\n",
        "\n",
        "      reward = SortinoRatio(self._returns,0) if np.sum(self._returns) != 0 else -10\n",
        "      return ts.termination(np.array(self._state, dtype=np.float32), reward=reward)\n",
        "    else:\n",
        "      return ts.transition(\n",
        "          np.array(self._state, dtype=np.float32), reward=reward, discount=.9999)"
      ],
      "metadata": {
        "id": "rd6r5RIKcwXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3gkt6dv_0e_"
      },
      "outputs": [],
      "source": [
        "env = TradingEnv()\n",
        "\n",
        "\n",
        "utils.validate_py_environment(env, episodes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMb4Gnsh_ktX"
      },
      "outputs": [],
      "source": [
        "train_py_env = env\n",
        "eval_py_env = env\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit"
      ],
      "metadata": {
        "id": "EHsc_CIInHLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3YHkyIzjE3h",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "timeit_result = timeit.timeit(lambda: test_environment(env, 1000), number=10)\n",
        "print(timeit_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cumpJmHpHJk"
      },
      "outputs": [],
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZGBmV3KI0HG"
      },
      "outputs": [],
      "source": [
        "fc_layer_params = (20, 5)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smmf6KQZMjwZ"
      },
      "outputs": [],
      "source": [
        "def activity_regularizer(multiplier, decay=0.99):\n",
        "    # Initialize moving averages for mean and variance\n",
        "    mean_avg = tf.Variable(0.0, trainable=False, name='mean_avg')\n",
        "    variance_avg = tf.Variable(1.0, trainable=False, name='variance_avg')\n",
        "\n",
        "    def regularizer(y_pred):\n",
        "        # Calculate the current mean and variance\n",
        "        current_mean = tf.reduce_mean(y_pred)\n",
        "        current_variance = tf.math.reduce_variance(y_pred)\n",
        "\n",
        "        # Update moving averages\n",
        "        update_mean = tf.compat.v1.assign(mean_avg, decay * mean_avg + (1 - decay) * current_mean)\n",
        "        update_variance = tf.compat.v1.assign(variance_avg, decay * variance_avg + (1 - decay) * current_variance)\n",
        "\n",
        "        with tf.control_dependencies([update_mean, update_variance]):\n",
        "            # Use the moving averages for regularization\n",
        "            mean = mean_avg\n",
        "            variance = variance_avg\n",
        "\n",
        "        return multiplier * (tf.square(mean - 0) + tf.square(variance - 1))\n",
        "\n",
        "    return regularizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8KlpFbxHsrR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#import base64\n",
        "#import imageio\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent, DdqnAgent\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "\n",
        "# Globals\n",
        "NUMBER_EPOSODES = 1000000\n",
        "COLLECTION_STEPS = 1\n",
        "BATCH_SIZE = 32\n",
        "EVAL_EPISODES = 3\n",
        "EVAL_INTERVAL = 1000\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15AV9bY3HsrS"
      },
      "outputs": [],
      "source": [
        "print('Observation Spec:')\n",
        "print(train_env.time_step_spec().observation)\n",
        "\n",
        "print('Reward Spec:')\n",
        "print(train_env.time_step_spec().reward)\n",
        "\n",
        "print('Action Spec:')\n",
        "print(train_env.action_spec())\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(env)\n",
        "evaluation_env = tf_py_environment.TFPyEnvironment(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70bNgM1EHsrS"
      },
      "outputs": [],
      "source": [
        "from tf_agents.networks import network\n",
        "\n",
        "\n",
        "class CustomQNetwork(network.Network):\n",
        "    def __init__(self, input_tensor_spec, action_spec, name='CustomQNetwork'):\n",
        "        super(CustomQNetwork, self).__init__(input_tensor_spec=input_tensor_spec,\n",
        "                                             state_spec=(),\n",
        "                                             name=name)\n",
        "        # Define your custom layers and architecture here\n",
        "        self.dense1 = layers.Dense(100,activation='gelu', activity_regularizer=activity_regularizer(0.01))\n",
        "        self.drop1 = layers.Dropout(.2)\n",
        "        self.dense2 = layers.Dense(100,activation='gelu', activity_regularizer=activity_regularizer(0.01))\n",
        "        self.drop2 = layers.Dropout(.2)\n",
        "        self.dense2 = layers.Dense(100,activation='gelu', activity_regularizer=activity_regularizer(0.01))\n",
        "        self.drop2 = layers.Dropout(.2)\n",
        "        self.q_values_layer = tf.keras.layers.Dense(action_spec.maximum - action_spec.minimum + 1,\n",
        "                                                    activation=None)\n",
        "\n",
        "    def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "        # Pass the observation through your custom layers\n",
        "        x = self.dense1(observation)\n",
        "        x = self.drop1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.drop2(x)\n",
        "        q_values = self.q_values_layer(x)\n",
        "        return q_values, network_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOcJ5ohTHsrS"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CategoricalQNetwork(network.Network):\n",
        "    def __init__(self, input_tensor_spec, action_spec, num_atoms=51,name='CategoricalQNetwork'):\n",
        "        super(CategoricalQNetwork, self).__init__(input_tensor_spec=input_tensor_spec,\n",
        "                                                  state_spec=(),\n",
        "                                                  name=name)\n",
        "        # Check if action_spec is a BoundedTensorSpec\n",
        "        if not isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n",
        "            raise TypeError('action_spec must be a BoundedTensorSpec. Got: %s' % (action_spec,))\n",
        "\n",
        "        self._num_actions = action_spec.maximum - action_spec.minimum + 1\n",
        "        self._num_atoms = num_atoms\n",
        "\n",
        "        # Create a BoundedTensorSpec for the q_network action\n",
        "        q_network_action_spec = tensor_spec.BoundedTensorSpec((), tf.int32,\n",
        "                                                              minimum=0,\n",
        "                                                              maximum=self._num_actions * num_atoms - 1)\n",
        "\n",
        "        # Create the q_network\n",
        "        self._q_network = CustomQNetwork(input_tensor_spec=input_tensor_spec,\n",
        "                                             action_spec=q_network_action_spec,\n",
        "                                             )\n",
        "    def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "        logits, network_state = self._q_network(observation, step_type, network_state, training=training)\n",
        "        logits = tf.reshape(logits, [-1, self._num_actions, self._num_atoms])\n",
        "        return logits, network_state\n",
        "    @property\n",
        "    def num_atoms(self):\n",
        "        return self._num_atoms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vxRI1cRHsrT"
      },
      "outputs": [],
      "source": [
        "observation_spec = train_env.observation_spec()  # Specify the input tensor spec for your network\n",
        "action_spec = train_env.action_spec()\n",
        "num_atoms = 5\n",
        "dropout_rate = 0.2\n",
        "custom_network = CategoricalQNetwork(observation_spec,action_spec, num_atoms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOq6mCfKHsrT"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    categorical_q_network=custom_network,\n",
        "    optimizer=optimizer,\n",
        "    min_q_value=-3,\n",
        "    max_q_value=3,\n",
        "    n_step_update=2,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=1,\n",
        "    train_step_counter=train_step_counter)\n",
        "agent.initialize()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4tEbNarHsrT"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_average_reward(environment, policy, episodes=10):\n",
        "\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        time_step = environment.reset()\n",
        "        episode_reward = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = environment.step(action_step.action)\n",
        "        episode_reward += time_step.reward\n",
        "\n",
        "    total_reward += episode_reward\n",
        "    avg_reward = total_reward / episodes\n",
        "\n",
        "    return avg_reward.numpy()[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlfObfFGHsrT"
      },
      "outputs": [],
      "source": [
        "class ExperienceReplay(object):\n",
        "    def __init__(self, agent, enviroment):\n",
        "        self._replay_buffer = TFUniformReplayBuffer(\n",
        "            data_spec=agent.collect_data_spec,\n",
        "            batch_size=enviroment.batch_size,\n",
        "            max_length=10000)\n",
        "\n",
        "        self._random_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                enviroment.action_spec())\n",
        "\n",
        "        self._fill_buffer(train_env, self._random_policy, steps=100)\n",
        "\n",
        "        self.dataset = self._replay_buffer.as_dataset(\n",
        "            num_parallel_calls=tf.data.AUTOTUNE,\n",
        "            sample_batch_size=BATCH_SIZE,\n",
        "            num_steps=2).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        self.iterator = iter(self.dataset)\n",
        "\n",
        "\n",
        "\n",
        "    def _fill_buffer(self, enviroment, policy, steps):\n",
        "        for _ in range(steps):\n",
        "            self.timestamp_data(enviroment, policy)\n",
        "\n",
        "    def timestamp_data(self, environment, policy):\n",
        "        time_step = environment.current_time_step()\n",
        "        action_step = policy.action(time_step)\n",
        "        next_time_step = environment.step(action_step.action)\n",
        "        timestamp_trajectory = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "        self._replay_buffer.add_batch(timestamp_trajectory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "AFvLx_a9HsrU"
      },
      "outputs": [],
      "source": [
        "agent.train = common.function(agent.train)\n",
        "\n",
        "def train(agent):\n",
        "    experience_replay = ExperienceReplay(agent, train_env)\n",
        "\n",
        "    agent.train_step_counter.assign(0)\n",
        "\n",
        "    avg_reward = get_average_reward(eval_env, agent.policy, EVAL_EPISODES)\n",
        "    rewards = [avg_reward]\n",
        "\n",
        "    for _ in range(NUMBER_EPOSODES):\n",
        "\n",
        "        for _ in range(COLLECTION_STEPS):\n",
        "            experience_replay.timestamp_data(train_env, agent.collect_policy)\n",
        "\n",
        "        experience, info = next(experience_replay.iterator)\n",
        "        train_loss = agent.train(experience).loss\n",
        "\n",
        "        if agent.train_step_counter.numpy() % EVAL_INTERVAL == 0:\n",
        "            avg_reward = get_average_reward(eval_env, agent.policy, EVAL_EPISODES)\n",
        "            print('Episode {0} - Average reward = {1}, Loss = {2}.'.format(\n",
        "\t\t\t\t\tagent.train_step_counter.numpy(), avg_reward, train_loss))\n",
        "            rewards.append(avg_reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "print(\"**********************************\")\n",
        "print(\"Training DDQN\")\n",
        "print(\"**********************************\")\n",
        "ddqn_reward = train(agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRBz--oy5Nxy"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return_print(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for episode in range(num_episodes):\n",
        "    balance_arr = []\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      obs = time_step.observation.numpy()\n",
        "\n",
        "      balance = env.get_balance()\n",
        "      balance_arr.append(balance)\n",
        "      # Print the episode number and observation at each step\n",
        "      print(f\"Episode {episode + 1}: Observation = {obs}\")\n",
        "\n",
        "      episode_return += time_step.reward\n",
        "    plt.plot(balance_arr)\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27YSeeoMHsrU"
      },
      "outputs": [],
      "source": [
        "avg_reward = get_average_reward(eval_env, agent.policy, 1)\n",
        "print(avg_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "GSG1pY7R5Nx0",
        "outputId": "b46769c9-18fc-49f4-d85c-2394edb9e4a9",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3b9d32b46aa1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavg_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'compute_avg_return_print' is not defined"
          ]
        }
      ],
      "source": [
        "avg_return = compute_avg_return_print(eval_env, agent.policy, 20)\n",
        "print(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9Waz9CaPhVM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "weights = custom_network.get_weights()\n",
        "print(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K34WAN9GHsrV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}