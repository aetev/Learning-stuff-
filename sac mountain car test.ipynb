{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Learning-stuff-/blob/main/sac%20mountain%20car%20test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z1IPtawZ_QU6",
        "outputId": "5c0f93a1-000d-4055-db13-d1dab4a42544",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_agents[reverb]"
      ],
      "metadata": {
        "id": "gkseWxmn_R_h",
        "outputId": "0fb4a9c8-fa44-4c8a-adbf-36a58aafa4a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf_agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (8.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.19.0)\n",
            "Requirement already satisfied: rlds in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: dm-reverb~=0.11.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (0.11.0)\n",
            "Requirement already satisfied: tensorflow~=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.11.0->tf_agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.11.0->tf_agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tf_agents[reverb]) (0.32.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.19.0->tf_agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.12.0->tf_agents[reverb]) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.12.0->tf_agents[reverb]) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.12.0->tf_agents[reverb]) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf_agents[reverb]) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import reverb\n",
        "import tempfile\n",
        "import PIL.Image\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.train import actor\n",
        "from tf_agents.train import learner\n",
        "from tf_agents.train import triggers\n",
        "from tf_agents.train.utils import spec_utils\n",
        "from tf_agents.train.utils import strategy_utils\n",
        "from tf_agents.train.utils import train_utils"
      ],
      "metadata": {
        "id": "BDqp9KQn_yrH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
        "# 1e5 is just so this doesn't take too long (1 hr)\n",
        "num_iterations = 100000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256 # @param {type:\"integer\"}\n",
        "\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\n",
        "target_update_period = 1 # @param {type:\"number\"}\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "actor_fc_layer_params = (256, 256)\n",
        "critic_joint_fc_layer_params = (256, 256)\n",
        "\n",
        "log_interval = 5000 # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
        "eval_interval = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "policy_save_interval = 5000 # @param {type:\"integer\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8Xo8gJchAtla"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'MountainCarContinuous-v0'\n",
        "collect_env  = suite_gym.load(env_name)\n",
        "eval_env   = suite_gym.load(env_name)"
      ],
      "metadata": {
        "id": "2lIl-hO-_0Mm"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu = False\n",
        "\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ],
      "metadata": {
        "id": "CkDfs-xfBJUU"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation_spec, action_spec, time_step_spec = (\n",
        "      spec_utils.get_tensor_specs(collect_env))\n",
        "\n",
        "with strategy.scope():\n",
        "  critic_net = critic_network.CriticNetwork(\n",
        "        (observation_spec, action_spec),\n",
        "        observation_fc_layer_params=None,\n",
        "        action_fc_layer_params=None,\n",
        "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        last_kernel_initializer='glorot_uniform')"
      ],
      "metadata": {
        "id": "RbBZPAOXAf78"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=actor_fc_layer_params,\n",
        "      continuous_projection_net=(\n",
        "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
      ],
      "metadata": {
        "id": "jI0afo1FBPga"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  train_step = train_utils.create_train_step()\n",
        "\n",
        "  tf_agent = sac_agent.SacAgent(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        actor_network=actor_net,\n",
        "        critic_network=critic_net,\n",
        "        actor_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=actor_learning_rate),\n",
        "        critic_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=critic_learning_rate),\n",
        "        alpha_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=alpha_learning_rate),\n",
        "        target_update_tau=target_update_tau,\n",
        "        target_update_period=target_update_period,\n",
        "        td_errors_loss_fn=tf.math.squared_difference,\n",
        "        gamma=gamma,\n",
        "        reward_scale_factor=reward_scale_factor,\n",
        "        train_step_counter=train_step)\n",
        "\n",
        "  tf_agent.initialize()"
      ],
      "metadata": {
        "id": "6V3WUw25B1k7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)\n",
        "\n",
        "table_name = 'uniform_table'\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    sequence_length=2,\n",
        "    table_name=table_name,\n",
        "    local_server=reverb_server)"
      ],
      "metadata": {
        "id": "8OtP7sjGBf6R"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = reverb_replay.as_dataset(\n",
        "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
        "experience_dataset_fn = lambda: dataset"
      ],
      "metadata": {
        "id": "S2GPKTjeD8zP"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_eval_policy = tf_agent.policy\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_eval_policy, use_tf_function=True)\n",
        "\n",
        "tf_collect_policy = tf_agent.collect_policy\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_collect_policy, use_tf_function=True)"
      ],
      "metadata": {
        "id": "Q8FrWue9EAej"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  collect_env.time_step_spec(), collect_env.action_spec())"
      ],
      "metadata": {
        "id": "9_GBDROTEQXK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  reverb_replay.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)"
      ],
      "metadata": {
        "id": "eJP741I1ESXV"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  random_policy,\n",
        "  train_step,\n",
        "  steps_per_run=initial_collect_steps,\n",
        "  observers=[rb_observer])\n",
        "initial_collect_actor.run()"
      ],
      "metadata": {
        "id": "g8sD1uCkExqN"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  collect_policy,\n",
        "  train_step,\n",
        "  steps_per_run=1,\n",
        "  metrics=actor.collect_metrics(10),\n",
        "  summary_dir=os.path.join(\"/content/drive/MyDrive\", learner.TRAIN_DIR),\n",
        "  observers=[rb_observer, env_step_metric])"
      ],
      "metadata": {
        "id": "gdA_u_j_Ey6p"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_actor = actor.Actor(\n",
        "  eval_env,\n",
        "  eval_policy,\n",
        "  train_step,\n",
        "  episodes_per_run=num_eval_episodes,\n",
        "  metrics=actor.eval_metrics(num_eval_episodes),\n",
        "  summary_dir=os.path.join(\"/content/drive/MyDrive\", 'eval'),\n",
        ")"
      ],
      "metadata": {
        "id": "IHYlGKpsE0Hs"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_dir = os.path.join(\"/content/drive/MyDrive\", learner.POLICY_SAVED_MODEL_DIR)\n",
        "\n",
        "# Triggers to save the agent's policy checkpoints.\n",
        "learning_triggers = [\n",
        "    triggers.PolicySavedModelTrigger(\n",
        "        saved_model_dir,\n",
        "        tf_agent,\n",
        "        train_step,\n",
        "        interval=policy_save_interval),\n",
        "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
        "]\n",
        "\n",
        "agent_learner = learner.Learner(\n",
        "  \"/content/drive/MyDrive\",\n",
        "  train_step,\n",
        "  tf_agent,\n",
        "  experience_dataset_fn,\n",
        "  triggers=learning_triggers,\n",
        "  strategy=strategy)"
      ],
      "metadata": {
        "id": "ybubwekdFFNs",
        "outputId": "c7d77e9e-fe18-4ca3-ba9e-c65e08293eaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/distributions/distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
            "Instructions for updating:\n",
            "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7f730872dde0>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7f730872dde0>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
            "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
            "WARNING:absl:`0/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation`.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7f730872dde0>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
            "WARNING:absl:Found untraced functions such as ActorDistributionNetwork_layer_call_fn, ActorDistributionNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, TanhNormalProjectionNetwork_layer_call_fn while saving (showing 5 of 14). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7f730872dde0>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
            "WARNING:absl:Found untraced functions such as ActorDistributionNetwork_layer_call_fn, ActorDistributionNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, TanhNormalProjectionNetwork_layer_call_fn while saving (showing 5 of 14). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as ActorDistributionNetwork_layer_call_fn, ActorDistributionNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, TanhNormalProjectionNetwork_layer_call_fn while saving (showing 5 of 14). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:497: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_eval_metrics():\n",
        "  eval_actor.run()\n",
        "  results = {}\n",
        "  for metric in eval_actor.metrics:\n",
        "    results[metric.name] = metric.result()\n",
        "  return results\n",
        "\n",
        "metrics = get_eval_metrics()"
      ],
      "metadata": {
        "id": "NyOHolq_FUhG"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_eval_metrics(step, metrics):\n",
        "  eval_results = (', ').join(\n",
        "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
        "  print('step = {0}: {1}'.format(step, eval_results))\n",
        "\n",
        "log_eval_metrics(0, metrics)"
      ],
      "metadata": {
        "id": "yjlwf8G-FV0m",
        "outputId": "12642f68-533c-4e3d-befb-e6fd394c624c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 0: AverageReturn = -39.634983, AverageEpisodeLength = 999.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Training.\n",
        "  collect_actor.run()\n",
        "  loss_info = agent_learner.run(iterations=1)\n",
        "\n",
        "  # Evaluating.\n",
        "  step = agent_learner.train_step_numpy\n",
        "\n",
        "  if eval_interval and step % eval_interval == 0:\n",
        "    metrics = get_eval_metrics()\n",
        "    log_eval_metrics(step, metrics)\n",
        "    returns.append(metrics[\"AverageReturn\"])\n",
        "\n",
        "  if log_interval and step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
        "\n",
        "rb_observer.close()\n",
        "reverb_server.stop()"
      ],
      "metadata": {
        "id": "aspLZmK1Fjjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}