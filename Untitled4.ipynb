{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xI7rEeMMgDQI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgyl_WtbgO5k",
        "outputId": "5c098cb7-1374-4c0c-eae2-bcee92c51172"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7, input_shape=(100,)))\n",
        "    model.add(layers.Dense(14*14,activation='relu'))\n",
        "    model.add(layers.Dense(28*28,activation='sigmoid'))\n",
        "    model.add(layers.Reshape((28,28,1)))\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    input_img = layers.Input(shape=(28,28,1))\n",
        "\n",
        "    x = layers.Conv2D(64,3,2,padding='same')(input_img)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    dense_output = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "    dense_output = layers.Dense(64, activation='relu')(x)\n",
        "\n",
        "    dense_output = layers.Dense(1, activation='linear')(dense_output)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=input_img, outputs=dense_output)\n",
        "    return model"
      ],
      "metadata": {
        "id": "kik7wQ1qgOV-"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0004)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.0004)"
      ],
      "metadata": {
        "id": "lS0DF8rjhHdY"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 10\n",
        "\n",
        "#@tf.function\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = tf.reduce_mean(real_output)\n",
        "    fake_loss = tf.reduce_mean(fake_output)\n",
        "    total_loss = fake_loss - real_loss\n",
        "    return total_loss\n",
        "\n",
        "#@tf.function\n",
        "def generator_loss(fake_output):\n",
        "    return -tf.reduce_mean(fake_output)\n",
        "\n",
        "#@tf.function\n",
        "def gradient_penalty(real_images, fake_images):\n",
        "    alpha = tf.random.uniform([BATCH_SIZE, 1, 1, 1], 0., 1.)\n",
        "    real_images, fake_images = tf.cast(real_images, tf.float32), tf.cast(fake_images, tf.float32)\n",
        "    interpolated_images = alpha * real_images + ((1 - alpha) * fake_images)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated_images)\n",
        "        pred = discriminator(interpolated_images, training=True)\n",
        "    gradients = tape.gradient(pred, [interpolated_images])[0]\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "    gp = tf.reduce_mean((norm - 1.)**2)\n",
        "    return gp"
      ],
      "metadata": {
        "id": "DYQpIZyEgSlN"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NOISE_DIM = 100\n",
        "GP_WEIGHT = 10\n",
        "\n",
        "\n",
        "#@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        gp = gradient_penalty(images, generated_images)\n",
        "        disc_loss += gp * GP_WEIGHT\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    tf.print(\"disc_loss\",disc_loss,'gen_loss',gen_loss)"
      ],
      "metadata": {
        "id": "oyW06YaegVMB"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for batch in range(len(dataset) // BATCH_SIZE):\n",
        "\n",
        "            target_images = dataset[batch * BATCH_SIZE: (batch+1) * BATCH_SIZE]\n",
        "\n",
        "\n",
        "            train_step(target_images)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      print(epoch)\n",
        "\n"
      ],
      "metadata": {
        "id": "Sx_KbIfjiogE"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "x_train2 = np.expand_dims(x_train, axis=-1)\n",
        "x_train2 = (x_train2 - np.min(x_train2)) / (np.max(x_train2) - np.min(x_train2))\n",
        "train(x_train2, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IWp_3RkPg248",
        "outputId": "7a4a87ae-4605-4655-a1ac-d901f2c11155"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "disc_loss 6.19548798 gen_loss 0.00536694517\n",
            "disc_loss 4.44998932 gen_loss 0.794716358\n",
            "disc_loss 2.81679821 gen_loss 1.48987556\n",
            "disc_loss 0.648393869 gen_loss 2.1395359\n",
            "disc_loss -0.970346451 gen_loss 2.83594441\n",
            "disc_loss -1.97498667 gen_loss 3.52835608\n",
            "disc_loss -3.63129473 gen_loss 4.28111887\n",
            "disc_loss -5.40681553 gen_loss 5.04475451\n",
            "disc_loss -5.56603622 gen_loss 5.799891\n",
            "disc_loss -5.66479 gen_loss 6.46719503\n",
            "disc_loss -5.70435047 gen_loss 7.14926386\n",
            "disc_loss -6.35442 gen_loss 7.97894382\n",
            "disc_loss -6.43544292 gen_loss 8.80826378\n",
            "disc_loss -7.66492653 gen_loss 9.582201\n",
            "disc_loss -8.00772095 gen_loss 10.0660801\n",
            "disc_loss -8.63428879 gen_loss 10.6993799\n",
            "disc_loss -9.65016365 gen_loss 11.209281\n",
            "disc_loss -9.82804871 gen_loss 11.6489029\n",
            "disc_loss -10.3520927 gen_loss 12.454\n",
            "disc_loss -10.300374 gen_loss 12.6810474\n",
            "disc_loss -10.7863579 gen_loss 13.2805233\n",
            "disc_loss -11.1656532 gen_loss 13.7350597\n",
            "disc_loss -10.3760147 gen_loss 13.8227844\n",
            "disc_loss -11.4525986 gen_loss 14.2384357\n",
            "disc_loss -10.9912071 gen_loss 14.3496037\n",
            "disc_loss -10.9774189 gen_loss 14.4185925\n",
            "disc_loss -10.4137383 gen_loss 14.3210068\n",
            "disc_loss -9.94197655 gen_loss 14.3509827\n",
            "disc_loss -10.1453762 gen_loss 13.9594088\n",
            "disc_loss -10.4807205 gen_loss 14.0688229\n",
            "disc_loss -9.23328114 gen_loss 13.4593945\n",
            "disc_loss -10.0108471 gen_loss 13.7609758\n",
            "disc_loss -9.47967529 gen_loss 13.2558765\n",
            "disc_loss -9.76601696 gen_loss 13.1418228\n",
            "disc_loss -9.36506844 gen_loss 12.8270435\n",
            "disc_loss -9.31740952 gen_loss 12.7925491\n",
            "disc_loss -8.83323097 gen_loss 12.5877752\n",
            "disc_loss -8.36307716 gen_loss 12.1866856\n",
            "disc_loss -8.70251656 gen_loss 11.9802618\n",
            "disc_loss -8.59704113 gen_loss 11.568018\n",
            "disc_loss -7.82899046 gen_loss 11.8579636\n",
            "disc_loss -7.77804279 gen_loss 11.0684452\n",
            "disc_loss -7.73939228 gen_loss 10.8967571\n",
            "disc_loss -7.15713 gen_loss 10.4852915\n",
            "disc_loss -7.0774045 gen_loss 10.6383467\n",
            "disc_loss -7.42662907 gen_loss 10.1870174\n",
            "disc_loss -7.24435806 gen_loss 10.0223131\n",
            "disc_loss -6.64504337 gen_loss 9.54075813\n",
            "disc_loss -6.97279787 gen_loss 9.55536175\n",
            "disc_loss -6.41251516 gen_loss 9.03824\n",
            "disc_loss -5.88559294 gen_loss 8.86459541\n",
            "disc_loss -6.37298775 gen_loss 8.31953335\n",
            "disc_loss -5.71121264 gen_loss 8.11211872\n",
            "disc_loss -5.62796831 gen_loss 7.87820816\n",
            "disc_loss -6.23643494 gen_loss 8.23526573\n",
            "disc_loss -6.08256531 gen_loss 7.87153339\n",
            "disc_loss -6.12397623 gen_loss 7.54039526\n",
            "disc_loss -5.93720675 gen_loss 7.71605062\n",
            "disc_loss -5.56824255 gen_loss 7.19123936\n",
            "disc_loss -5.42294 gen_loss 7.08251667\n",
            "disc_loss -4.80251408 gen_loss 6.82350445\n",
            "disc_loss -4.72488785 gen_loss 6.09684086\n",
            "disc_loss -5.49106503 gen_loss 6.76198959\n",
            "disc_loss -4.27770519 gen_loss 5.82419872\n",
            "disc_loss -4.32258749 gen_loss 5.58841801\n",
            "disc_loss -4.62010098 gen_loss 5.92357111\n",
            "disc_loss -3.42290378 gen_loss 4.57621765\n",
            "disc_loss -4.28655481 gen_loss 5.06249714\n",
            "disc_loss -3.44604969 gen_loss 4.46456861\n",
            "disc_loss -3.22804046 gen_loss 3.96481013\n",
            "disc_loss -3.41920304 gen_loss 3.98795938\n",
            "disc_loss -2.75202036 gen_loss 3.30040479\n",
            "disc_loss -3.28518224 gen_loss 3.36541605\n",
            "disc_loss -3.47488928 gen_loss 3.58223581\n",
            "disc_loss -3.40399885 gen_loss 3.34704733\n",
            "disc_loss -3.27386737 gen_loss 2.95767355\n",
            "disc_loss -3.53830075 gen_loss 3.05736661\n",
            "disc_loss -2.9268651 gen_loss 2.56317353\n",
            "disc_loss -3.2313571 gen_loss 2.58801746\n",
            "disc_loss -2.52483416 gen_loss 1.85394633\n",
            "disc_loss -2.46328807 gen_loss 1.91629064\n",
            "disc_loss -3.05897307 gen_loss 2.03830123\n",
            "disc_loss -2.88302684 gen_loss 1.51713407\n",
            "disc_loss -3.24003744 gen_loss 1.96891427\n",
            "disc_loss -2.9404943 gen_loss 1.58786082\n",
            "disc_loss -2.10464048 gen_loss 1.22209477\n",
            "disc_loss -2.83926201 gen_loss 1.56548786\n",
            "disc_loss -2.78910112 gen_loss 1.11764264\n",
            "disc_loss -2.40440059 gen_loss 0.869256794\n",
            "disc_loss -1.96690536 gen_loss 0.537034154\n",
            "disc_loss -2.27137756 gen_loss 0.640384\n",
            "disc_loss -1.88971126 gen_loss 0.446668059\n",
            "disc_loss -2.3357985 gen_loss 0.644375563\n",
            "disc_loss -2.43751597 gen_loss 0.411480606\n",
            "disc_loss -1.86743796 gen_loss 0.23036173\n",
            "disc_loss -1.99273026 gen_loss 0.198482662\n",
            "disc_loss -2.17626739 gen_loss 0.482232153\n",
            "disc_loss -2.20986414 gen_loss 0.547029853\n",
            "disc_loss -2.215693 gen_loss 0.644062638\n",
            "disc_loss -1.72266221 gen_loss 0.439942777\n",
            "disc_loss -1.60401547 gen_loss 0.46126461\n",
            "disc_loss -1.50574172 gen_loss 0.442962468\n",
            "disc_loss -1.38840532 gen_loss 0.274323344\n",
            "disc_loss -1.65993571 gen_loss 0.773448944\n",
            "disc_loss -1.61310422 gen_loss 0.705598176\n",
            "disc_loss -1.26875484 gen_loss 0.470892817\n",
            "disc_loss -1.48984635 gen_loss 0.946705937\n",
            "disc_loss -1.42164683 gen_loss 1.06991065\n",
            "disc_loss -1.65231156 gen_loss 1.34852827\n",
            "disc_loss -1.88336062 gen_loss 1.43831623\n",
            "disc_loss -1.76043427 gen_loss 1.63427043\n",
            "disc_loss -1.40013969 gen_loss 1.48534894\n",
            "disc_loss -1.89983463 gen_loss 1.90089834\n",
            "disc_loss -1.71394813 gen_loss 1.99259448\n",
            "disc_loss -1.76756692 gen_loss 2.15434885\n",
            "disc_loss -1.70266414 gen_loss 2.30520988\n",
            "disc_loss -0.989437819 gen_loss 2.29309082\n",
            "disc_loss -1.31446087 gen_loss 2.55732822\n",
            "disc_loss -0.940121353 gen_loss 2.602422\n",
            "disc_loss -0.971011281 gen_loss 2.36268759\n",
            "disc_loss -1.09380805 gen_loss 2.41413903\n",
            "disc_loss -1.36272085 gen_loss 2.35636926\n",
            "disc_loss -1.65117669 gen_loss 2.39151096\n",
            "disc_loss -1.25464022 gen_loss 2.02095842\n",
            "disc_loss -0.470892 gen_loss 1.96112025\n",
            "disc_loss -1.0201838 gen_loss 1.93447614\n",
            "disc_loss -1.59508157 gen_loss 1.62126195\n",
            "disc_loss -1.05177367 gen_loss 1.39731538\n",
            "disc_loss -0.817251146 gen_loss 1.20381665\n",
            "disc_loss -0.853866339 gen_loss 0.797129631\n",
            "disc_loss -1.09628725 gen_loss 0.732378364\n",
            "disc_loss -1.42592645 gen_loss 0.560144305\n",
            "disc_loss -1.48224592 gen_loss 0.58077991\n",
            "disc_loss -1.19702673 gen_loss 0.188030481\n",
            "disc_loss -1.31979966 gen_loss -0.09631937\n",
            "disc_loss -1.41547322 gen_loss -0.11665418\n",
            "disc_loss -1.67736506 gen_loss -0.462699741\n",
            "disc_loss -1.61465919 gen_loss -0.757050872\n",
            "disc_loss -1.41801071 gen_loss -0.899669\n",
            "disc_loss -0.862175 gen_loss -0.987767577\n",
            "disc_loss -0.803935409 gen_loss -1.31894994\n",
            "disc_loss -0.887885034 gen_loss -1.4346596\n",
            "disc_loss -0.673890233 gen_loss -1.53931296\n",
            "disc_loss -0.813181281 gen_loss -1.5745188\n",
            "disc_loss -0.705710828 gen_loss -1.65458584\n",
            "disc_loss -0.447896183 gen_loss -1.60317326\n",
            "disc_loss -0.973676145 gen_loss -1.52657473\n",
            "disc_loss -0.979075193 gen_loss -1.52039075\n",
            "disc_loss -1.20394707 gen_loss -1.4444803\n",
            "disc_loss -0.731554031 gen_loss -1.44952619\n",
            "disc_loss -0.841578901 gen_loss -1.36032486\n",
            "disc_loss -0.886291325 gen_loss -1.31617856\n",
            "disc_loss -0.920767307 gen_loss -1.20045531\n",
            "disc_loss -0.960271239 gen_loss -1.02598417\n",
            "disc_loss -1.15101027 gen_loss -0.941896737\n",
            "disc_loss -0.981264532 gen_loss -0.837706447\n",
            "disc_loss -1.39236248 gen_loss -0.731108665\n",
            "disc_loss -0.775621653 gen_loss -0.554945588\n",
            "disc_loss -0.959051251 gen_loss -0.35300833\n",
            "disc_loss -1.10715866 gen_loss -0.255874783\n",
            "disc_loss -1.12826979 gen_loss -0.103363082\n",
            "disc_loss -0.909138262 gen_loss -0.000628167414\n",
            "disc_loss -1.1726439 gen_loss 0.233249813\n",
            "disc_loss -0.978744328 gen_loss 0.422984451\n",
            "disc_loss -1.07227385 gen_loss 0.623934865\n",
            "disc_loss -1.37215924 gen_loss 0.787415266\n",
            "disc_loss -1.26604855 gen_loss 0.958734632\n",
            "disc_loss -1.34403586 gen_loss 1.1157887\n",
            "disc_loss -1.26036084 gen_loss 1.29451966\n",
            "disc_loss -1.28153825 gen_loss 1.46178746\n",
            "disc_loss -1.33002269 gen_loss 1.70842326\n",
            "disc_loss -1.28527689 gen_loss 1.86164474\n",
            "disc_loss -1.36424482 gen_loss 2.05227423\n",
            "disc_loss -1.22294199 gen_loss 2.21240401\n",
            "disc_loss -1.49832952 gen_loss 2.32300758\n",
            "disc_loss -1.70552468 gen_loss 2.49890661\n",
            "disc_loss -1.34334993 gen_loss 2.57811499\n",
            "disc_loss -1.75435162 gen_loss 2.70372033\n",
            "disc_loss -1.82380831 gen_loss 2.84081507\n",
            "disc_loss -2.07696462 gen_loss 2.98582\n",
            "disc_loss -1.29829299 gen_loss 2.97408271\n",
            "disc_loss -1.62541461 gen_loss 3.25532079\n",
            "disc_loss -1.07828331 gen_loss 3.22537923\n",
            "disc_loss -1.41844428 gen_loss 3.20157886\n",
            "disc_loss -1.10608149 gen_loss 3.26703453\n",
            "disc_loss -1.25369787 gen_loss 3.3308723\n",
            "disc_loss -1.43712056 gen_loss 3.40439987\n",
            "disc_loss -1.42968786 gen_loss 3.27683878\n",
            "disc_loss -1.72628725 gen_loss 3.34430265\n",
            "disc_loss -1.36476934 gen_loss 3.17783403\n",
            "disc_loss -1.30554497 gen_loss 3.21784902\n",
            "disc_loss -1.54164779 gen_loss 3.09292173\n",
            "disc_loss -1.06545341 gen_loss 3.18516731\n",
            "disc_loss -1.27314866 gen_loss 3.10591865\n",
            "disc_loss -0.904534936 gen_loss 2.96592522\n",
            "disc_loss -1.1625762 gen_loss 2.86663866\n",
            "disc_loss -1.02768028 gen_loss 2.79777336\n",
            "disc_loss -1.32694101 gen_loss 2.80294585\n",
            "disc_loss -0.715256453 gen_loss 2.57379484\n",
            "disc_loss -0.864207208 gen_loss 2.40585971\n",
            "disc_loss -1.22578657 gen_loss 2.31446409\n",
            "disc_loss -0.892487824 gen_loss 2.1318984\n",
            "disc_loss -1.16627431 gen_loss 1.96793103\n",
            "disc_loss -1.60769773 gen_loss 1.85013175\n",
            "disc_loss -1.14343786 gen_loss 1.74343526\n",
            "disc_loss -1.02501333 gen_loss 1.56530666\n",
            "disc_loss -1.62300909 gen_loss 1.43937659\n",
            "disc_loss -1.36040187 gen_loss 1.30455863\n",
            "disc_loss -1.33580339 gen_loss 1.22467232\n",
            "disc_loss -1.52237225 gen_loss 1.10468531\n",
            "disc_loss -1.39688468 gen_loss 0.987889588\n",
            "disc_loss -1.24966037 gen_loss 0.956974208\n",
            "disc_loss -1.65802252 gen_loss 0.84751004\n",
            "disc_loss -1.15016222 gen_loss 0.83912164\n",
            "disc_loss -1.1550411 gen_loss 0.730693698\n",
            "disc_loss -1.56306279 gen_loss 0.717641294\n",
            "disc_loss -1.02711833 gen_loss 0.675948441\n",
            "disc_loss -1.23531663 gen_loss 0.631745398\n",
            "disc_loss -1.23921752 gen_loss 0.704841316\n",
            "disc_loss -1.60119569 gen_loss 0.669638336\n",
            "disc_loss -1.45689583 gen_loss 0.604942\n",
            "disc_loss -0.827461481 gen_loss 0.616086364\n",
            "disc_loss -1.38058186 gen_loss 0.636172175\n",
            "disc_loss -1.64070618 gen_loss 0.576582611\n",
            "disc_loss -1.42811239 gen_loss 0.550762355\n",
            "disc_loss -1.40842092 gen_loss 0.498675823\n",
            "disc_loss -1.31378436 gen_loss 0.458390325\n",
            "disc_loss -1.47599459 gen_loss 0.45704326\n",
            "disc_loss -1.25167501 gen_loss 0.465455055\n",
            "disc_loss -1.35251939 gen_loss 0.35218963\n",
            "disc_loss -1.59615493 gen_loss 0.387037039\n",
            "disc_loss -1.03227973 gen_loss 0.437309831\n",
            "disc_loss -0.555110216 gen_loss 0.413297951\n",
            "disc_loss -0.80948782 gen_loss 0.383159548\n",
            "disc_loss -0.680853 gen_loss 0.359587\n",
            "disc_loss -0.529497921 gen_loss 0.347263277\n",
            "disc_loss -0.61237824 gen_loss 0.34409219\n",
            "disc_loss -1.04484129 gen_loss 0.315183282\n",
            "disc_loss -0.852845728 gen_loss 0.375557482\n",
            "disc_loss -0.854575217 gen_loss 0.386272401\n",
            "disc_loss -0.539471507 gen_loss 0.36496377\n",
            "disc_loss -0.972439349 gen_loss 0.386454284\n",
            "disc_loss -0.873362303 gen_loss 0.452181756\n",
            "disc_loss -0.942836285 gen_loss 0.477515638\n",
            "disc_loss -0.979277432 gen_loss 0.456726938\n",
            "disc_loss -1.05891812 gen_loss 0.4950912\n",
            "disc_loss -1.25562191 gen_loss 0.555263817\n",
            "disc_loss -0.747820854 gen_loss 0.434250832\n",
            "disc_loss -1.0551486 gen_loss 0.468047559\n",
            "disc_loss -1.0853 gen_loss 0.39936173\n",
            "disc_loss -1.1850971 gen_loss 0.413001537\n",
            "disc_loss -1.18039751 gen_loss 0.421015114\n",
            "disc_loss -0.934190392 gen_loss 0.419319302\n",
            "disc_loss -1.25856435 gen_loss 0.381359726\n",
            "disc_loss -1.17173481 gen_loss 0.434574753\n",
            "disc_loss -1.27206087 gen_loss 0.388706505\n",
            "disc_loss -1.43619823 gen_loss 0.429804325\n",
            "disc_loss -1.43949759 gen_loss 0.494664282\n",
            "disc_loss -1.29495311 gen_loss 0.501336157\n",
            "disc_loss -1.3237437 gen_loss 0.480823129\n",
            "disc_loss -1.63434923 gen_loss 0.51873225\n",
            "disc_loss -1.56851518 gen_loss 0.558409393\n",
            "disc_loss -1.59286487 gen_loss 0.637846529\n",
            "disc_loss -1.37842131 gen_loss 0.549774289\n",
            "disc_loss -1.55663431 gen_loss 0.591776073\n",
            "disc_loss -1.14485276 gen_loss 0.53096503\n",
            "disc_loss -1.36419606 gen_loss 0.539964616\n",
            "disc_loss -1.21896148 gen_loss 0.460662454\n",
            "disc_loss -1.63153815 gen_loss 0.615848422\n",
            "disc_loss -1.20158231 gen_loss 0.470704556\n",
            "disc_loss -1.51609838 gen_loss 0.521833658\n",
            "disc_loss -1.61140716 gen_loss 0.597825468\n",
            "disc_loss -1.50298357 gen_loss 0.662651658\n",
            "disc_loss -1.02402699 gen_loss 0.62872839\n",
            "disc_loss -1.55011022 gen_loss 0.711723447\n",
            "disc_loss -1.69996941 gen_loss 0.8793\n",
            "disc_loss -1.97494435 gen_loss 0.796010256\n",
            "disc_loss -1.73757887 gen_loss 0.792325199\n",
            "disc_loss -1.78571451 gen_loss 0.86574477\n",
            "disc_loss -1.81659806 gen_loss 0.92830503\n",
            "disc_loss -1.33504593 gen_loss 0.901679814\n",
            "disc_loss -1.84157813 gen_loss 0.929232121\n",
            "disc_loss -1.87270629 gen_loss 0.888965786\n",
            "disc_loss -1.55239069 gen_loss 1.01392329\n",
            "disc_loss -1.66579449 gen_loss 0.87906152\n",
            "disc_loss -2.3503058 gen_loss 0.940203667\n",
            "disc_loss -1.74843609 gen_loss 0.876511216\n",
            "disc_loss -1.56950295 gen_loss 0.843573272\n",
            "disc_loss -1.21019626 gen_loss 0.703127\n",
            "disc_loss -1.79986477 gen_loss 0.857077897\n",
            "disc_loss -1.75933814 gen_loss 0.857723713\n",
            "disc_loss -1.51907849 gen_loss 0.83166492\n",
            "disc_loss -1.60894954 gen_loss 0.799560189\n",
            "disc_loss -1.22629356 gen_loss 0.757548153\n",
            "disc_loss -1.4640404 gen_loss 0.819797516\n",
            "disc_loss -1.49731278 gen_loss 0.921507\n",
            "disc_loss -1.63346982 gen_loss 0.969969153\n",
            "disc_loss -1.54807401 gen_loss 1.00876212\n",
            "disc_loss -1.5242281 gen_loss 1.02668595\n",
            "disc_loss -1.54824328 gen_loss 1.0853188\n",
            "disc_loss -1.34260058 gen_loss 1.07337666\n",
            "disc_loss -1.40750098 gen_loss 1.09917271\n",
            "disc_loss -1.32265019 gen_loss 1.13033891\n",
            "disc_loss -1.28370523 gen_loss 1.11948609\n",
            "disc_loss -1.71227098 gen_loss 1.13076115\n",
            "disc_loss -1.25025725 gen_loss 1.12646735\n",
            "disc_loss -1.54448891 gen_loss 1.17392\n",
            "disc_loss -1.45549524 gen_loss 1.16851461\n",
            "disc_loss -1.32407832 gen_loss 1.27143407\n",
            "disc_loss -1.51700068 gen_loss 1.25788164\n",
            "disc_loss -1.38222146 gen_loss 1.27842748\n",
            "disc_loss -1.61551428 gen_loss 1.37148154\n",
            "disc_loss -1.93025541 gen_loss 1.39478445\n",
            "disc_loss -1.84973705 gen_loss 1.46406651\n",
            "disc_loss -1.7302562 gen_loss 1.46648\n",
            "disc_loss -1.50341344 gen_loss 1.47004223\n",
            "disc_loss -1.56469798 gen_loss 1.47129059\n",
            "disc_loss -1.89101624 gen_loss 1.57958448\n",
            "disc_loss -1.56810749 gen_loss 1.47506118\n",
            "disc_loss -2.03354502 gen_loss 1.58164966\n",
            "disc_loss -2.06368 gen_loss 1.60659945\n",
            "disc_loss -2.13085556 gen_loss 1.70520246\n",
            "disc_loss -1.35811687 gen_loss 1.48540115\n",
            "disc_loss -1.76860726 gen_loss 1.76983094\n",
            "disc_loss -2.00282 gen_loss 1.66859531\n",
            "disc_loss -2.28697729 gen_loss 1.61128235\n",
            "disc_loss -1.94750798 gen_loss 1.66659856\n",
            "disc_loss -2.18936086 gen_loss 1.64429665\n",
            "disc_loss -1.77132225 gen_loss 1.57824159\n",
            "disc_loss -1.99143612 gen_loss 1.5529778\n",
            "disc_loss -1.92905211 gen_loss 1.52941859\n",
            "disc_loss -1.37651682 gen_loss 1.59601152\n",
            "disc_loss -2.26793242 gen_loss 1.45568156\n",
            "disc_loss -2.35350037 gen_loss 1.40653586\n",
            "disc_loss -2.05296803 gen_loss 1.44834924\n",
            "disc_loss -2.04390407 gen_loss 1.37253129\n",
            "disc_loss -1.64299953 gen_loss 1.38886726\n",
            "disc_loss -1.59897721 gen_loss 1.31949461\n",
            "disc_loss -1.75077212 gen_loss 1.29221642\n",
            "disc_loss -1.93603659 gen_loss 1.31537616\n",
            "disc_loss -2.27973962 gen_loss 1.22383571\n",
            "disc_loss -2.0550108 gen_loss 1.2858566\n",
            "disc_loss -1.7201 gen_loss 1.30655706\n",
            "disc_loss -1.84076297 gen_loss 1.30105412\n",
            "disc_loss -2.23839331 gen_loss 1.27672124\n",
            "disc_loss -2.27604866 gen_loss 1.3249675\n",
            "disc_loss -2.07359052 gen_loss 1.30076492\n",
            "disc_loss -1.82302392 gen_loss 1.26446915\n",
            "disc_loss -1.56335378 gen_loss 1.29692614\n",
            "disc_loss -2.13302588 gen_loss 1.29267049\n",
            "disc_loss -1.58935153 gen_loss 1.22958732\n",
            "disc_loss -1.80437136 gen_loss 1.32023382\n",
            "disc_loss -1.84204173 gen_loss 1.2157613\n",
            "disc_loss -1.55023444 gen_loss 1.16395152\n",
            "disc_loss -1.93558121 gen_loss 1.23140109\n",
            "disc_loss -1.55237663 gen_loss 1.12608933\n",
            "disc_loss -1.69696593 gen_loss 1.14620686\n",
            "disc_loss -1.54008985 gen_loss 1.03165245\n",
            "disc_loss -2.16365385 gen_loss 1.09353864\n",
            "disc_loss -1.770594 gen_loss 1.01738143\n",
            "disc_loss -1.89866698 gen_loss 0.889259696\n",
            "disc_loss -1.33005476 gen_loss 0.82358849\n",
            "disc_loss -1.43580699 gen_loss 0.610946298\n",
            "disc_loss -1.40681791 gen_loss 0.601887107\n",
            "disc_loss -1.34840751 gen_loss 0.484001637\n",
            "disc_loss -1.74268222 gen_loss 0.571016\n",
            "disc_loss -1.2735467 gen_loss 0.437736511\n",
            "disc_loss -2.04565525 gen_loss 0.41948089\n",
            "disc_loss -2.07527018 gen_loss 0.309787095\n",
            "disc_loss -1.89641321 gen_loss 0.566432118\n",
            "disc_loss -1.8212446 gen_loss 0.405189902\n",
            "disc_loss -2.03393555 gen_loss 0.497259945\n",
            "disc_loss -2.39965296 gen_loss 0.631364\n",
            "disc_loss -2.17293978 gen_loss 0.511409581\n",
            "disc_loss -2.33804536 gen_loss 0.644582272\n",
            "disc_loss -2.8411305 gen_loss 0.711409\n",
            "disc_loss -1.99546587 gen_loss 0.712762833\n",
            "disc_loss -1.85793054 gen_loss 0.663553655\n",
            "disc_loss -2.05506182 gen_loss 0.858150363\n",
            "disc_loss -1.96114266 gen_loss 0.690883338\n",
            "disc_loss -1.63904357 gen_loss 0.888555408\n",
            "disc_loss -1.80187321 gen_loss 1.1001457\n",
            "disc_loss -1.75644743 gen_loss 1.10219705\n",
            "disc_loss -1.74849927 gen_loss 0.997193456\n",
            "disc_loss -1.6598208 gen_loss 1.15867758\n",
            "disc_loss -2.14697313 gen_loss 1.25633872\n",
            "disc_loss -1.83037353 gen_loss 1.15489399\n",
            "disc_loss -2.03244042 gen_loss 1.30876374\n",
            "disc_loss -1.95679736 gen_loss 1.33423769\n",
            "disc_loss -1.17720842 gen_loss 1.34935451\n",
            "disc_loss -1.74664569 gen_loss 1.40881753\n",
            "disc_loss -2.18163514 gen_loss 1.46781373\n",
            "disc_loss -1.71690392 gen_loss 1.46632719\n",
            "disc_loss -2.05795598 gen_loss 1.42701125\n",
            "disc_loss -1.68491125 gen_loss 1.45336318\n",
            "disc_loss -1.5077076 gen_loss 1.4419086\n",
            "disc_loss -1.78635 gen_loss 1.37712884\n",
            "disc_loss -2.20970368 gen_loss 1.35032487\n",
            "disc_loss -2.2086432 gen_loss 1.24539888\n",
            "disc_loss -2.12614369 gen_loss 1.31977272\n",
            "disc_loss -1.8463515 gen_loss 1.24435115\n",
            "disc_loss -1.39028704 gen_loss 1.31354928\n",
            "disc_loss -1.84798551 gen_loss 1.21975017\n",
            "disc_loss -1.1869185 gen_loss 1.12695181\n",
            "disc_loss -1.92207503 gen_loss 1.07021749\n",
            "disc_loss -1.95557654 gen_loss 1.05413008\n",
            "disc_loss -2.1152308 gen_loss 1.10000646\n",
            "disc_loss -1.40798914 gen_loss 0.979594231\n",
            "disc_loss -2.13673615 gen_loss 0.949817777\n",
            "disc_loss -1.48783147 gen_loss 0.965592563\n",
            "disc_loss -2.33368635 gen_loss 0.880764484\n",
            "disc_loss -1.84491134 gen_loss 0.810382068\n",
            "disc_loss -1.7049185 gen_loss 0.865761399\n",
            "disc_loss -1.91442716 gen_loss 0.874745846\n",
            "disc_loss -2.34100699 gen_loss 0.976517677\n",
            "disc_loss -1.59429824 gen_loss 0.928340793\n",
            "disc_loss -1.60562825 gen_loss 0.802978337\n",
            "disc_loss -1.64977705 gen_loss 0.78750968\n",
            "disc_loss -2.38889146 gen_loss 0.918773293\n",
            "disc_loss -2.18377018 gen_loss 0.948003471\n",
            "disc_loss -2.00380421 gen_loss 0.908978343\n",
            "disc_loss -1.34073055 gen_loss 0.78747189\n",
            "disc_loss -1.8614856 gen_loss 0.816846192\n",
            "disc_loss -1.83587289 gen_loss 0.758375347\n",
            "disc_loss -1.80620253 gen_loss 0.67355597\n",
            "disc_loss -1.63365078 gen_loss 0.755343795\n",
            "disc_loss -2.23720789 gen_loss 0.677838385\n",
            "disc_loss -1.91812372 gen_loss 0.631244302\n",
            "disc_loss -1.77891421 gen_loss 0.54467696\n",
            "disc_loss -2.27737784 gen_loss 0.519292295\n",
            "disc_loss -1.3988272 gen_loss 0.463076353\n",
            "disc_loss -1.37886655 gen_loss 0.398107469\n",
            "disc_loss -2.3909893 gen_loss 0.507775\n",
            "disc_loss -2.0280633 gen_loss 0.599304795\n",
            "disc_loss -1.48171186 gen_loss 0.47283268\n",
            "disc_loss -2.09880781 gen_loss 0.536957264\n",
            "disc_loss -1.87398458 gen_loss 0.481144607\n",
            "disc_loss -2.24622774 gen_loss 0.561762929\n",
            "disc_loss -2.14897704 gen_loss 0.456342876\n",
            "disc_loss -2.10845113 gen_loss 0.447824091\n",
            "disc_loss -1.79004705 gen_loss 0.581088841\n",
            "disc_loss -1.63710189 gen_loss 0.548106\n",
            "disc_loss -1.77194321 gen_loss 0.514707208\n",
            "disc_loss -2.0564754 gen_loss 0.707237303\n",
            "disc_loss -1.72516191 gen_loss 0.683985114\n",
            "disc_loss -1.81467938 gen_loss 0.748068571\n",
            "disc_loss -2.02439523 gen_loss 0.639795899\n",
            "disc_loss -1.81504512 gen_loss 0.605520308\n",
            "disc_loss -1.71669674 gen_loss 0.465506494\n",
            "disc_loss -1.52710211 gen_loss 0.612561524\n",
            "disc_loss -1.82842195 gen_loss 0.553388953\n",
            "disc_loss -1.98553479 gen_loss 0.373482645\n",
            "disc_loss -1.5348208 gen_loss 0.421035856\n",
            "disc_loss -2.31146669 gen_loss 0.278575778\n",
            "disc_loss -2.44035292 gen_loss 0.193864897\n",
            "disc_loss -2.60486555 gen_loss 0.247128397\n",
            "disc_loss -1.88275719 gen_loss 0.0499567538\n",
            "disc_loss -1.9815042 gen_loss 0.00969245471\n",
            "disc_loss -2.95055914 gen_loss 0.27000013\n",
            "disc_loss -2.1017592 gen_loss 0.218328863\n",
            "disc_loss -2.84485459 gen_loss 0.017187912\n",
            "disc_loss -1.90580404 gen_loss 0.0872481912\n",
            "disc_loss -1.88181472 gen_loss -0.218522981\n",
            "disc_loss -3.05731535 gen_loss -0.221125633\n",
            "disc_loss -2.49504042 gen_loss -0.550757408\n",
            "disc_loss -1.87811255 gen_loss -0.55022037\n",
            "disc_loss -3.32085729 gen_loss -0.281786293\n",
            "disc_loss -2.58661222 gen_loss -0.290653884\n",
            "disc_loss -2.02009583 gen_loss -0.268948644\n",
            "disc_loss -1.81656146 gen_loss -0.551658809\n",
            "disc_loss -2.58563352 gen_loss -0.605944693\n",
            "disc_loss -2.06772 gen_loss -0.63385129\n",
            "disc_loss -2.86351418 gen_loss -0.268112242\n",
            "disc_loss -2.14571571 gen_loss -0.454402268\n",
            "disc_loss -2.27632141 gen_loss -0.606409848\n",
            "disc_loss -2.09059715 gen_loss -0.679982781\n",
            "disc_loss -2.18997741 gen_loss -0.516122103\n",
            "disc_loss -1.56658912 gen_loss -0.927851319\n",
            "disc_loss -1.92610741 gen_loss -0.313107312\n",
            "disc_loss -1.6689384 gen_loss -0.44438988\n",
            "disc_loss -1.81088793 gen_loss -0.535771\n",
            "disc_loss -2.95398927 gen_loss -0.152178973\n",
            "disc_loss -2.3002 gen_loss -0.227790758\n",
            "disc_loss -3.20524526 gen_loss 0.0703854859\n",
            "disc_loss -2.81041932 gen_loss -0.133628696\n",
            "disc_loss -3.22265196 gen_loss 0.0899517313\n",
            "disc_loss -2.67631221 gen_loss 0.118606761\n",
            "disc_loss -3.95077 gen_loss 0.251888335\n",
            "disc_loss -2.56895447 gen_loss 0.379715353\n",
            "disc_loss -1.49543738 gen_loss 0.128163308\n",
            "disc_loss -1.58836162 gen_loss 0.240596503\n",
            "disc_loss -1.53121161 gen_loss 0.364707589\n",
            "disc_loss -0.780235112 gen_loss 0.150638312\n",
            "disc_loss -1.4563061 gen_loss 0.0283091255\n",
            "disc_loss -1.45840836 gen_loss 0.588367879\n",
            "disc_loss -1.91583371 gen_loss 0.614920616\n",
            "disc_loss -1.16329336 gen_loss 0.627715945\n",
            "disc_loss -1.25995517 gen_loss 0.473092616\n",
            "disc_loss -1.926162 gen_loss 0.880212426\n",
            "disc_loss -1.88757944 gen_loss 1.15510523\n",
            "disc_loss -1.75963676 gen_loss 1.11344397\n",
            "disc_loss -1.16088927 gen_loss 1.04425395\n",
            "disc_loss -2.20451069 gen_loss 1.32029569\n",
            "disc_loss -2.01759768 gen_loss 1.23399615\n",
            "disc_loss -1.82664704 gen_loss 1.27931356\n",
            "disc_loss -2.08743596 gen_loss 1.35339379\n",
            "disc_loss -1.58559763 gen_loss 1.59312475\n",
            "disc_loss -2.305686 gen_loss 1.75686336\n",
            "disc_loss -2.05849481 gen_loss 1.48463225\n",
            "disc_loss -1.71558523 gen_loss 1.40187132\n",
            "disc_loss -1.8214978 gen_loss 1.21496773\n",
            "disc_loss -1.6256218 gen_loss 1.4982233\n",
            "disc_loss -1.92643368 gen_loss 1.29064012\n",
            "disc_loss -1.35964227 gen_loss 1.32978189\n",
            "disc_loss -2.5611012 gen_loss 1.58257079\n",
            "disc_loss -2.30215788 gen_loss 1.69483876\n",
            "disc_loss -2.52145 gen_loss 1.43489647\n",
            "disc_loss -1.73963368 gen_loss 1.37957728\n",
            "disc_loss -1.95379817 gen_loss 1.29195142\n",
            "disc_loss -2.44322205 gen_loss 1.48230207\n",
            "disc_loss -2.23581815 gen_loss 1.19681\n",
            "disc_loss -2.48759055 gen_loss 1.09580636\n",
            "disc_loss -2.75763345 gen_loss 1.52905369\n",
            "disc_loss -2.74770093 gen_loss 1.39961231\n",
            "disc_loss -2.50862169 gen_loss 1.28576016\n",
            "disc_loss -2.28654552 gen_loss 1.32459605\n",
            "disc_loss -2.1988647 gen_loss 1.00833154\n",
            "disc_loss -2.36427498 gen_loss 0.833715558\n",
            "disc_loss -2.20378518 gen_loss 1.40104747\n",
            "disc_loss -1.47119784 gen_loss 0.940986454\n",
            "disc_loss -1.72518814 gen_loss 1.11537683\n",
            "disc_loss -1.72879982 gen_loss 0.980444133\n",
            "disc_loss -2.04989982 gen_loss 0.906462789\n",
            "disc_loss -1.71902263 gen_loss 0.848011851\n",
            "disc_loss -1.83576941 gen_loss 0.90006\n",
            "disc_loss -1.06060934 gen_loss 0.35572046\n",
            "disc_loss -1.29021597 gen_loss 0.769072831\n",
            "disc_loss -2.09828877 gen_loss 0.508634567\n",
            "disc_loss -1.71474397 gen_loss 0.0979785919\n",
            "disc_loss -1.53770411 gen_loss 0.395717889\n",
            "disc_loss -1.80686915 gen_loss 0.405950636\n",
            "disc_loss -2.09288311 gen_loss 0.138233334\n",
            "disc_loss -2.01512885 gen_loss -0.0594546124\n",
            "disc_loss -1.78941929 gen_loss 0.0683617815\n",
            "disc_loss -1.92481303 gen_loss -0.0418773182\n",
            "disc_loss -2.22284245 gen_loss -0.364882708\n",
            "disc_loss -2.26551867 gen_loss -0.113770708\n",
            "disc_loss -1.22357 gen_loss -0.593524873\n",
            "disc_loss -1.79532993 gen_loss -0.420079887\n",
            "disc_loss -1.71529865 gen_loss -0.632535577\n",
            "disc_loss -1.38866591 gen_loss -0.62546885\n",
            "disc_loss -1.87026989 gen_loss -0.657190144\n",
            "disc_loss -1.27446413 gen_loss -0.819813073\n",
            "disc_loss -1.94622481 gen_loss -0.860469639\n",
            "disc_loss -2.02944016 gen_loss -1.01964509\n",
            "disc_loss -1.9914676 gen_loss -0.93071878\n",
            "disc_loss -2.07510161 gen_loss -1.01040745\n",
            "disc_loss -1.90462089 gen_loss -0.974419713\n",
            "disc_loss -1.56215906 gen_loss -1.0810684\n",
            "disc_loss -1.02618074 gen_loss -1.25382113\n",
            "disc_loss -1.69953799 gen_loss -1.16992795\n",
            "disc_loss -1.67140257 gen_loss -1.0617125\n",
            "disc_loss -1.10620248 gen_loss -1.3154552\n",
            "disc_loss -1.54442644 gen_loss -1.12527823\n",
            "disc_loss -1.34945273 gen_loss -1.41827524\n",
            "disc_loss -0.623373508 gen_loss -1.20656991\n",
            "disc_loss -0.89515835 gen_loss -1.24245811\n",
            "disc_loss -1.66810012 gen_loss -1.20070493\n",
            "disc_loss -0.830166101 gen_loss -1.1951704\n",
            "disc_loss -1.45638752 gen_loss -0.716923177\n",
            "disc_loss -1.175632 gen_loss -0.838971257\n",
            "disc_loss -0.784944 gen_loss -0.922822595\n",
            "disc_loss -1.60972106 gen_loss -0.58995229\n",
            "disc_loss -0.87810266 gen_loss -0.526770175\n",
            "disc_loss -0.972670317 gen_loss -0.468700588\n",
            "disc_loss -0.643324614 gen_loss -0.928089738\n",
            "disc_loss -0.839436829 gen_loss -0.691645622\n",
            "disc_loss -1.76010501 gen_loss -0.201766536\n",
            "disc_loss -1.33536184 gen_loss -0.546495318\n",
            "disc_loss -1.36510682 gen_loss -0.344950914\n",
            "disc_loss -1.40105152 gen_loss -0.135764524\n",
            "disc_loss -1.01371562 gen_loss -0.0652782768\n",
            "disc_loss -0.954605877 gen_loss 0.117860578\n",
            "disc_loss -1.64520884 gen_loss 0.242031723\n",
            "disc_loss -1.73640442 gen_loss 0.426587343\n",
            "disc_loss -1.6376 gen_loss 0.48107034\n",
            "disc_loss -1.04311121 gen_loss 0.536349714\n",
            "disc_loss -2.07969022 gen_loss 0.736988366\n",
            "disc_loss -1.65510762 gen_loss 0.730501\n",
            "disc_loss -1.09452951 gen_loss 0.682013333\n",
            "disc_loss -1.38552797 gen_loss 0.917731464\n",
            "disc_loss -1.68424225 gen_loss 1.18471694\n",
            "disc_loss -2.2990694 gen_loss 1.23460853\n",
            "disc_loss -2.57661724 gen_loss 1.43540156\n",
            "disc_loss -1.60807717 gen_loss 1.2881062\n",
            "disc_loss -0.932864666 gen_loss 0.952802479\n",
            "disc_loss -1.5180763 gen_loss 1.44469881\n",
            "disc_loss -1.94220316 gen_loss 1.42571771\n",
            "disc_loss -1.27894855 gen_loss 1.18126774\n",
            "disc_loss -1.9525665 gen_loss 1.53690398\n",
            "disc_loss -1.87626147 gen_loss 1.78070927\n",
            "disc_loss -1.80592561 gen_loss 1.31521332\n",
            "disc_loss -1.58006239 gen_loss 1.10465682\n",
            "disc_loss -2.52944446 gen_loss 1.40465426\n",
            "disc_loss -2.01437068 gen_loss 1.58420384\n",
            "disc_loss -2.21900535 gen_loss 1.43278\n",
            "disc_loss -2.27655149 gen_loss 1.16416919\n",
            "disc_loss -1.68828082 gen_loss 1.19058847\n",
            "disc_loss -2.31461096 gen_loss 1.42793405\n",
            "disc_loss -2.03059888 gen_loss 1.26990569\n",
            "disc_loss -2.52881289 gen_loss 0.996682346\n",
            "disc_loss -2.01480556 gen_loss 0.981380105\n",
            "disc_loss -3.00607681 gen_loss 1.23634017\n",
            "disc_loss -2.79970479 gen_loss 0.920185745\n",
            "disc_loss -2.72536778 gen_loss 0.857639\n",
            "disc_loss -2.42292452 gen_loss 0.951145768\n",
            "disc_loss -1.77019334 gen_loss 0.681802273\n",
            "disc_loss -2.13892293 gen_loss 0.672118366\n",
            "disc_loss -2.25504494 gen_loss 0.557794929\n",
            "disc_loss -2.59352326 gen_loss 0.569487\n",
            "disc_loss -1.3007158 gen_loss 0.172835454\n",
            "disc_loss -2.30326343 gen_loss 0.514849544\n",
            "disc_loss -1.62918723 gen_loss 0.380455464\n",
            "disc_loss -2.36985564 gen_loss 0.477137178\n",
            "disc_loss -1.88060808 gen_loss 0.438731134\n",
            "disc_loss -1.15174973 gen_loss 0.299966276\n",
            "disc_loss -2.56450033 gen_loss 0.200170711\n",
            "disc_loss -1.28555512 gen_loss 0.251119584\n",
            "disc_loss -2.40847611 gen_loss 0.194415525\n",
            "disc_loss -2.18998623 gen_loss 0.270721585\n",
            "disc_loss -1.79136014 gen_loss 0.233237267\n",
            "disc_loss -0.769263 gen_loss 0.0132288039\n",
            "disc_loss -1.81000006 gen_loss 0.177512765\n",
            "disc_loss -1.73037601 gen_loss 0.133554742\n",
            "disc_loss -1.63084173 gen_loss 0.194355711\n",
            "disc_loss -2.46708417 gen_loss 0.147007197\n",
            "disc_loss -2.19867086 gen_loss 0.234806344\n",
            "disc_loss -2.42992735 gen_loss 0.122181296\n",
            "disc_loss -1.66160464 gen_loss 0.00239155302\n",
            "disc_loss -2.12927365 gen_loss 0.281996906\n",
            "disc_loss -2.17414522 gen_loss 0.0605318174\n",
            "disc_loss -1.97293806 gen_loss 0.171813637\n",
            "disc_loss -1.99981391 gen_loss 0.0951675922\n",
            "disc_loss -1.24183726 gen_loss -0.00782385468\n",
            "disc_loss -1.81141055 gen_loss -0.268992513\n",
            "disc_loss -1.41571963 gen_loss -0.100916266\n",
            "disc_loss -1.3072834 gen_loss -0.157033682\n",
            "disc_loss -0.786607504 gen_loss 0.0698255152\n",
            "disc_loss -1.17532289 gen_loss -0.433842331\n",
            "disc_loss -2.53620958 gen_loss -0.154745504\n",
            "disc_loss -1.40956748 gen_loss 0.212208316\n",
            "disc_loss -1.53846014 gen_loss 0.112321541\n",
            "disc_loss -1.5658226 gen_loss 0.2191706\n",
            "disc_loss -1.3643558 gen_loss 0.157443255\n",
            "disc_loss -1.52106905 gen_loss 0.172114879\n",
            "disc_loss -1.59081459 gen_loss 0.480355114\n",
            "disc_loss -1.83280921 gen_loss 0.535657406\n",
            "disc_loss -1.32902813 gen_loss 0.501525402\n",
            "disc_loss -1.54388547 gen_loss 0.469497383\n",
            "disc_loss -1.530913 gen_loss 0.372380406\n",
            "disc_loss -1.22726858 gen_loss 0.546161532\n",
            "disc_loss -1.33833849 gen_loss 0.434544384\n",
            "disc_loss -1.37914455 gen_loss 0.568983197\n",
            "disc_loss -1.54893601 gen_loss 0.713827252\n",
            "disc_loss -1.3750391 gen_loss 0.600595832\n",
            "disc_loss -1.43214715 gen_loss 0.914408863\n",
            "disc_loss -1.76548052 gen_loss 0.823103905\n",
            "disc_loss -0.975991488 gen_loss 0.892106414\n",
            "disc_loss -1.77248657 gen_loss 1.0603174\n",
            "disc_loss -1.76331735 gen_loss 0.95179379\n",
            "disc_loss -1.28027415 gen_loss 0.987133324\n",
            "disc_loss -1.74985218 gen_loss 1.44856012\n",
            "disc_loss -1.77583778 gen_loss 1.34552407\n",
            "disc_loss -2.06282067 gen_loss 1.09076715\n",
            "disc_loss -1.79675603 gen_loss 1.75575948\n",
            "disc_loss -1.78321517 gen_loss 1.28674674\n",
            "disc_loss -2.73046017 gen_loss 1.71647704\n",
            "disc_loss -1.6385802 gen_loss 1.53288496\n",
            "disc_loss -1.45810246 gen_loss 1.39618719\n",
            "disc_loss -2.19890285 gen_loss 1.81595767\n",
            "disc_loss -2.22543263 gen_loss 1.81691396\n",
            "disc_loss -2.10956049 gen_loss 1.65113711\n",
            "disc_loss -2.1898737 gen_loss 1.69949317\n",
            "disc_loss -1.45271981 gen_loss 1.29025197\n",
            "disc_loss -2.24830055 gen_loss 2.1096282\n",
            "disc_loss -1.34568858 gen_loss 1.74203515\n",
            "disc_loss -2.23318934 gen_loss 1.68696952\n",
            "disc_loss -1.63845444 gen_loss 1.91641581\n",
            "disc_loss -1.89910185 gen_loss 1.63318086\n",
            "disc_loss -2.35528135 gen_loss 2.08872795\n",
            "disc_loss -1.26597452 gen_loss 1.85215819\n",
            "disc_loss -1.93260682 gen_loss 1.81930804\n",
            "disc_loss -1.62562776 gen_loss 1.55828929\n",
            "disc_loss -2.58748412 gen_loss 1.85122561\n",
            "disc_loss -1.79129791 gen_loss 1.45405066\n",
            "disc_loss -0.718222141 gen_loss 1.29012465\n",
            "disc_loss -1.98925877 gen_loss 1.63267362\n",
            "disc_loss -2.74578571 gen_loss 1.786219\n",
            "disc_loss -1.93102396 gen_loss 1.45571589\n",
            "disc_loss -2.21557379 gen_loss 1.34758592\n",
            "disc_loss -2.24538374 gen_loss 1.29990888\n",
            "disc_loss -1.83446527 gen_loss 1.54275107\n",
            "disc_loss -2.17179346 gen_loss 1.12853408\n",
            "disc_loss -2.77570152 gen_loss 1.21026444\n",
            "disc_loss -1.58245957 gen_loss 1.26392102\n",
            "disc_loss -1.90084851 gen_loss 0.848034561\n",
            "disc_loss -1.69850039 gen_loss 1.23688829\n",
            "disc_loss -1.80683434 gen_loss 1.16896105\n",
            "disc_loss -1.15724242 gen_loss 1.08016133\n",
            "disc_loss -1.85732186 gen_loss 1.03053391\n",
            "disc_loss -1.77319074 gen_loss 0.843615353\n",
            "disc_loss -2.30667019 gen_loss 0.93653965\n",
            "disc_loss -2.19533372 gen_loss 1.14068604\n",
            "disc_loss -2.21477389 gen_loss 0.926143289\n",
            "disc_loss -1.96966016 gen_loss 0.984402657\n",
            "disc_loss -1.8321594 gen_loss 0.960335076\n",
            "disc_loss -1.64604461 gen_loss 0.800269485\n",
            "disc_loss -1.70478868 gen_loss 0.762934625\n",
            "disc_loss -1.07756543 gen_loss 0.530207634\n",
            "disc_loss -2.29975653 gen_loss 0.684598148\n",
            "disc_loss -1.07723582 gen_loss 0.564636886\n",
            "disc_loss -1.6446476 gen_loss 0.598416924\n",
            "disc_loss -2.19164324 gen_loss 0.406984985\n",
            "disc_loss -1.78821993 gen_loss 0.470149338\n",
            "disc_loss -1.8198384 gen_loss 0.492795944\n",
            "disc_loss -1.24264956 gen_loss 0.176600531\n",
            "disc_loss -1.63817275 gen_loss 0.515792966\n",
            "disc_loss -1.54058182 gen_loss 0.323391467\n",
            "disc_loss -2.02006364 gen_loss 0.442611396\n",
            "disc_loss -0.504425824 gen_loss -0.146954149\n",
            "disc_loss -1.61635303 gen_loss 0.264729589\n",
            "disc_loss -1.84892857 gen_loss 0.160429806\n",
            "disc_loss -1.48483205 gen_loss 0.205614\n",
            "disc_loss -1.41049337 gen_loss 0.109333143\n",
            "disc_loss -1.42670202 gen_loss 0.298885286\n",
            "disc_loss -1.54581928 gen_loss 0.340048879\n",
            "disc_loss -0.903188646 gen_loss 0.00945339818\n",
            "disc_loss -1.43210745 gen_loss 0.166620553\n",
            "disc_loss -0.707881093 gen_loss 0.0190773066\n",
            "disc_loss -1.25517702 gen_loss 0.0968693048\n",
            "disc_loss -0.97631979 gen_loss 0.0986143351\n",
            "disc_loss -0.675756037 gen_loss 0.0466678068\n",
            "disc_loss -1.20000553 gen_loss 0.0443527885\n",
            "disc_loss -0.96259439 gen_loss 0.0339151323\n",
            "disc_loss -1.04975212 gen_loss -0.0520669408\n",
            "disc_loss -1.03271043 gen_loss 0.133239225\n",
            "disc_loss -1.39712429 gen_loss 0.00628066342\n",
            "disc_loss -0.763897479 gen_loss 0.0714135319\n",
            "disc_loss -1.37866843 gen_loss -0.131524593\n",
            "disc_loss -0.790560782 gen_loss -0.0292472132\n",
            "disc_loss -1.29370821 gen_loss 0.028319668\n",
            "disc_loss -0.898543656 gen_loss -0.0102300402\n",
            "disc_loss -1.49834549 gen_loss 0.0187362973\n",
            "disc_loss -1.44649804 gen_loss -0.0881948322\n",
            "disc_loss -1.01021695 gen_loss -0.155949786\n",
            "disc_loss -0.813619077 gen_loss -0.093641147\n",
            "disc_loss -1.4843328 gen_loss -0.0853108317\n",
            "disc_loss -1.90705812 gen_loss -0.0466359369\n",
            "disc_loss -1.13845372 gen_loss 0.10329368\n",
            "disc_loss -1.39266634 gen_loss -0.104739234\n",
            "disc_loss -2.00623393 gen_loss -0.131946847\n",
            "disc_loss -0.901668549 gen_loss 0.0454049\n",
            "disc_loss -0.916938365 gen_loss -0.291540086\n",
            "disc_loss -0.841914833 gen_loss 0.036757715\n",
            "disc_loss -1.3504504 gen_loss 0.0841832533\n",
            "disc_loss -1.44238853 gen_loss -0.00406237552\n",
            "disc_loss -1.27957618 gen_loss -0.0342446528\n",
            "disc_loss -0.975467741 gen_loss -0.148178339\n",
            "disc_loss -1.67720878 gen_loss -0.216858715\n",
            "disc_loss -1.40560615 gen_loss -0.252723694\n",
            "disc_loss -1.6192013 gen_loss -0.140040904\n",
            "disc_loss -1.18058848 gen_loss -0.271743298\n",
            "disc_loss -1.2802614 gen_loss -0.374443591\n",
            "disc_loss -1.66953564 gen_loss -0.0775128454\n",
            "disc_loss -1.30724251 gen_loss -0.2072763\n",
            "disc_loss -1.51876509 gen_loss -0.276216805\n",
            "disc_loss -1.62143087 gen_loss -0.233112901\n",
            "disc_loss -2.20087552 gen_loss -0.0027314066\n",
            "disc_loss -0.705604136 gen_loss -1.1211\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-fb4f987b5b30>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_train2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-138-872591e30e6e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Save the model every 15 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-137-91da32355992>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mgradients_of_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mgradients_of_discriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_discriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_gradients_aggregation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         return tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[1;32m   1261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply_gradients_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[0m in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             distribution.extended.update(\n\u001b[0m\u001b[1;32m   1353\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2990\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m   2991\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2993\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m       return self._replica_ctx_update(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4060\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4061\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4062\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4064\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4066\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4067\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4068\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4069\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1347\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_step_xla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;34mf\"`tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             )\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/adam.py\u001b[0m in \u001b[0;36mupdate_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mv_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1831\u001b[0m   if not isinstance(y, ops.Tensor) and not isinstance(\n\u001b[1;32m   1832\u001b[0m       y, sparse_tensor.SparseTensor):\n\u001b[0;32m-> 1833\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noise = tf.random.normal(shape=(10,100))\n",
        "test = generator.predict(noise)\n",
        "plt.imshow(test[1].squeeze(), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "9vPp5AiDkrAF",
        "outputId": "b2cd2f68-0d94-4e90-9706-67be185699e3"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 79ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(x_train2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhuRFc0ro5T_",
        "outputId": "fa0685b7-8209-4b66-c3e2-708e024867b5"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    }
  ]
}