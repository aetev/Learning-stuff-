{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4146e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import reverb\n",
    "import tempfile\n",
    "import PIL.Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas_ta as ta\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import utils\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.networks import Sequential\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.train import triggers\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.train.utils import strategy_utils\n",
    "from tf_agents.train.utils import train_utils\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "41f6edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_change(df, column_name):\n",
    "    relative_changes = []\n",
    "    for i in range(len(df)):\n",
    "        if i == 0:\n",
    "            relative_changes.append(0)  # First element has no previous value\n",
    "        else:\n",
    "            relative_change = (df.iloc[i] - df.iloc[i-1]) / df.iloc[i-1]\n",
    "            relative_changes.append(relative_change)\n",
    "    return pd.DataFrame({column_name: relative_changes})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f12bd9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012178838054657292\n"
     ]
    }
   ],
   "source": [
    "df_csv = pd.read_csv('/tmp/eurusd_hour.csv')\n",
    "pd_close = df_csv['BC']\n",
    "bid_close = pd_close.to_numpy()\n",
    "\n",
    "\n",
    "change = get_relative_change(pd_close,\"Change\")\n",
    "rsi = ta.rsi(pd_close,14,scalar=1)\n",
    "stdev = ta.stdev(pd_close,14)\n",
    "macd = ta.macd(pd_close,14)\n",
    "\n",
    "\n",
    "df = pd.concat([change,rsi,stdev,macd], axis=1)\n",
    "df = df.dropna(axis=0)\n",
    "df = df.reset_index()\n",
    "\n",
    "change = df.pop('Change')\n",
    "df.pop('index')\n",
    "\n",
    "indicators = df.to_numpy()\n",
    "change = change.to_numpy()\n",
    "change_dev = np.std(change)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(change_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1fe21922",
   "metadata": {},
   "outputs": [],
   "source": [
    "commission_rate = .005\n",
    "default_val = 10\n",
    "max_risk = .05\n",
    "max_ep_len = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "94cdc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardGameEnv(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=[-1], maximum=[1], name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(6,), dtype=np.float32, minimum=-10, maximum=10, name='observation')\n",
    "        \n",
    "       \n",
    "        self._state = np.append(indicators[0],(0))\n",
    "        self._episode_ended = False\n",
    "        self._step_count = 1\n",
    "        self._net_profit = 0\n",
    "        self._prev_action = 0\n",
    "        self._total_pen = 0\n",
    "\n",
    "        \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    \n",
    "\n",
    "    def _reset(self):  \n",
    "        \n",
    "        self._state = np.append(indicators[0],(0))\n",
    "        self._episode_ended = False\n",
    "        self._step_count = 1\n",
    "        self._net_profit = 0\n",
    "        self._prev_action = 0\n",
    "        self._total_pen = 0\n",
    "\n",
    "\n",
    "        \n",
    "        return ts.restart(np.array(self._state, dtype=np.float32))\n",
    "\n",
    "    \n",
    "    \n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            return self.reset()          \n",
    "        \n",
    "        action = action[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        cur_val = (action*default_val) * change[self._step_count]\n",
    "        \n",
    "        cur_prec = (cur_val/default_val)\n",
    "        \n",
    "        self._net_profit += cur_val          \n",
    "        \n",
    "        discount_pen = 2 if cur_prec > 0 else 1 \n",
    "\n",
    "        move_penalty = (abs(_prev_action - action)/discount_pen)/10\n",
    "        \n",
    "        self._total_pen -= move_penalty\n",
    "        \n",
    "        self._prev_action = action  \n",
    "        \n",
    "        #update state\n",
    "        self._state = np.append(indicators[self._step_count],(action))\n",
    "        \n",
    "        #end episode if max steps or invalid_action\n",
    "        if self._step_count >= max_ep_len:\n",
    "            self._episode_ended = True\n",
    "            \n",
    "        self._step_count += 1\n",
    "\n",
    "        if self._episode_ended:\n",
    "            reward = self._total_pen   \n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward=reward)\n",
    "        else:\n",
    "            reward = cur_prec\n",
    "            return ts.transition(np.array(self._state, dtype=np.float32), reward=reward,discount=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "da0fb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = CardGameEnv()\n",
    "utils.validate_py_environment(environment, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6978fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
